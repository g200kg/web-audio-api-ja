<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <title>Web Audio API (日本語訳)</title>
  <!--<meta name="revision"
  content="$Id: index.html,v 1.3 2013-10-07 20:39:19 schepers Exp $" />-->
  <link rel="stylesheet" href="style.css" type="text/css" />
  <!--
          <script src="section-links.js" type="application/ecmascript"></script>
          <script src="dfn.js" type="application/ecmascript"></script>
          -->
  <!--[if IE]>
          <style type='text/css'>
            .ignore {
              -ms-filter:"progid:DXImageTransform.Microsoft.Alpha(Opacity=50)";
              filter: alpha(opacity=50);
            }
          </style>
          <![endif]-->
    <link rel="stylesheet" href="http://www.w3.org/StyleSheets/TR/W3C-WD" type="text/css" />
    <style type="text/css">
    body {
      line-height:125%;
    }
    .annotate {
      color:red;
    }
    em {
      padding:4px;
    }
    dfn {
      padding:4px;
    }
    </style>
<script type="text/javascript">
function OpenOriginal(x) {
  var url="http://www.w3.org/TR/2013/WD-webaudio-20131010/#"+x;
  window.open(url, "waapioriginal", "width=1000, height=600, toolbar=1,location=1,resizable=1,scrollbars=1,status=1");
}
</script>
</head>
<body>
<h1>Web Audio API (日本語訳)</h1>
<div style="background-color: #ddf; border: 1px solid #668; padding:10px 20px">
  <p>この文書は、W3Cの文書 "「Web Audio API」 W3C Working Draft 10 October 2013" の日本語訳です。
  </p>
  <h1>    Working Draft は2015年12月8日版に更新されています。</h1>
  <h2><a href="index.html">    現在翻訳作業中です  --- 2015年12月8日版(日本語訳)</a></h2>


  <p>Web Audio APIの正式な文書は英語版のみであり、日本語訳には翻訳に起因する誤りが含まれている場合があります。<br/>
    正式な文書はW3Cのサイト : <a href="http://www.w3.org/TR/webaudio/">http://www.w3.org/TR/webaudio/</a>にあります。
  </p>
  <p>なお、翻訳元の文書もまだドラフト(Working Draft)です。現状ではまだ説明が不足している部分があったり、今後も頻繁に更新される可能性がある事に注意してください。
  </p>
  <p>
    日本語訳GitHub : <b><a href="https://github.com/g200kg/web-audio-api-ja">https://github.com/g200kg/web-audio-api-ja</a></b><br/>
    日本語訳公開URL : <b><a href="http://g200kg.github.io/web-audio-api-ja/">http://g200kg.github.io/web-audio-api-ja/</a></b>
  </p>
  <hr/>
  <p>

    最新の Working Draftでは2013年10月10日版から
    かなり多くの変更が入っています。主なところは次のようなものです。
    <ul style="line-height:100%">
      <li>AudioContextの処理を一時停止/再開させるsuspend()/resume()/close()が追加されました</li>
      <li>AudioContextのdecodeAudioData()はPromiseを使用するようになりました</li>
      <li>offlineAudioContextのstartRendering()はPromiseを使用するようになりました</li>
      <li>WebWorkerを使用してオーディオデータを処理するAudioWorkerが追加されました</li>
      <li>AudioWorkerの追加に伴いScriptProcessorノードはDEPRECATED扱いになりました</li>
      <li>2chステレオ音声向けのStereoPannerノードが追加されました</li>
      <li>AudioBufferSourceノードにdetuneパラメータが追加されました</li>
      <li>AudioNodeのdisconnect()はノードをまとめてではなくコネクション毎に切断できるようになりました</li>
      <li>AudioBufferにサンプルデータの部分的な読出し/書き込みを行うcopyFromChannel()/copyToChannel()が追加されました。大きなデータの一部として複数回読み出しを行うような場合、パフォーマンス面からgetChannelData()よりもcopyFromChannel()が推奨されます</li>
      <li>AnalyserノードにFloat32Arrayで波形データを取得するgetFloatTimeDomainData()が追加されました</li>
      <li>AnalyserのFFTやbiquadFilterなどの処理内容が明確に定義されました</li>
    </ul>
    最新の情報については最新版エディターズドラフトを参照ください。
    <a
    href="http://webaudio.github.io/web-audio-api/">http://webaudio.github.io/web-audio-api/</a>
  </p>
  <p>2016年1月17日</p>
  <hr/>
  <p>Tatsuya Shinyagaito @ g200kg<br/>
    誤りその他があればGitHubページ、下のサイト経由などで連絡をお願いいたします。<br/>
    <b><a href="http://www.g200kg.com/">http://www.g200kg.com/</a></b><br/>
  </p>

</div>

<hr/>

<div class="head">
<p><a href="http://www.w3.org/"><img width="72" height="48" alt="W3C"
src="http://www.w3.org/Icons/w3c_home" /></a> </p>

<h1 id="title" class="title">Web Audio API </h1>

<h2 id="w3c-date-document"><acronym
title="World Wide Web Consortium">W3C</acronym> Working Draft 10 October 2013
</h2>

<dl>
  <dt>このバージョン: </dt>
    <dd><a
    href="http://www.w3.org/TR/2013/WD-webaudio-20131010/">http://www.w3.org/TR/2013/WD-webaudio-20131010/</a>
    </dd>
  <dt>最新のエディターズドラフト: </dt>
    <dd><a
    href="http://webaudio.github.io/web-audio-api/">http://webaudio.github.io/web-audio-api/</a>
    </dd>
  <dt>最新の公開バージョン: </dt>
    <dd><a
      href="http://www.w3.org/TR/webaudio/">http://www.w3.org/TR/webaudio/</a>
    </dd>
  <dt>前のバージョン: </dt>
     <dd><a
      href="http://www.w3.org/TR/2012/WD-webaudio-20121213/">http://www.w3.org/TR/2012/WD-webaudio-20121213/</a>
    </dd>
</dl>

<dl>
  <dt>編集者: </dt>
    <dd>Paul Adenot, Mozilla Foundation</dd>
    <dd>Chris Wilson, Google</dd>
    <dd>Chris Rogers, Google (until August 2013)</dd>
    <dt>執筆者: </dt>
    <dd>See <a href="#L17335">Acknowledgements</a></dd>
</dl>

<p class="copyright"><a href="http://www.w3.org/Consortium/Legal/ipr-notice#Copyright">Copyright</a> © 2013 <a href="http://www.w3.org/"><acronym title="World Wide Web Consortium">W3C</acronym></a><sup>®</sup> (<a href="http://www.csail.mit.edu/"><acronym title="Massachusetts Institute of Technology">MIT</acronym></a>, <a href="http://www.ercim.eu/"><acronym title="European Research Consortium for Informatics and Mathematics">ERCIM</acronym></a>, <a href="http://www.keio.ac.jp/">Keio</a>, <a href="http://ev.buaa.edu.cn/">Beihang</a>), All Rights Reserved. W3C <a href="http://www.w3.org/Consortium/Legal/ipr-notice#Legal_Disclaimer">liability</a>, <a href="http://www.w3.org/Consortium/Legal/ipr-notice#W3C_Trademarks">trademark</a> and <a href="http://www.w3.org/Consortium/Legal/copyright-documents">document use</a> rules apply.</p>
<hr />
</div>
<div id="abstract-section" class="section">
<h2 id="abstract">要約 <button onclick="OpenOriginal('abstract')">原文</button></h2>

<p>この仕様はWebアプリケーションにおけるオーディオの処理および合成に関する高レベルの JavaScript <acronym title="Application Programming Interface">API</acronym> について記述します。
その基本的な枠組みとなっているのはオーディオのルーティンググラフであり、多数の<a href="#AudioNode-section"><code>AudioNode</code></a>オブジェクトが互いに接続される事で最終的なオーディオ出力が定義されます。
実際の処理は基本的に下層にある実装 (典型的には最適化されたアセンブリ言語/C/C++コード)で行われますが、
<a href="#JavaScriptProcessing-section">JavaScriptによる直接的な処理と合成</a> もサポートされています。</p>

<a href="#introduction">序文</a>セクションではこの仕様の背後にある動機についても取り上げます。

<p>このAPIは他のAPIやWebプラットフォームの要素、特に(<code>responseType</code> 、<code>response</code> 属性を使った)
XMLHttpRequest と共に使われるように設計されています。
ゲームやインタラクティブなアプリケーションでは、<code>canvas</code> 2D および WebGL 3D グラフィックスAPIと共に使われる事が予想されます。
</p>
</div>


<div id="sotd-section" class="section">
<h2 id="sotd">この文書の位置づけ <button onclick="OpenOriginal('sotd')">原文</button></h2>

<p><em>このセクションは、この文書の公開時における状況を記述したものです。 他の文書がこの文書に取って代わるかもしれません。
現在のW3Cの刊行物およびこの技術レポートの最新改訂版のリストは、<a href="http://www.w3.org/TR/">W3C技術レポート</a>http://www.w3.org/TR/の索引から見つけることができます。</em></p>

<p>これは<cite>Web Audio API</cite>仕様の公開ワーキングドラフトの第5版です。
これはW3C WebApps活動の一部である<a href="http://www.w3.org/2011/audio/"><b>W3C Audio Working Group</b></a>によって策定されました。</p>
<p></p>

<p>この文書に関するコメントは &lt;<a href="mailto:public-audio@w3.org">public-audio@w3.org</a>&gt;
  (<a href="http://lists.w3.org/Archives/Public/public-audio/">public archives</a> of
the W3C audio mailing list)で受け付けます。 Webコンテンツおよびブラウザ開発者はこのドラフトをレビューする事を勧めます。
</p>

<p>ワーキングドラフトとしての公開はW3Cメンバーによる承認を意味するものではありません。
これは草案文書であり、いつでも他の文書によって改訂、置き換えあるいは廃止される可能性があります。
この文書を作業中のもの以外として引用する事は不適切です。</p>

<p>この文書は<a href="http://www.w3.org/Consortium/Patent-Policy-20040205/">2004年2月5日 W3C Patent Policy</a>の基に運用されるグループによって作成されました。
W3Cはグループの成果物から作成された<a rel="disclosure" href="http://www.w3.org/2004/01/pp-impl/46884/status">特許開示リスト</a>を維持しています。
また、このページは特許開示の方法についても提示しています。
特許について実際の知識を持ち、それが<a href="http://www.w3.org/Consortium/Patent-Policy-20040205/#def-essential">基本的な請求事項を含む</a>を含むと考える者は、
<a href="http://www.w3.org/Consortium/Patent-Policy-20040205/#sec-Disclosure">W3C Patent Policy 6節</a>に従って情報を開示しなくてはなりません。
</p>
</div>

<div id="toc">
<h2 id="L13522">目次 <button onclick="OpenOriginal('toc')">原文</button></h2>

<div class="toc">
<ul>
  <li><a href="#introduction">1. 序文</a>
    <ul>
      <li><a href="#Features">1.1. 機能</a></li>
      <li><a href="#ModularRouting">1.2. モジュラールーティング</a></li>
      <li><a href="#APIOverview">1.3. APIの概要</a></li>
    </ul>
  </li>
  <li><a href="#conformance">2. 準拠</a></li>
  <li><a href="#API-section">4. オーディオAPI</a>
    <ul>
      <li><a href="#AudioContext-section">4.1. AudioContext インターフェース</a>
        <ul>
          <li><a href="#attributes-AudioContext">4.1.1. 属性</a></li>
          <li><a href="#methodsandparams-AudioContext">4.1.2. メソッドとパラメータ</a></li>
          <li><a href="#lifetime-AudioContext">4.1.3. ライフタイム</a></li>
        </ul>
      </li>
      <li><a href="#OfflineAudioContext-section">4.1b. OfflineAudioContext インターフェース</a>
      </li>

      <li><a href="#AudioNode-section">4.2. AudioNode インターフェース</a>
        <ul>
          <li><a href="#attributes-AudioNode">4.2.1. 属性</a></li>
          <li><a href="#methodsandparams-AudioNode">4.2.2. メソッドとパラメータ</a></li>
          <li><a href="#lifetime-AudioNode">4.2.3. ライフタイム</a></li>
        </ul>
      </li>
      <li><a href="#AudioDestinationNode">4.4. The AudioDestinationNode インターフェース</a>
        <ul>
          <li><a href="#attributes-AudioDestinationNode">4.4.1. 属性</a></li>
        </ul>
      </li>
      <li><a href="#AudioParam">4.5. AudioParam インターフェース</a>
        <ul>
          <li><a href="#attributes-AudioParam">4.5.1. 属性</a></li>
          <li><a href="#methodsandparams-AudioParam">4.5.2. メソッドとパラメータ</a></li>
          <li><a href="#computedValue-AudioParam-section">4.5.3. 値の計算</a></li>
          <li><a href="#example1-AudioParam-section">4.5.4. AudioParam オートメーションの例</a></li>
        </ul>
      </li>
      <li><a href="#GainNode">4.7. GainNode インターフェース</a>
        <ul>
          <li><a href="#attributes-GainNode">4.7.1. 属性</a></li>
        </ul>
      </li>
      <li><a href="#DelayNode">4.8. DelayNode インターフェース</a>
        <ul>
          <li><a href="#attributes-GainNode_2">4.8.1. 属性</a></li>
        </ul>
      </li>
      <li><a href="#AudioBuffer">4.9. AudioBuffer インターフェース</a>
        <ul>
          <li><a href="#attributes-AudioBuffer">4.9.1. 属性</a></li>
          <li><a href="#methodsandparams-AudioBuffer">4.9.2. メソッドとパラメータ</a></li>
        </ul>
      </li>
      <li><a href="#AudioBufferSourceNode">4.10. AudioBufferSourceNode インターフェース</a>
        <ul>
          <li><a href="#attributes-AudioBufferSourceNode">4.10.1. 属性</a></li>
          <li><a href="#methodsandparams-AudioBufferSourceNode">4.10.2. メソッドとパラメータ</a></li>
        </ul>
      </li>
      <li><a href="#MediaElementAudioSourceNode">4.11. MediaElementAudioSourceNode インターフェース</a></li>
      <li><a href="#ScriptProcessorNode">4.12. ScriptProcessorNode インターフェース</a>
        <ul>
          <li><a href="#attributes-ScriptProcessorNode">4.12.1. 属性</a></li>
        </ul>
      </li>
      <li><a href="#AudioProcessingEvent">4.13. AudioProcessingEvent インターフェース</a>
        <ul>
          <li><a href="#attributes-AudioProcessingEvent">4.13.1. 属性</a></li>
        </ul>
      </li>
      <li><a href="#PannerNode">4.14. PannerNode インターフェース</a>
        <ul>
          <li><a href="#attributes-PannerNode_attributes">4.14.2. 属性</a></li>
          <li><a href="#Methods_and_Parameters">4.14.3. メソッドとパラメータ</a></li>
        </ul>
      </li>
      <li><a href="#AudioListener">4.15. AudioListener インターフェース</a>
        <ul>
          <li><a href="#attributes-AudioListener">4.15.1. 属性</a></li>
          <li><a href="#L15842">4.15.2. メソッドとパラメータ</a></li>
        </ul>
      </li>
      <li><a href="#ConvolverNode">4.16. ConvolverNode インターフェース</a>
        <ul>
          <li><a href="#attributes-ConvolverNode">4.16.1. 属性</a></li>
        </ul>
      </li>
      <li><a href="#AnalyserNode">4.17. AnalyserNode インターフェース</a>
        <ul>
          <li><a href="#attributes-ConvolverNode_2">4.17.1. 属性</a></li>
          <li><a href="#methods-and-parameters">4.17.2. メソッドとパラメータ</a></li>
        </ul>
      </li>
      <li><a href="#ChannelSplitterNode">4.18. ChannelSplitterNode インターフェース</a>
        <ul>
          <li><a href="#example-1">例:</a></li>
        </ul>
      </li>
      <li><a href="#ChannelMergerNode">4.19. ChannelMergerNode インターフェース</a>
        <ul>
          <li><a href="#example-2">例:</a></li>
        </ul>
      </li>
      <li><a href="#DynamicsCompressorNode">4.20. DynamicsCompressorNode インターフェース</a>
        <ul>
          <li><a href="#attributes-DynamicsCompressorNode">4.20.1. 属性</a></li>
        </ul>
      </li>
      <li><a href="#BiquadFilterNode">4.21. BiquadFilterNode インターフェース</a>
        <ul>
          <li><a href="#BiquadFilterNode-description">4.21.1 "lowpass"</a></li>
          <li><a href="#HIGHPASS">4.21.2 "highpass"</a></li>
          <li><a href="#BANDPASS">4.21.3 "bandpass"</a></li>
          <li><a href="#LOWSHELF">4.21.4 "lowshelf"</a></li>
          <li><a href="#L16352">4.21.5 "highshelf"</a></li>
          <li><a href="#PEAKING">4.21.6 "peaking"</a></li>
          <li><a href="#NOTCH">4.21.7 "notch"</a></li>
          <li><a href="#ALLPASS">4.21.8 "allpass"</a></li>
          <li><a href="#Methods">4.21.9. メソッド</a></li>
        </ul>
      </li>
      <li><a href="#WaveShaperNode">4.22. WaveShaperNode インターフェース</a>
        <ul>
          <li><a href="#attributes-WaveShaperNode">4.22.1. 属性</a></li>
        </ul>
      </li>
      <li><a href="#OscillatorNode">4.23. OscillatorNode インターフェース</a>
        <ul>
          <li><a href="#attributes-OscillatorNode">4.23.1. 属性</a></li>
            <li><a href="#methodsandparams-OscillatorNode-section">4.23.2. メソッドとパラメータ</a></li>
        </ul>
      </li>
      <li><a href="#PeriodicWave">4.24. PeriodicWave インターフェース</a>
      </li>
      <li><a href="#MediaStreamAudioSourceNode">4.25. MediaStreamAudioSourceNode インターフェース</a></li>
      <li><a href="#MediaStreamAudioDestinationNode">4.26. MediaStreamAudioDestinationNode インターフェース</a></li>
    </ul>
  </li>
  <li><a href="#MixerGainStructure">6. ミキサーゲイン構成</a>
    <ul>
      <li><a href="#background">背景</a></li>
      <li><a href="#SummingJunction">サミング入力</a></li>
      <li><a href="#gain-Control">ゲインコントロール</a></li>
      <li><a href="#Example-mixer-with-send-busses">例: センドバス付ミキサー</a></li>
    </ul>
  </li>
  <li><a href="#DynamicLifetime">7. 動的ライフタイム</a>
    <ul>
      <li><a href="#DynamicLifetime-background">背景</a></li>
      <li><a href="#Example-DynamicLifetime">例</a></li>
    </ul>
  </li>
  <li><a href="#UpMix">9. チャンネルのアップミックスとダウンミックス</a>
    <ul>
      <li><a href="#ChannelLayouts">9.1. スピーカーチャンネル配置</a>
      <ul>
        <li><a href="#ChannelOrdering">9.1.1. チャンネルの順序</a></li>
        <li><a href="#UpMix-sub">9.1.2. アップミックスのスピーカー配置</a></li>
        <li><a href="#down-mix">9.1.3. ダウンミックスのスピーカー配置</a></li>
      </ul>
      </li>

      <li><a href="#ChannelRules-section">9.2. チャンネルルールの例</a></li>

    </ul>
  </li>
  <li><a href="#Spatialization">11. 空間音響 / バンニング </a>
    <ul>
      <li><a href="#Spatialization-background">背景</a></li>
      <li><a href="#Spatialization-panning-algorithm">パンニングアルゴリズム</a></li>
      <li><a href="#Spatialization-distance-effects">距離効果</a></li>
      <li><a href="#Spatialization-sound-cones">サウンドコーン</a></li>
      <li><a href="#Spatialization-doppler-shift">ドップラー効果</a></li>
    </ul>
  </li>
  <li><a href="#Convolution">12. コンボリューションによる線形エフェクト</a>
    <ul>
      <li><a href="#Convolution-background">背景</a></li>
      <li><a href="#Convolution-motivation">標準として使用する動機</a></li>
      <li><a href="#Convolution-implementation-guide">実装ガイド</a></li>
      <li><a href="#Convolution-reverb-effect">リバーブ・エフェクト (マトリックス付き)</a></li>
      <li><a href="#recording-impulse-responses">インパルスレスポンスの記録</a></li>
      <li><a href="#tools">ツール</a></li>
      <li><a href="#recording-setup">レコーディング・セットアップ</a></li>
      <li><a href="#warehouse">倉庫空間</a></li>
    </ul>
  </li>
  <li><a href="#JavaScriptProcessing">13. JavaScript による合成と処理</a>
    <ul>
      <li><a href="#custom-DSP-effects">カスタムDSPエフェクト</a></li>
      <li><a href="#educational-applications">教育アプリケーション</a></li>
      <li><a href="#javaScript-performance">JavaScript パフォーマンス</a></li>
    </ul>
  </li>
  <li><a href="#Performance">15. パフォーマンスについての考察</a>
    <ul>
      <li><a href="#Latency">15.1. レイテンシー: それは何で何故重要なのか</a></li>
      <li><a href="#audio-glitching">15.2. オーディオグリッジ</a></li>
      <li><a href="#hardware-scalability">15.3. ハードウェアのスケーラビリティ</a>
        <ul>
          <li><a href="#CPU-monitoring">15.3.1. CPU監視</a></li>
          <li><a href="#Voice-dropping">15.3.2. ボイスドロッピング</a></li>
          <li><a href="#Simplification-of-Effects-Processing">15.3.3. エフェクトの単純化</a></li>
          <li><a href="#Sample-rate">15.3.4. サンプルレート</a></li>
          <li><a href="#pre-flighting">15.3.5. プリフライト</a></li>
          <li><a href="#Authoring-for-different-user-agents">15.3.6. 異なるユーザーエージェントのためのオーサリング</a></li>
          <li><a href="#Scalability-of-Direct-JavaScript-Synthesis">15.3.7. JavaScriptによる直接合成/処理のスケーラビリティ</a></li>
        </ul>
      </li>
      <li><a href="#JavaScriptPerformance">15.4. リアルタイム処理と合成に関するJavaScriptの課題: </a></li>
    </ul>
  </li>
  <li><a href="#ExampleApplications">16. アプリケーション例</a>
    <ul>
      <li><a href="#basic-sound-playback">基本的な音の再生</a></li>
      <li><a href="#threeD-environmentse-and-games">3D環境とゲーム</a></li>
      <li><a href="#musical-applications">音楽アプリケーション</a></li>
      <li><a href="#music-visualizers">ミュージックビジュアライザー</a></li>
      <li><a href="#educational-applications_2">教育アプリケーション</a></li>
      <li><a href="#artistic-audio-exploration">芸術的な音の探究</a></li>
    </ul>
  </li>
  <li><a href="#SecurityConsiderations">17. セキュリティに関する考察</a></li>
  <li><a href="#PrivacyConsiderations">18. プライバシーに関する考察</a></li>
  <li><a href="#requirements">19. 要求と使用例</a></li>
  <li><a href="#L17310">A.リファレンス</a>
    <ul>
      <li><a href="#Normative-references">A.1 基準リファレンス</a></li>
      <li><a href="#Informative-references">A.2 参考リファレンス</a></li>
    </ul>
  </li>
  <li><a href="#L17335">B.謝辞</a></li>
  <li><a href="#ChangeLog">C. Web Audio API 更新履歴</a></li>
</ul>

</div>
</div>

<div id="sections">

<div id="div-introduction" class="section">
<h2 id="introduction">1. 序文 <button onclick="OpenOriginal('introduction')">原文</button></h2>

<p class="norm">このセクションは参考情報です</p>

<p>これまでのWeb上のオーディオはかなり未発達なもので、ごく最近までFlashやQuickTimeのようなプラグインを通して配信しなくてはなりませんでした。
HTML5での<code>audio</code>要素の導入は、基本的なストリーミング・オーディオ再生を可能にする重要なものでした。
しかし、より複雑なオーディオアプリケーションを扱うには、それだけではまだ充分強力ではありません。
洗練されたWebベースのゲームやインタラクティブ・アプリケーションのためには別の解決策が必要とされます。
この仕様では、近年のデスクトップ・オーディオ制作アプリケーションに見られるミキシング、プロセシング、フィルタリング処理に加え、近年のゲームオーディオエンジンに見られるような機能も持たせる事を目標とします。
</p>

<p>このAPIはさまざまな<a href="#ExampleApplications-section">使用例</a>を考慮して設計されています。
理想的には<i>全ての</i>使用例が、JavaScriptから制御される最適化されたC++エンジンを使って無理なく実装でき、ブラウザで動作するようにサポートされなくてはなりません。
とは言っても、近年のデスクトップ・オーディオソフトウェアは極めて高度な機能を持ち、それらの一部はこのシステムを使ったとしても構築する事が困難か不可能と考えられます。
Apple社のLogic Audioがそのようなアプリケーションの1つであり、外部MIDIコントローラー、任意のプラグイン・オーディオエフェクトやシンセサイザー、高度に最適化されたオーディオファイルのディスクへの読み込み/書き出し、密に統合されたタイムストレッチなどなどをサポートしています。
それでもなお、ここで提案するシステムは、音楽に関するものを含めて、かなり複雑なゲームやインタラクティブ・アプリケーションの広い範囲を充分にサポートする事が可能です。
またそれは、WebGLによってもたらされる、より高度なグラフィックスの機能をよく引き立たせる事が可能です。
このAPIはより高度な機能を後から追加できるように設計されています。
</p>
<div id="Features-section" class="section">
<h2 id="Features">1.1. 機能 <button onclick="OpenOriginal('Features')">原文</button></h2>

<p>このAPIは、これらの基本機能をサポートします: </p>
<ul>
  <li>単純な、または<a href="#MixerGainStructure-section">複数のセンド、サブミックス</a>を含む複雑なミキシング/エフェクト・アーキテクチャーのための<a href="#ModularRouting-section">モジュラールーティング</a></li>
  <li> 非常に高度なリズムの精度を必要とするドラムマシンやシーケンサーなどのアプリケーションのための、低<a href="#Latency-section">レイテンシー</a>な<a href="#AudioParam">サンプル単位の時間精度での音の再生</a>。これには、エフェクトを<a href="#DynamicLifetime-section">動的に生成</a>できるようにする事も含まれます</li>
  <li>エンベロープ、フェードイン/フェードアウト、グラニュラーエフェクト、フィルタスイープ、LFOなどのためのオーディオパラメータのオートメーション</li>
  <li>分割や結合など、オーディオストリームのチャンネルに対する柔軟な扱い</li>
  <li><code>audio</code> または <code>video</code> <a href="#MediaElementAudioSourceNode">メディア要素</a>からのオーディオに対する処理</li>
  <li>getUserMedia()からの<a href="#MediaStreamAudioSourceNode">MediaStream</a>を使用したライブオーディオに対する処理</li>
  <li>WebRTCとの統合
   <ul>
    <li><a href="#MediaStreamAudioSourceNode">MediaStream</a>を使ったリモート・ピアから受け取ったオーディオの処理</li>
    <li>生成または加工されたオーディオストリームの<a href="#MediaStreamAudioDestinationNode">MediaStream</a>を使ったリモート・ピアへの送信</li>
   </ul>
  </li>
  <li><a href="#JavaScriptProcessing-section">JavaScriptでの直接的な</a>オーディオストリームの合成および加工</li>
  <li>3Dゲームや没入環境を幅広くサポートする<a href="#Spatialization-section">空間音響</a>:
    <ul>
      <li>パンニングモデル: 等価パワー, HRTF, パススルー </li>
      <li>距離減衰 </li>
      <li>サウンドコーン </li>
      <li>障害物 / 遮蔽物 </li>
      <li>ドップラー効果 </li>
      <li>ソース / リスナー</li>
    </ul>
  </li>
  <li>広範囲の線形エフェクト、特に非常に高い品質のルーム・エフェクトに使用できる<a href="#Convolution-section">コンボリューションエンジン</a>。
    これによって可能なエフェクトの例を以下に示します:
    <ul>
      <li>小さい / 大きい部屋 </li>
      <li>大聖堂 </li>
      <li>コンサートホール </li>
      <li>洞窟 </li>
      <li>トンネル </li>
      <li>廊下 </li>
      <li>森 </li>
      <li>野外劇場 </li>
      <li>出入り口を経由した遠くの部屋 </li>
      <li>極端なフィルタ </li>
      <li>風変りな巻き戻し効果 </li>
      <li>極端なコムフィルタ効果 </li>
    </ul>
  </li>
  <li>ミックス全体の制御やスウィートニング<span class="annotate">(訳注:ビデオに効果音などをつけるMA作業)</span>のためのダイナミック・コンプレッション </li>
  <li>効率的な<a href="#AnalyserNode">リアルタイムの時間領域および周波数領域解析 / ミュージックビジュアライザーのサポート</a></li>
  <li>効率的な双2次フィルタによる、ローパス、ハイパス、その他一般的なフィルタ</li>
  <li>ディストーションやその他の非線形エフェクトのためのウェーブシェイピング・エフェクト</li>
  <li>オシレータ</li>

</ul>
</div>

<div id="ModularRouting-section">
<h2 id="ModularRouting">1.2. モジュラールーティング <button onclick="OpenOriginal('ModularRouting')">原文</button></h2>

<p>モジュラールーティングによって異なる <a href="#AudioNode-section"><code>AudioNode</code></a> オブジェクト同士を任意に接続できます。
それぞれのノードは<dfn>入力</dfn>および<dfn>出力</dfn>を持っています。
<dfn>ソースノード</dfn>は入力は持たず、1つの出力を持ちます。
<dfn>デスティネーションノード</dfn>は1つの入力を持ち、出力は持っていません。この最も一般的な例が最終的なオーディオハードウェアに繋がる<a href="#AudioDestinationNode-section"><code>AudioDestinationNode</code></a>です。
フィルタなどの他のノードはソースとデスティネーションの間に配置することができます。
2つのオブジェクトが互いに接続している場合、低レベルのストリーム形式の詳細について開発者が煩わされる事なく、<a href="#UpMix-section">適正な処理が行われます</a>。
例えばもしモノラルの音声ストリームがステレオの入力に接続されていても、左右のチャンネルに<a href="#UpMix-section">適正</a>にミックスされます。
</p>

<p>最も単純な例は、1つの音声ソースを出力に直接接続したものです。
 すべての接続は<a href="#AudioDestinationNode-section"><code>AudioDestinationNode</code></a>を持つ<a href="#AudioContext-section"><code>AudioContext</code></a>内部で行われます:
</p>


<img alt="modular routing" src="images/modular-routing1.png" />

<p>この単純なルーティングを図示します。この例では単一の音を再生しています: </p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">ECMAScript</span> </div>

<div class="blockContent">
<pre class="code"><code class="es-code">

var context = new AudioContext();

function playSound() {
    var source = context.createBufferSource();
    source.buffer = dogBarkingBuffer;
    source.connect(context.destination);
    source.start(0);
}
                    </code></pre>
</div>
</div>

<p>これはもっと複雑な例で、3つのソースとコンボリューションリバーブが最終出力段にあるダイナミックコンプレッサーを介して送られます:</p>
<img alt="modular routing2" src="images/modular-routing2.png" />

<div class="example">

<div class="exampleHeader">
Example</div>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">ECMAScript</span></div>

<div class="blockContent">
<pre class="code"><code class="es-code">

var context = 0;
var compressor = 0;
var reverb = 0;

var source1 = 0;
var source2 = 0;
var source3 = 0;

var lowpassFilter = 0;
var waveShaper = 0;
var panner = 0;

var dry1 = 0;
var dry2 = 0;
var dry3 = 0;

var wet1 = 0;
var wet2 = 0;
var wet3 = 0;

var masterDry = 0;
var masterWet = 0;

function setupRoutingGraph () {
    context = new AudioContext();

    // Create the effects nodes.
    lowpassFilter = context.createBiquadFilter();
    waveShaper = context.createWaveShaper();
    panner = context.createPanner();
    compressor = context.createDynamicsCompressor();
    reverb = context.createConvolver();

    // Create master wet and dry.
    masterDry = context.createGain();
    masterWet = context.createGain();

    // Connect final compressor to final destination.
    compressor.connect(context.destination);

    // Connect master dry and wet to compressor.
    masterDry.connect(compressor);
    masterWet.connect(compressor);

    // Connect reverb to master wet.
    reverb.connect(masterWet);

    // Create a few sources.
    source1 = context.createBufferSource();
    source2 = context.createBufferSource();
    source3 = context.createOscillator();

    source1.buffer = manTalkingBuffer;
    source2.buffer = footstepsBuffer;
    source3.frequency.value = 440;

    // Connect source1
    dry1 = context.createGain();
    wet1 = context.createGain();
    source1.connect(lowpassFilter);
    lowpassFilter.connect(dry1);
    lowpassFilter.connect(wet1);
    dry1.connect(masterDry);
    wet1.connect(reverb);

    // Connect source2
    dry2 = context.createGain();
    wet2 = context.createGain();
    source2.connect(waveShaper);
    waveShaper.connect(dry2);
    waveShaper.connect(wet2);
    dry2.connect(masterDry);
    wet2.connect(reverb);

    // Connect source3
    dry3 = context.createGain();
    wet3 = context.createGain();
    source3.connect(panner);
    panner.connect(dry3);
    panner.connect(wet3);
    dry3.connect(masterDry);
    wet3.connect(reverb);

    // Start the sources now.
    source1.start(0);
    source2.start(0);
    source3.start(0);
}
 </code></pre>
</div>
</div>
</div>
</div>

<div id="APIOverview-section" class="section">
<h2 id="APIOverview">1.3. APIの概要 <button onclick="OpenOriginal('APIOverview')">原文</button></h2>

<p>定義されているインターフェースは次の通りです: </p>
<ul>
  <li><a class="dfnref" href="#AudioContext-section">AudioContext</a>インターフェースは、AudioNodeの接続を表すオーディオ信号グラフを持ちます。
  </li>
  <li><a class="dfnref" href="#AudioNode-section">AudioNode</a>インターフェースは、オーディオのソース、オーディオの出力、その間にある処理モジュールを表します。
    AudioNodeは<a href="#ModularRouting-section">モジュラー方式</a>で動的に互いに接続されます。
     <code>AudioNode</code>は<code>AudioContext</code>のコンテキスト内に存在します。
  </li>
  <li><a class="dfnref" href="#AudioDestinationNode-section">AudioDestinationNode</a>インターフェースは、AudioNodeのサブクラスでオーディオの最終的な出力地点を表します。
  </li>
  <li><a class="dfnref" href="#AudioBuffer-section">AudioBuffer</a>インターフェースは、
    メモリ内に保持されるオーディオのリソースで使用されます。これらはワンショットの音、またはもっと長いオーディオクリップを表します。
  </li>
  <li><a class="dfnref" href="#AudioBufferSourceNode-section">AudioBufferSourceNode</a>インターフェースは、AudioBufferからの音を発生するAudioNodeです。
  </li>
  <li><a class="dfnref" href="#MediaElementAudioSourceNode-section">MediaElementAudioSourceNode</a>インターフェースは、
    <code>audio</code>、<code>video</code>その他のメディア要素からの音のソースのAudioNodeです。
  </li>
  <li><a class="dfnref" href="#MediaStreamAudioSourceNode-section">MediaStreamAudioSourceNode</a>インターフェースは、
    ライブオーディオ入力やリモート・ピアから受け取ったようなMediaStreamからのオーディオソースのAudioNodeです。
  </li>
  <li><a class="dfnref" href="#MediaStreamAudioDestinationNode-section">MediaStreamAudioDestinationNode</a>インターフェースは、
    リモート・ピアに送信するMediaStreamへ出力するAudioNodeです。
  </li>
  <li><a class="dfnref" href="#ScriptProcessorNode-section">ScriptProcessorNode</a>インターフェースは、
    音の合成や加工をjavaScriptで直接行うAudioNodeです。
  </li>
  <li><a class="dfnref" href="#AudioProcessingEvent-section">AudioProcessingEvent</a>インターフェースは、
    <code>ScriptProcessorNode</code>オブジェクトで使用されるイベントタイプです。
  </li>
  <li><a class="dfnref" href="#AudioParam-section">AudioParam</a>インターフェースは、AudioNodeの個別の機能、例えば音量などを制御します。
  </li>
  <li><a class="dfnref" href="#GainNode-section">GainNode</a>インターフェースは、明示的なゲイン制御を行います。
    AudioNodeへの入力は(ユニティ・ゲイン<span class="annotate">(訳注:ゲインが1である事)</span>の加算による)複数の接続をサポートしているため、
    GainNodeを使う事でミキサーが<a href="#MixerGainStructure-section">簡単に構成</a>できます。
  </li>
  <li><a class="dfnref" href="#BiquadFilterNode-section">BiquadFilterNode</a>インターフェースは、
    次のような一般的な低次のフィルタのAudioNodeです。:
    <ul>
      <li>ローパス </li>
      <li>ハイパス </li>
      <li>バンドパス </li>
      <li>ローシェルフ </li>
      <li>ハイシェルフ </li>
      <li>ピーキング </li>
      <li>ノッチ </li>
      <li>オールパス </li>
    </ul>
  </li>
  <li><a class="dfnref" href="#DelayNode-section">DelayNode</a>インターフェースは、動的に調整可能な遅延を加えるAudioNodeです。
  </li>
  <li><a class="dfnref" href="#PannerNode-section">PannerNode</a>インターフェースは、3次元空間での空間音響 / 空間定位を行います。
  </li>
  <li><a class="dfnref" href="#AudioListener-section">AudioListener</a>インターフェースは、<code>PannerNode</code>と共に空間音響のために使われます。
  </li>
  <li><a class="dfnref" href="#ConvolverNode-section">ConvolverNode</a>インターフェースは、
    (例えばコンサートホールでの残響効果のような)<a href="#Convolution-section">リアルタイム線形エフェクト</a>を加えるAudioNodeです。
  </li>
  <li><a class="dfnref" href="#AnalyserNode-section">AnalyserNode</a>インターフェースは、ミュージックビジュアライザーやその他の視覚化アプリケーションで使用されます。
  </li>
  <li><a class="dfnref" href="#ChannelSplitterNode-section">ChannelSplitterNode</a>インターフェースは、
    ルーティンググラフ内のオーディオストリームの個別のチャンネルにアクセスするために使用します。
  </li>
  <li><a class="dfnref" href="#ChannelMergerNode-section">ChannelMergerNode</a>インターフェースは、
    複数のオーディオストリームから1つのオーディオストリームにチャンネルの結合を行います。
  </li>
  <li><a href="#DynamicsCompressorNode-section">DynamicsCompressorNode</a>インターフェースは、
    ダイナミック・コンプレッションのためのAudioNodeです。
  </li>
  <li><a class="dfnref" href="#dfn-WaveShaperNode">WaveShaperNode</a>インターフェースは、
    例えばディストーションや微妙なウォーミング効果<span class="annotate">(訳注:いわゆるサチュレーション効果の事)</span>など、非線形のウェーブシェイピング・エフェクトを加える
    ためのAudioNodeです。
  </li>
  <li><a class="dfnref" href="#dfn-OscillatorNode">OscillatorNode</a>インターフェースは、周期的な波形を発生するオーディオソースとなります。
  </li>
</ul>

</div>
</div>

<div id="conformance-section" class="section">
<h2 id="conformance">2. 準拠 <button onclick="OpenOriginal('conformance')">原文</button></h2>

<p>この仕様中の全ては、例とセクションの最初に参考情報と書かれているものを除いて、基準要件となります。</p>

<p>この文書中のキーワード"<span class="rfc2119">MUST</span>"、 "<span class="rfc2119">MUST NOT</span>"、
  "<span class="rfc2119">REQUIRED</span>"、"<span class="rfc2119">SHALL</span>"、
  "<span class="rfc2119">SHALL NOT</span>"、"<span class="rfc2119">RECOMMENDED</span>"、"<span class="rfc2119">MAY</span>"、
  "<span class="rfc2119">OPTIONAL</span>"
  は<cite><a href="http://www.ietf.org/rfc/rfc2119">Key words for use in RFCs to Indicate Requirement Levels</a></cite> <a href="#RFC2119">[RFC2119]</a>
  に記述されているように解釈されます。
</p>

<p>次の準拠クラスがこの仕様によって定義されています:</p>
<dl>
  <dt><dfn id="dfn-conforming-implementation">準拠した実装</dfn></dt>
    <dd><p>この仕様の全ての<span class="rfc2119">MUST</span>-、 <span class="rfc2119">REQUIRED</span>-、 <span class="rfc2119">SHALL</span>
      レベルの基準を満足する実装がされているユーザーエージェントが、<a class="dfnref" href="#dfn-conforming-implementation">準拠した実装</a>と判断されます。</p>
    </dd>
</dl>

</div>

<div id="API-section-section" class="section">

<h2 id="API-section">4. オーディオAPI <button onclick="OpenOriginal('API-section')">原文</button></h2>

<div id="AudioContext-section-section" class="section">
<h2 id="AudioContext-section">4.1. AudioContextインターフェース <button onclick="OpenOriginal('AudioContext-section')">原文</button></h2>

<p>このインターフェースは<a href="#AudioNode-section"><code>AudioNode</code></a>の集合やそれらの接続を表すインターフェースです。
それは(最終的にユーザーが聴く事になる)<a href="#AudioDestinationNode-section"><code>AudioDestinationNode</code></a>までの信号を任意に接続する事ができます。
ノードはコンテキストから作成され、お互いに<a href="#ModularRouting-section">接続</a>されます。
多くの場合、1つのドキュメントに対し、1つのAudioContextが使用されます。
</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="audio-context-idl">

callback DecodeSuccessCallback = void (AudioBuffer decodedData);
callback DecodeErrorCallback = void ();

[Constructor]
interface <dfn id="dfn-AudioContext">AudioContext</dfn> : EventTarget {

    readonly attribute AudioDestinationNode destination;
    readonly attribute float sampleRate;
    readonly attribute double currentTime;
    readonly attribute AudioListener listener;

    AudioBuffer createBuffer(unsigned long numberOfChannels, unsigned long length, float sampleRate);

    void decodeAudioData(ArrayBuffer audioData,
                         DecodeSuccessCallback successCallback,
                         optional DecodeErrorCallback errorCallback);


    <span class="comment">// AudioNode creation </span>
    AudioBufferSourceNode createBufferSource();

    MediaElementAudioSourceNode createMediaElementSource(HTMLMediaElement mediaElement);

    MediaStreamAudioSourceNode createMediaStreamSource(MediaStream mediaStream);
    MediaStreamAudioDestinationNode createMediaStreamDestination();

    ScriptProcessorNode createScriptProcessor(optional unsigned long bufferSize = 0,
                                              optional unsigned long numberOfInputChannels = 2,
                                              optional unsigned long numberOfOutputChannels = 2);

    AnalyserNode createAnalyser();
    GainNode createGain();
    DelayNode createDelay(optional double maxDelayTime = 1.0);
    BiquadFilterNode createBiquadFilter();
    WaveShaperNode createWaveShaper();
    PannerNode createPanner();
    ConvolverNode createConvolver();

    ChannelSplitterNode createChannelSplitter(optional unsigned long numberOfOutputs = 6);
    ChannelMergerNode createChannelMerger(optional unsigned long numberOfInputs = 6);

    DynamicsCompressorNode createDynamicsCompressor();

    OscillatorNode createOscillator();
    PeriodicWave createPeriodicWave(Float32Array real, Float32Array imag);

};
</code></pre>
</div>
</div>

<div id="attributes-AudioContext-section" class="section">
<h3 id="attributes-AudioContext">4.1.1. 属性 <button onclick="OpenOriginal('attributes-AudioContext')">原文</button></h3>
<dl>
  <dt id="dfn-destination"><code>destination</code></dt>
    <dd><p><a href="#AudioDestinationNode-section"><code>AudioDestinationNode</code></a>
      は単一の入力を持ち、全てのオーディオの最終的な出口を表しています。
      通常これは実際のオーディオハードウェアを表します。
      動作中のすべてのAudioNodeは直接または間接的にこの<code>destination</code>に接続されます。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-sampleRate"><code>sampleRate</code></dt>
    <dd><p>AudioContextが扱うオーディオのサンプルレート(1秒あたりのサンプルフレーム数)。
      コンテキスト内のすべてのAudioNodeはこのレートで動作する事を想定しています。
      これを想定するため、サンプレートコンバータや"可変速"処理はリアルタイム処理内ではサポートされません。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-currentTime"><code>currentTime</code></dt>
    <dd><p>これはコンテキストが作成された時点を0として実時間で増加してゆく、秒で表される時刻です。
      全てのスケジュールされた時刻はこれに対する相対値となります。
      これは再生、ポーズ、位置指定などが可能な"トランスポート"<span class="annotate">(訳注:テープレコーダーなどの再生/巻き戻しなどの機構の事)</span>の時刻ではなく、
      常に増加する方向に進んでゆきます。GarageBandのようなタイムライン"トランスポート"型の
      システムはこの上に非常に簡単に(JavaScriptで)構築できます。
      この時刻は常に増加してゆくハードウェアのタイムスタンプに対応しています。
    </p></dd>
</dl>
<dl>
  <dt id="dfn-listener"><code>listener</code></dt>
    <dd><p>3D<a
      href="#Spatialization-section">空間音響</a>で使われる<a href="#AudioListener-section"><code>AudioListener</code></a></p>
    </dd>
</dl>
</div>

<div id="methodsandparams-AudioContext-section" class="section">
<h3 id="methodsandparams-AudioContext">4.1.2. メソッドとパラメータ <button onclick="OpenOriginal('methodsandparams-AudioContext')">原文</button></h3>
<dl>
  <dt id="dfn-createBuffer"><code>createBuffer</code> メソッド</dt>
    <dd><p>与えられたサイズのAudioBufferを作成します。バッファ内のデータは0(無音)で初期化されます。
      もし、<code>numberOfChannels</code>または<code>sampleRate</code>が範囲外の場合、またはlengthが0の場合、NOT_SUPPORTED_ERR例外が発生しなくてはなりません(MUST)。</p>
      <p><dfn id="dfn-numberOfChannels">numberOfChannels</dfn>パラメータはバッファが持つチャンネル数を指定します。
        実装は少なくとも32チャンネルをサポートしなくてはなりません。</p>
      <p><dfn id="dfn-length">length</dfn>パラメータはバッファのサイズをサンプルフレーム数で指定します。</p>
      <p><dfn id="dfn-sampleRate_2">sampleRate</dfn>パラメータはバッファ内のリニアPCMオーディオデータのサンプルレートを秒あたりのサンプルフレーム数で表します。
        実装は少なくとも22050から96000の範囲をサポートしなくてはなりません。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-decodeAudioData"><code>decodeAudioData</code> メソッド</dt>
    <dd><p>ArrayBuffer内にあるオーディオファイルのデータを非同期にデコードします。
      ArrayBufferは、例えばXMLHttpRequestで<code>responseType</code>に"arraybuffer"を指定した場合の
      <code>response</code>属性としてロードされます。
      オーディオファイルデータは<code>audio</code>要素でサポートされるどのフォーマットでも構いません。</p>
      <p><dfn id="dfn-audioData">audioData</dfn>はオーディオファイルのデータを含むArrayBufferです。</p>
      <p><dfn id="dfn-successCallback">successCallback</dfn>は、デコードが終了した時に呼び出されるコールバック関数です。
        コールバック関数の引数はデコードされたPCMオーディオデータをあらわすAudioBufferになります。</p>
      <p><dfn id="dfn-errorCallback">errorCallback</dfn>はオーディオファイルデータをデコード中にエラーが起こった場合に呼び出されるコールバック関数です。</p>

      <p>
      次のステップが実行されなくてはなりません:
      </p>
      <ol>

      <li>JavaScriptからアクセスや変更ができないように<dfn>audioData</dfn>のArrayBufferを一時的に中立化します。</li>
      <li>デコード処理が別のスレッドで実行されるようにキューに登録します。</li>
      <li>デコードスレッドが、エンコードされている<dfn>audioData</dfn>からリニアPCMへのデコードを試みます。
        もしオーディオフォーマットが認識できない、あるいはサポートされていない、あるいはデータが壊れている/想定していない/矛盾しているなどのためデコードエラーが発生した場合、
        <dfn>audioData</dfn>の中立化状態は解除され、メインスレッドのイベントループで<dfn>errorCallback</dfn>関数を呼び出すようにスケジュールされてこのステップを終了します。</li>
      <li>デコードスレッドがリニアPCMオーディオデータのデコード結果を得て、その<dfn>audioData</dfn>のサンプルレートがもし異なっている場合、AudioContextのサンプルレートにリサンプリングされます。
        最終的な結果(必要ならサンプルレート変換の後)がAudioBufferに格納されます。
      </li>
      <li><dfn>audioData</dfn>の中立化状態が解除されます。</li>
      <li>ステップ(4)のAudioBufferを引数として<dfn>successCallback</dfn>関数がメインスレッドのイベントループから呼び出されるようにスケジュールされます。</li>
      </ol>
    </dd>
</dl>
<dl>
  <dt id="dfn-createBufferSource"><code>createBufferSource</code>メソッド</dt>
    <dd><p><a href="#AudioBufferSourceNode-section"><code>AudioBufferSourceNode</code></a>を作成します。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-createMediaElementSource"><code>createMediaElementSource</code>メソッド</dt>
    <dd><p>与えられたHTMLMediaElementから<a href="#MediaElementAudioSourceNode-section"><code>MediaElementAudioSourceNode</code></a>を作成します。
      このメソッドを呼び出すとHTMLMediaElementからのオーディオの再生はAudioContextの処理グラフ内で処理されるようにルートを再設定されます。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-createMediaStreamSource"><code>createMediaStreamSource</code>メソッド</dt>
    <dd><p>与えられたMediaStreamから<a href="#MediaStreamAudioSourceNode-section"><code>MediaStreamAudioSourceNode</code></a>を作成します。
      このメソッドを呼ぶとMediaStreamからのオーディオの再生はAudioContextの処理グラフ内で処理されるようにルートを再設定されます。</p>
    </dd>
</dl>

<dl>
  <dt id="dfn-createMediaStreamDestination"><code>createMediaStreamDestination</code>メソッド</dt>
    <dd><p><a href="#MediaStreamAudioDestinationNode-section"><code>MediaStreamAudioDestinationNode</code></a>を作成します。</p>
    </dd>
</dl>

<dl>
  <dt id="dfn-createScriptProcessor"><code>createScriptProcessor</code>メソッド</dt>
    <dd><p>JavaScriptによるオーディオデータ直接処理のための<a href="#ScriptProcessorNode"><code>ScriptProcessorNode</code></a>を作成します。
      もし、<code>bufferSize</code>または<code>numberOfInputChannels</code>または<code>numberOfOutputChannels</code>が範囲外の場合、INDEX_SIZE_ERR例外が発生しなくてはなりません(MUST)。</p>
      <p><dfn id="dfn-bufferSize">bufferSize</dfn>パラメータはサンプルフレーム数でバッファのサイズを指定します。
        もしそれが渡されない場合、または値が0である場合、実装はノードのライフタイムを通して一定な、動作環境に最適な2の累乗のバッファサイズを選択します。
        それ以外の場合、明示的にバッファサイズを指定します。それは次の値のどれかでなければなりません:
        256、512、1024、2048、4096、8192、16384。
        この値は<code>audioprocess</code>イベントが発生する頻度とそれぞれの呼び出しでどれだけのサンプルフレームを処理する必要があるかを制御します。
        <code>bufferSize</code>が小さい値ならば<a href="#Latency-section">レイテンシー</a>は低く(良く)なります。
        オーディオが途切れ、<a href="#Glitching-section">グリッジ</a>が発生する事を避けるには大きな値が必要となります。
        レイテンシーとオーディオ品質の間のバランスを取るためには、プログラマはこのバッファサイズを指定せず、実装に最適なバッファサイズを選択させる事が推奨されます。
      </p>
      <p><dfn id="dfn-numberOfInputChannels">numberOfInputChannels</dfn>パラメータ (デフォルトは2) はこのノードの入力チャンネル数を指定します。
        32チャンネルまでがサポートされなくてはなりません。</p>
      <p><dfn id="dfn-numberOfOutputChannels">numberOfOutputChannels</dfn>パラメータ (デフォルトは2) はこのノードの出力チャンネル数を指定します。
        32チャンネルまでがサポートされなくてはなりません。</p>
      <p><code>numberOfInputChannels</code> と <code>numberOfOutputChannels</code>の両方が0になってはいけません。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-createAnalyser"><code>createAnalyser</code>メソッド</dt>
    <dd><p><a href="#AnalyserNode-section"><code>AnalyserNode</code></a>を作成します。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-createGain"><code>createGain</code>メソッド</dt>
    <dd><p><a href="#GainNode-section"><code>GainNode</code></a>を作成します。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-createDelay"><code>createDelay</code>メソッド</dt>
    <dd><p>様々な遅延機能を表す<a href="#DelayNode-section"><code>DelayNode</code></a>を作成します。
      初期化時のデフォルト遅延時間は0秒です。</p>
      <p><dfn id="dfn-maxDelayTime">maxDelayTime</dfn>パラメータはオプションであり、その遅延機能の遅延時間の最大値を秒で指定します。
        もし指定する場合は、その値は0よりも大きく3分よりも小さくなければなりません(MUST)。そうでなければ NOT_SUPPORTED_ERR例外が発生しなくてはなりません(MUST)。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-createBiquadFilter"><code>createBiquadFilter</code>メソッド</dt>
    <dd><p>幾つかのタイプのフィルタに設定可能な2次フィルタを表す<a href="#BiquadFilterNode-section"><code>BiquadFilterNode</code></a>を作成します。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-createWaveShaper"><code>createWaveShaper</code>メソッド</dt>
    <dd><p>非線形な歪み効果を表す<a href="#WaveShaperNode-section"><code>WaveShaperNode</code></a>を作成します。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-createPanner"><code>createPanner</code>メソッド</dt>
    <dd><p><a href="#PannerNode-section"><code>PannerNode</code></a>を作成します。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-createConvolver"><code>createConvolver</code>メソッド</dt>
    <dd><p><a href="#ConvolverNode-section"><code>ConvolverNode</code></a>を作成します。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-createChannelSplitter"><code>createChannelSplitter</code>メソッド</dt>
    <dd><p>チャンネル分割器を表す<a href="#ChannelSplitterNode-section"><code>ChannelSplitterNode</code></a>を作成します。
      パラメータの値が無効の場合、INDEX_SIZE_ERR例外が発生しなくてはなりません(MUST)。</p>
      <p><dfn id="dfn-numberOfOutputs">numberOfOutputs</dfn>パラメータは出力の数を指定し、32までサポートされなくてはなりません。
        もし指定されない場合は6となります。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-createChannelMerger"><code>createChannelMerger</code>メソッド</dt>
    <dd><p>チャンネル結合器を表す<a href="#ChannelMergerNode-section"><code>ChannelMergerNode</code></a>を作成します。
      パラメータの値が無効の場合、INDEX_SIZE_ERR例外が発生しなくてはなりません(MUST)。</p>
      <p><dfn id="dfn-numberOfInputs">numberOfInputs</dfn>パラメータは入力の数を指定し、32までサポートされなくてはなりません。
        もし指定されない場合は6となります。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-createDynamicsCompressor"><code>createDynamicsCompressor</code>メソッド</dt>
    <dd><p><a href="#DynamicsCompressorNode-section"><code>DynamicsCompressorNode</code></a>を作成します。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-createOscillator"><code>createOscillator</code>メソッド</dt>
    <dd><p><a href="#OscillatorNode-section"><code>OscillatorNode</code></a>を作成します。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-createPeriodicWave"><code>createPeriodicWave</code>メソッド</dt>
    <dd><p>任意の倍音構成を表す<a href="#PeriodicWave-section"><code>PeriodicWave</code></a>を作成します。
      <code>real</code> および <code>imag</code>パラメータは<code>Float32Array</code>型で0より大きく4096以下の同じ長さでなくてはなりません。
      そうでない場合、INDEX_SIZE_ERR例外が発生しなくてはなりません(MUST)。
      これらのパラメータは任意の周期波形を表す<a href="http://en.wikipedia.org/wiki/Fourier_series">フーリエ級数</a>の係数の一部分を表します。
      作成されたPeriodicWaveは<a href="#OscillatorNode-section"><code>OscillatorNode</code></a>と共に使用され、
      絶対値の最大が1となる<em>正規化</em>された時間領域の波形を表現します。
      別の言い方をするとこれは<a href="#OscillatorNode-section"><code>OscillatorNode</code></a>が発生する波形の最大ピークが0dBFSであるという事です。
      これは都合よくWeb Audio APIで使用される最大振幅の信号に対応します。
      PeriodicWaveは作成時に正規化されるため、<code>real</code>および <code>imag</code>パラメータは相対値を表します。
      </p>
      <p><dfn id="dfn-real">real</dfn>パラメータは<code>コサイン</code>項(慣習的な言い方でA項)の配列を表します。
      オーディオの用語では最初の要素(インデックス0)は周期波形のDCオフセットであり、通常は0に設定されます。
      2番目の要素(インデックス1)は基本周波数を表します。3番目の要素は最初の倍音を表し、それ以降も同様に続いてゆきます。</p>
      <p><dfn id="dfn-imag">imag</dfn>パラメータは<code>サイン</code>項(慣習的な言い方でB項)の配列を表します。
      最初の要素(インデックス0)はフーリエ級数には存在しないため、0でなくてはなりません(これは無視されます)。
      2番目の要素(インデックス1)は基本周波数を表します。3番目の要素は最初の倍音を表し、それ以降も同様に続いてゆきます。</p>
    </dd>
</dl>
</div>

<h3 id="lifetime-AudioContext">4.1.3. ライフタイム <button onclick="OpenOriginal('lifetime-AudioContext')">原文</button></h3>
<p class="norm">このセクションは参考情報です。</p>

<p>
<code>AudioContext</code>は一度作成されると、再生される音がなくなるかそのページを離れるまで音の再生を続けます。
</p>
</div>

<div id="OfflineAudioContext-section-section" class="section">
<h2 id="OfflineAudioContext-section">4.1b. OfflineAudioContextインターフェース <button onclick="OpenOriginal('OfflineAudioContext-section')">原文</button></h2>
<p>
OfflineAudioContextは特殊なAudioContextであり、レンダリング/ミキシングで使用され、(可能性としては)リアルタイムよりも高速に動作します。
それはオーディオハードウェアに出力しない代わりに可能な限り高速に動作して、終了時にイベントハンドラを結果を格納したAudioBufferと共に呼び出します。
</p>

<p>
</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="offline-audio-context-idl">
[Constructor(unsigned long numberOfChannels, unsigned long length, float sampleRate)]
interface <dfn id="dfn-OfflineAudioContext">OfflineAudioContext</dfn> : AudioContext {

    void startRendering();

    attribute EventHandler oncomplete;

};
</code></pre>
</div>
</div>

<div id="attributes-OfflineAudioContext-section" class="section">
<h3 id="attributes-OfflineAudioContext">4.1b.1. 属性 <button onclick="OpenOriginal('attributes-OfflineAudioContext')">原文</button></h3>
<dl>
  <dt id="dfn-oncomplete"><code>oncomplete</code></dt>
    <dd><p><a href="#OfflineAudioCompletionEvent-section">OfflineAudioCompletionEvent</a>型のイベントハンドラです。</p>
    </dd>
</dl>
</div>


<div id="methodsandparams-OfflineAudioContext-section" class="section">
<h3 id="methodsandparams-OfflineAudioContext">4.1b.2. メソッドとパラメータ <button onclick="OpenOriginal('methodsandparams-OfflineAudioContext')">原文</button></h3>
<dl>
  <dt id="dfn-startRendering"><code>startRendering</code>メソッド</dt>
    <dd><p>
      現在の接続とスケジュールされたパラメータ変化を与えて、オーディオのレンダリングを開始します。
      レンダリング終了時に<code>oncomplete</code>ハンドラが一度だけ呼び出されます。
      このメソッドを呼び出すのは一度だけでなくてはなりません。
      そうでない場合、INVALID_STATE_ERR例外が発生します(MUST)。</p>
    </dd>
</dl>
</div>
</div>

<div id="OfflineAudioCompletionEvent-section" class="section">
<h2 id="OfflineAudioCompletionEvent">4.1c. OfflineAudioCompletionEventインターフェース <button onclick="OpenOriginal('OfflineAudioCompletionEvent')">原文</button></h2>

<p>これは<a href="#OfflineAudioContext-section"><code>OfflineAudioContext</code></a>にディスパッチされる
  <code>Event</code>オブジェクトです。
</p>


<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="offline-audio-completion-event-idl">

interface <dfn id="dfn-OfflineAudioCompletionEvent">OfflineAudioCompletionEvent</dfn> : Event {

    readonly attribute AudioBuffer renderedBuffer;

};
</code></pre>
</div>
</div>

<div id="attributes-OfflineAudioCompletionEvent-section" class="section">
<h3 id="attributes-OfflineAudioCompletionEvent">4.1c.1. 属性 <button onclick="OpenOriginal('attributes-OfflineAudioCompletionEvent')">原文</button></h3>
<dl>
  <dt id="dfn-renderedBuffer"><code>renderedBuffer</code></dt>
    <dd><p>OfflineAudioContextがレンダリングを完了した後の、レンダリングされたオーディオデータを含むAudioBufferです。
      OfflineAudioContextのコンストラクタの<code>numberOfChannels</code>パラメータに等しいチャンネル数を持ちます。</p>
    </dd>
</dl>
</div>
</div>

<div id="AudioNode-section-section" class="section">
<h2 id="AudioNode-section">4.2. AudioNode インターフェース <button onclick="OpenOriginal('AudioNode-section')">原文</button></h2>

<p>AudioNodeは<a href="#AudioContext-section"><code>AudioContext</code></a>の構成ブロックです。
  このインターフェースはオーディオのソース、出力先、中間の処理モジュールを表しています。これらのモジュールは互いに接続されて、
  音をオーディオハードウェアに出力するための<a href="#ModularRouting-section">処理グラフ</a>を形成します。
  それぞれのノードは<dfn>入力</dfn>や<dfn>出力</dfn>を持つ事ができます。
  <dfn>ソースノード</dfn>は入力を持たず、単一の出力を持ちます。
  <a href="#AudioDestinationNode-section"><code>AudioDestinationNode</code></a>は
  単一の入力を持ち、出力を持たず、オーディオハードウェアへの最終的な出力地点を表しています。
  それぞれのタイプの<code>AudioNode</code>はどのようにオーディオを処理したり合成したりするかの詳細が異なっています。
  しかし、一般的に<code>AudioNode</code>は(持っていれば)その入力を処理し、(持っていれば)その出力にオーディオ信号を送り出します。
 </p>

<p>それぞれの<dfn>出力</dfn>は1つ以上の<dfn>チャンネル</dfn>を持っています。正確なチャンネル数はそれぞれのAudioNodeの詳細に依存します。
</p>

<p>出力は1つ以上の<code>AudioNode</code>入力に接続でき、つまり<em>ファンアウト</em>がサポートされています。
入力は初期化時には接続されていません。しかし、1つ以上の<code>AudioNode</code>出力から接続する事ができ、即ち、<em>ファンイン</em>がサポートされています。
AudioNodeの出力をAudioNodeの入力に接続するため<code>connect()</code>メソッドが呼ばれた時、それを、その入力への<dfn>接続</dfn>と呼びます。
</p>

<p>各AudioNodeの<dfn>入力</dfn>はその時々で特定のチャンネル数を持ちます。この数はその入力への<dfn>接続</dfn>によって変化します。
  もし入力が接続を持っていない場合、チャンネル数は1で無音となります。
</p>

<p><code>AudioNode</code>はそれぞれの<dfn>入力</dfn>について、その入力への接続のミックス(通常はアップミックス)を行います。
  この詳細に関して参考情報としては<a href="#MixerGainStructure-section">ミキサーゲイン構成</a>、基準としての要件については<a href="#UpMix-section">チャンネルのアップミックスとダウンミックス</a>を参照してください。
</p>

<p>パフォーマンス的な理由から実際の実装ではそれぞれの<code>AudioNode</code>は決まった<em>ブロックサイズ</em>のサンプルフレームをブロック単位に処理する事が必要になると思われます。
  実装による振る舞いを統一するため、この値を明示的に定めます。<em>ブロックサイズ</em>はサンプルレート44.1kHzにおいて約3msとなる128サンプルフレームとします。
</p>

<p>AudioNodeは、<cite><a href="http://dom.spec.whatwg.org/">DOM</a></cite><a href="#DOM">[DOM]</a>で記述される<em>EventTarget</em>です。
  これは他のEventTargetがイベントを受け取るのと同じ方法でイベントをAudioNodeにディスパッチする事が可能である事を意味します。
</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="audio-node-idl">

enum <dfn>ChannelCountMode</dfn> {
    "max",
    "clamped-max",
    "explicit"
};

enum <dfn>ChannelInterpretation</dfn> {
    "speakers",
    "discrete"
};

interface <dfn id="dfn-AudioNode">AudioNode</dfn> : EventTarget {

    void connect(AudioNode destination, optional unsigned long output = 0, optional unsigned long input = 0);
    void connect(AudioParam destination, optional unsigned long output = 0);
    void disconnect(optional unsigned long output = 0);

    readonly attribute AudioContext context;
    readonly attribute unsigned long numberOfInputs;
    readonly attribute unsigned long numberOfOutputs;

    // Channel up-mixing and down-mixing rules for all inputs.
    attribute unsigned long channelCount;
    attribute ChannelCountMode channelCountMode;
    attribute ChannelInterpretation channelInterpretation;

};
</code></pre>
</div>
</div>

<div id="attributes-AudioNode-section" class="section">
<h3 id="attributes-AudioNode">4.2.1. 属性 <button onclick="OpenOriginal('attributes-AudioNode')">原文</button></h3>
<dl>
  <dt id="dfn-context"><code>context</code></dt>
    <dd><p>このAudioNodeを所有するAudioContextです。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-numberOfInputs_2"><code>numberOfInputs</code></dt>
    <dd><p>このAudioNodeに信号を供給する入力の数です。<dfn>ソースノード</dfn>ではこれは0となります。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-numberOfOutputs_2"><code>numberOfOutputs</code></dt>
    <dd><p>このAudioNodeから出てゆく出力の数です。AudioDestinationNodeではこれは0となります。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-channelCount"><code>channelCount</code></dt>
    <dd><p>チャンネル数です。このノードの入力の接続におけるアップミックスおよびダウンミックスで使用されます。値が別途定められている特定のノードを除いて、デフォルトは2です。
      この属性は入力を持たないノードでは意味を持ちません。
      もしこの値が0にセットされると、実装はNOT_SUPPORTED_ERR例外を発生しなくてはなりません(MUST)。</p>
      <p>この属性の詳細ついては<a href="#UpMix-section">チャンネルのアップミックスとダウンミックス</a>セクションを参照してください。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-channelCountMode"><code>channelCountMode</code></dt>
    <dd><p>このノードの入力のアップミックスおよびダウンミックス時のチャンネル数の数え方を決定します。
      この属性は入力を持たないノードでは意味を持ちません。</p>
      <p>この属性の詳細ついては<a href="#UpMix-section">チャンネルのアップミックスとダウンミックス</a>セクションを参照してください。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-channelInterpretation"><code>channelInterpretation</code></dt>
    <dd><p>このノードのアップミックスおよびダウンミックス時に各チャンネルをどのように扱うかを決定します。
      この属性は入力を持たないノードでは意味を持ちません。</p>
      <p>この属性の詳細ついては<a href="#UpMix-section">チャンネルのアップミックスとダウンミックス</a>セクションを参照してください。</p>
    </dd>
</dl>
</div>

<div id="methodsandparams-AudioNode-section" class="section">
<h3 id="methodsandparams-AudioNode">4.2.2. メソッドとパラメータ <button onclick="OpenOriginal('methodsandparams-AudioNode')">原文</button></h3>
<dl>
  <dt id="dfn-connect-AudioNode">AudioNodeへの<code>connect</code>メソッド</dt>
    <dd><p>AudioNodeを他のAudioNodeに接続します。</p>
      <p><dfn id="dfn-destination_2">destination</dfn>パラメータは接続先となるAudioNodeです。</p>
      <p><dfn id="dfn-output_2">output</dfn>パラメータはAudioNodeのどの出力を接続するかを指定するインデックスです。
        もしこのパラメータが範囲外の場合、INDEX_SIZE_ERR例外が発生しなくてはなりません(MUST)。</p>
      <p><dfn id="dfn-input_2">input</dfn>パラメータは接続先のAudioNodeのどの入力に接続するかを指定するインデックスです。
        もしこのパラメータが範囲外の場合INDEX_SIZE_ERR例外が発生しなくてはなりません(MUST)。</p>
      <p>connect()を複数回呼び出して、AudioNodeの出力を複数の入力に接続する事が可能です。
        つまり、"ファンアウト"がサポートされています。</p>
      <p>AudioNodeを他のAudioNodeに接続して<em>循環</em>を作る事が可能です。
        別の言い方をすれば、AudioNodeを他のAudioNodeに接続してゆき、最初のAudioNodeに戻って来ても構いません。
        ただしこれは<em>循環</em>の中に少なくとも1つの<a class="dfnref" href="#DelayNode-section">DelayNode</a>を
        含んでいる場合のみ可能です。そうでない場合、NOT_SUPPORTED_ERR例外が発生しなくてはなりません(MUST)。
      </p>

      <p>特定のノードの出力と特定のノードの入力の間には接続は1つのみ存在できます。
        同じ終端点を持つ複数の接続は無視されます。例えば:
      </p>

      <pre>
      nodeA.connect(nodeB);
      nodeA.connect(nodeB);

      は次と同じ効果を持ちます

      nodeA.connect(nodeB);
      </pre>

    </dd>
</dl>
<dl>
  <dt id="dfn-connect-AudioParam">AudioParamへの<code>connect</code>メソッド</dt>
    <dd><p>AudioNodeをAudioParamに接続し、パラメータの値をオーディオ信号レートで制御します。
      </p>

      <p><dfn id="dfn-destination_3">destination</dfn>パラメータは接続先のAudioParamです。
      </p>
      <p><dfn id="dfn-output_3-destination">output</dfn>パラメータはAudioNodeのどの出力から接続するかを指定します。
        もしこのパラメータが範囲外の場合、INDEX_SIZE_ERR例外が発生しなくてはなりません(MUST)。
      </p>

      <p>connect()を複数回呼び出して、AudioNodeの出力を複数のAudioParamに接続する事が可能です。
        即ち、"ファンアウト"をサポートしています。
      </p>
      <p>connect()を複数回呼び出して、複数のAudioNodeの出力を1つのAudioParamに接続する事が可能です。
        即ち、"ファンイン"をサポートしています。
      </p>
      <p>AudioParamはそれに接続されているAudioNodeの出力からオーディオデータを取り出し、それがモノラルでなければ、ダウンミックスによって<a href="#down-mix">モノラルに変換</a>します。
        そして接続されている各出力をミックスし、更に最終的にパラメータが持っているタイムラインスケジュールを含む<em>固有値</em>(AudioParamに何も接続されていない状態での<code>値</code>)とミックスします。
      </p>

      <p>特定のノードの出力と特定のAudioParamの間の接続は1つのみ存在できます。
        同じ終端点を持つ複数の接続は無視されます。例えば:
      </p>

      <pre>
      nodeA.connect(param);
      nodeA.connect(param);

      は次と同じ効果を持ちます

      nodeA.connect(param);
      </pre>

    </dd>
</dl>
<dl>
  <dt id="dfn-disconnect"><code>disconnect</code> メソッド</dt>
    <dd><p>AudioNodeの出力からの接続を切断します。</p>
      <p><dfn id="dfn-output_3-disconnect">output</dfn>パラメータは接続を切るAudioNodeの出力のインデックスです。
        もしこのoutputパラメータが範囲外の場合、INDEX_SIZE_ERR例外が発生しなくてはなりません(MUST)。</p>
    </dd>
</dl>
</div>
<div id="lifetime-AudioNode-section" class="section">
<h3 id="lifetime-AudioNode">4.2.3. ライフタイム <button onclick="OpenOriginal('lifetime-AudioNode')">原文</button></h3>

<p class="norm">このセクションは参考情報です。</p>

<p>実装では、必要のないリソース利用や、未使用または終了しているノードが際限なくメモリを使うのを避ける為の何らかの方法を選ぶでしょう。
以降の説明は、一般的に期待されるノードのライフタイムの管理の仕方を案内するものです。
</p>

<p>何かしら参照される限り、<code>AudioNode</code>は存在し続けるでしょう。参照には何種類かのタイプがあります。:
</p>

<ol>
<li><em>通常</em>のJavaScript参照。通常のガベージコレクションのルールに従います。</li>
<li><code>AudioBufferSourceNodes</code>と<code>OscillatorNodes</code>の<em>再生中</em>の参照。
  再生している間これらのノードは、自分自身への<em>再生中</em>参照を維持します。</li>
<li><em>接続</em>参照。別の<code>AudioNode</code>から接続されている場合に発生します。</li>
<li><em>余韻時間</em>参照。何かしらの内部プロセスで未解放のステートになっている場合は、その間、自分自身を維持します。
  例えば、<code>ConvolverNode</code>は無音の入力を受けた後でも再生し続ける余韻を持ちます。
  (大きなコンサートホールで柏手を打つと、ホール中に響きわたる音が聞こえるのをイメージして下さい)。
  いくつかの<code>AudioNode</code>は、この特性を持ちます。
  各ノードの詳細を見てください。
</li>
</ol>

<p>環状に接続され、<em>且つ</em>直接または間接的に<code>AudioContext</code>の<code>AudioDestinationNode</code>に接続されるすべての<code>AudioNode</code>は
  <code>AudioContext</code>が存在している間は存在したままになります。
</p>

<p>参照を全く持たなくなった時、<code>AudioNode</code>は削除されます。 しかしそれが削除される前に、他の<code>AudioNode</code>から自分への接続を切断します。
  同様にこのノードから他のノードに対するすべての接続参照(3)を解放します。
</p>

<p>上記の参照の種類に関わらず、ノードの<code>AudioContext</code>が削除された時には、もちろん<code>AudioNode</code>は削除されます。
</p>
</div>
</div>

<div id="AudioDestinationNode-section" class="section">
<h2 id="AudioDestinationNode">4.4. AudioDestinationNode インターフェース <button onclick="OpenOriginal('AudioDestinationNode')">原文</button></h2>

<p>これはユーザーが聴く事になる最終的な音の出力地点を表す<a href="#AudioNode-section"><code>AudioNode</code></a>です。
  それは多くの場合、スピーカーが接続されているオーディオ出力デバイスと考えられます。
  聴こえるべきすべてのレンダリングされた音はAudioContextのルーティンググラフの"終端点"であるこのノードに導かれます。
  AudioDestinationNodeは1つのAudioContextに付き、
  <a href="#AudioContext-section"><code>AudioContext</code></a>の<code>destination</code>属性を介して
  1つだけ存在します。
</p>
<pre>
      numberOfInputs  : 1
      numberOfOutputs : 0

      channelCount = 2;
      channelCountMode = "explicit";
      channelInterpretation = "speakers";
</pre>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="audio-destination-node-idl">

interface <dfn id="dfn-AudioDestinationNode">AudioDestinationNode</dfn> : AudioNode {

    readonly attribute unsigned long maxChannelCount;

};
</code></pre>
</div>
</div>

<div id="attributes-AudioDestinationNode-section" class="section">
<h3 id="attributes-AudioDestinationNode">4.4.1. 属性 <button onclick="OpenOriginal('attributes-AudioDestinationNode')">原文</button></h3>
<dl>
  <dt id="dfn-maxChannelCount"><code>maxChannelCount</code></dt>
    <dd><p><code>channelCount</code>属性で設定できるチャンネル数の最大値です。
      <code>AudioDestinationNode</code>は(通常は)オーディオハードウェアの終端点を表し、マルチチャンネル対応のハードウェアならば2チャンネル以上の音を出力する事ができます。
      <code>maxChannelCount</code>はハードウェアがサポートできるチャンネル数の最大値です。
      もしこの値が0ならば<code>channelCount</code>は変更できない事を表します。
      これは<code>OfflineAudioContext</code>の<code>AudioDestinationNode</code>やハードウェアの基本実装がステレオ出力のみに対応している場合です。</p>

    <p>通常のAudioContextのdestinationでは、<code>channelCount</code>のデフォルト値は2であり、0以外で<code>maxChannelCount</code>と同じか小さい値に設定する事ができます。
      値が有効な範囲外の場合、INDEX_SIZE_ERR例外が発生しなくてはなりません(MUST)。
      具体的な例をあげれば、もしオーディオハードウェアが8チャンネル出力をサポートしている場合、<code>numberOfChannels</code>を8に設定でき、8チャンネルの出力が得られます。
    </p>

    <p>OfflineAudioContextのAudioDestinationNodeについては、<code>channelCount</code>はOfflineAudioContextが作成された時点で決定され、この値は変更できません。
    </p>

    </dd>
</dl>

</div>
</div>

<div id="AudioParam-section" class="section">
<h2 id="AudioParam">4.5. AudioParamインターフェース <button onclick="OpenOriginal('AudioParam')">原文</button></h2>

<p>AudioParam は<a href="#AudioNode-section"><code>AudioNode</code></a>の例えば音量のような個別の機能をコントロールします。
パラメータは<code>value</code>属性を使って特定の値に即時にセットする事ができます。
あるいは(AudioContext.currentTimeと連動した)非常に高い時間精度で値の変化のスケジュールを組む事ができ、エンベロープ、音量のフェード、LFO、フィルタスイープ、グレイン窓、などに応用する事ができます。
このような方法で任意のタイムラインベースのオートメーション曲線をすべてのAudioParamに対して設定する事が可能です。
また更に、<code>AudioNode</code>からの出力の音声信号を<code>AudioParam</code>に接続する事ができ、<em>個別</em>に持っているパラメータの値に加算する事ができます。</p>

<p>幾つかの合成や処理の<code>AudioNode</code>は、オーディオサンプル単位で反映されなくてはならない<code>AudioParam</code>型の属性を持っています。
その他の<code>AudioParam</code>はサンプル単位の精度は重要ではなく、その値の変化はより粗く取り込まれます。
それぞれの<code>AudioParam</code>は<em>a-rate</em>パラメータつまりサンプル単位で反映されるか、それ以外の<em>k-rate</em>パラメータかが指定されます。
</p>

<p>実装はそれぞれの<code>AudioNode</code>について、128サンプルフレーム毎のブロック単位の処理を行わなくてはなりません。
</p>

<p>それぞれの128サンプルフレームのブロックに対して、<em>k-rate</em>パラメータは最初のサンプルのタイミングで取り込まれ、その値は
  ブロック全体に対して使用されなくてはなりません。<em>a-rate</em>パラメータはブロック内のサンプルフレーム毎に取り込まれなくてはなりません。
</p>


<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="audio-param-idl">

interface <dfn id="dfn-AudioParam">AudioParam</dfn> {

    attribute float value;
    readonly attribute float defaultValue;

    <span class="comment">// Parameter automation. </span>
    void setValueAtTime(float value, double startTime);
    void linearRampToValueAtTime(float value, double endTime);
    void exponentialRampToValueAtTime(float value, double endTime);

    <span class="comment">// Exponentially approach the target value with a rate having the given time constant. </span>
    void setTargetAtTime(float target, double startTime, double timeConstant);

    <span class="comment">// Sets an array of arbitrary parameter values starting at time for the given duration. </span>
    <span class="comment">// The number of values will be scaled to fit into the desired duration. </span>
    void setValueCurveAtTime(Float32Array values, double startTime, double duration);

    <span class="comment">// Cancels all scheduled parameter changes with times greater than or equal to startTime. </span>
    void cancelScheduledValues(double startTime);

};
</code></pre>
</div>
</div>



<div id="attributes-AudioParam-section" class="section">
<h3 id="attributes-AudioParam">4.5.1. 属性 <button onclick="OpenOriginal('attributes-AudioParam')">原文</button></h3>

<dl>
  <dt id="dfn-value"><code>value</code></dt>
    <dd><p>浮動小数のパラメータの値です。
      この属性の初期値は<code>defaultValue</code>です。
      もし<code>value</code>がオートメーションイベントが設定されている期間中に設定された場合、それは無視され、例外は発生しません。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-defaultValue"><code>defaultValue</code></dt>
    <dd><p><code>value</code>属性の初期値です。</p>
    </dd>
</dl>
</div>

<div id="methodsandparams-AudioParam-section" class="section">
<h3 id="methodsandparams-AudioParam">4.5.2. メソッドとパラメータ <button onclick="OpenOriginal('methodsandparams-AudioParam')">原文</button></h3>

<p><code>AudioParam</code>は時間軸に沿ったイベントリストを保持し、その初期値は空になっています。
  この時間はAudioContext.currentTimeによる時間軸を使用します。
  イベントは時刻に対して値を割り付けるものです。
  次のメソッドはそれぞれ対応する時刻を持っており、そのイベントは上記のイベントリストに保持されます。
  これらのメソッドは<em>オートメーション</em>メソッドと呼ばれます:
</p>

<ul>
<li>setValueAtTime() - <em>SetValue</em></li>
<li>linearRampToValueAtTime() - <em>LinearRampToValue</em></li>
<li>exponentialRampToValueAtTime() - <em>ExponentialRampToValue</em></li>
<li>setTargetAtTime() - <em>SetTarget</em></li>
<li>setValueCurveAtTime() - <em>SetValueCurve</em></li>
</ul>

<p>これらのメソッドが呼ばれる時、次の規則が適用されます:
</p>
<ul>
<li>もし、これらのイベントのどれかをある時刻に追加しようとした時、まったく同じ型のイベントが同じ時刻に既に存在している場合、新しいイベントは古いイベントを置き換えます。</li>
<li>もし、これらのイベントのどれかをある時刻に追加しようとした時、同じ時刻に異なる型のイベントが1つ以上存在している場合、新しいイベントはそれら既にあるイベントの後ろで、時刻がより後ろのイベントの前に追加されます。</li>
<li>もし、setValueCurveAtTime()がtime T、duration Dで呼ばれた時、Tより後ろでT+Dより手前に何らかのイベントが既に存在している場合、NOT_SUPPORTED_ERR例外が発生しなくてはなりません(MUST)。
  別の言い方をすると、他のイベントを含んだ期間の値のカーブをスケジュールする事はできません。</li>
<li>同様に、<em>SetValueCurve</em>イベントのtime Tとduration Dで示される期間内の時刻を指定して何らかの<em>オートメーション</em>メソッドを呼んだ場合、NOT_SUPPORTED_ERR例外が発生しなくてはなりません(MUST)。</li>
</ul>
<p>
</p>

<dl>
  <dt id="dfn-setValueAtTime"><code>setValueAtTime</code> メソッド</dt>
    <dd><p>指定の時刻になるとパラメータ値を変更するようにスケジュールします。</p>
      <p><dfn id="dfn-value_2">value</dfn>パラメータは指定の時刻になると設定される値です。</p>
      <p><dfn id="dfn-startTime_2">startTime</dfn>パラメータはAudioContext.currentTimeと同じ時間軸で指定される時刻です。</p>
      <p>
        もし<em>SetValue</em>イベント以降にこれ以上イベントがない場合、 時刻 t >= startTime に対して v(t) = value となります。
        別の言い方をすると、valueは定数値ののまま保持されます。
      </p>
      <p>
        もしこの<em>SetValue</em>イベントの次のイベント(時刻T1)の型が<em>LinearRampToValue</em>または<em>ExponentialRampToValue</em>ではない場合、時刻をtとした場合:
        startTime &lt;= t &lt; T1 に対して v(t) = value となります。
        別の言い方をすると、valueはこの期間内だけ保持され、"ステップ"を作る事ができます。
      </p>
      <p>
        もしこの<em>SetValue</em>イベントの次のイベントの型が<em>LinearRampToValue</em>または<em>ExponentialRampToValue</em>である場合は次の詳細を参照してください。
      </p>
    </dd>
</dl>
<dl>
  <dt id="dfn-linearRampToValueAtTime"><code>linearRampToValueAtTime</code> メソッド</dt>
    <dd><p>前にスケジュールされているパラメータ値から指定された値まで、直線的に連続して値を変化させる事をスケジュールします。</p>
      <p><dfn id="dfn-value_3">value</dfn>は直線的に変化して指定の時刻になった時に到達する値です。</p>
      <p><dfn id="dfn-endTime_3">endTime</dfn>はAudioContext.currentTimeと同じ時間軸で表された時刻です。</p>

      <p>
        T0 &lt;= t &lt; T1の期間における値 (T0は1つ前のイベントの時刻、T1はこのメソッドのendTimeで指定される時刻)は次のように計算されます:
      </p>
      <pre>
      v(t) = V0 + (V1 - V0) * ((t - T0) / (T1 - T0))
      </pre>
      <p>
        ここでV0は時刻T0における値、V1はこのメソッドで指定された値です。
      </p>
      <p>
        もしこのLinerRamptoValueイベント以降にこれ以上イベントが無い場合、時刻 t &gt;= T1 に対して v(t) = V1 となります。
      </p>

    </dd>
</dl>
<dl>
  <dt id="dfn-exponentialRampToValueAtTime"><code>exponentialRampToValueAtTime</code> メソッド</dt>
    <dd><p>前にスケジュールされているバラメータ値から指定された値まで、指数的に連続して値を変化させる事をスケジュールします。
      フィルタの周波数や再生スピードなどのパラメータは人間の聴覚特性のため、指数的変化が適しています。
      </p>
      <p><dfn id="dfn-value_4">value</dfn>は指定された時刻に指数的変化で到達する値です。
        もし値が0かそれ以下、または前のイベントによる値が0かそれ以下の場合、NOT_SUPPORTED_ERR例外が発生しなくてはなりません(MUST)。
      </p>
      <p><dfn id="dfn-endTime_4">endTime</dfn>はAudioContext.currentTimeと同じ時間軸で表された時刻です。</p>
      <p>
        T0 &lt;= t &lt; T1の期間における値 (T0は1つ前のイベントの時刻、T1はこのメソッドのendTimeで指定される時刻)は次のように計算されます:
      </p>
      <pre>
      v(t) = V0 * (V1 / V0) ^ ((t - T0) / (T1 - T0))
      </pre>
      <p>
        ここでV0は時刻T0における値、V1はこのメソッドで指定された値です。
      </p>
      <p>
        もしこのExponentialRamptoValueイベント以降に、これ以上イベントが無い場合、時刻 t &gt;= T1 に対して v(t) = V1 となります。
      </p>
    </dd>
</dl>
<dl>
  <dt id="dfn-setTargetAtTime"><code>setTargetAtTime</code> メソッド</dt>
    <dd><p>指定の時間から、指定の時定数によって指数的に目標の値に漸近を開始します。
      様々な使い方がありますが、これはADSRエンベロープの"ディケイ"および"リリース"を実装する際に役立ちます。
      値は指定の時間に即、目標値になるのではなく徐々に変化する事に注意してください。</p>
      <p><dfn id="dfn-target">target</dfn>は与えられた時刻になると<em>開始</em>する変化の目標値を指定します。
      </p>
      <p><dfn id="dfn-startTime">startTime</dfn>はAudioContext.currentTimeと同じ時間軸の時刻です。</p>
      <p><dfn id="dfn-timeConstant">timeConstant</dfn>は目標の値に(指数的に)漸近する一次フィルタの時定数です。
        大きな値だと変化はゆっくりになります。
      </p>
      <p>
        より正確には<em>timeConstant</em>は、ステップ状の入力(0から1に変化)が入力された場合、一次の線形時不変系によって値が1 - 1/e (約63.2%) に到達するまでの時間です。
      </p>
      <p>
        <em>T0</em> &lt;= t &lt; <em>T1</em> の期間: T0を<em>startTime</em>として、T1が次のイベントの時刻とした場合(次のイベントがなければ<em>無限</em>になります):</p>
      <pre>
      v(t) = V1 + (V0 - V1) * exp(-(t - T0) / <em>timeConstant</em>)
      </pre>
      <p>ここでV0はT0(<em>startTime</em>)における初期値(.value属性)、そしてV1は<em>target</em>の値です。
      </p>
    </dd>
</dl>
<dl>
  <dt id="dfn-setValueCurveAtTime"><code>setValueCurveAtTime</code> メソッド</dt>
    <dd><p>指定の時刻と期間に対して、任意の値の配列を設定します。配列の各値は必要とされる期間に合うように割り当てられます。</p>
      <p><dfn id="dfn-values">values</dfn>は値の曲線を表すFloat32Arrayの配列です。
        これらの値は指定された時刻に開始し、指定の期間で終了するように割り当てられます。</p>
      <p><dfn id="dfn-startTime_5">startTime</dfn>はAudioContext.currentTimeと同じ時間軸を使用する時刻です。</p>
      <p><dfn id="dfn-duration_5">duration</dfn>は(<em>time</em><span class="annotate">(訳注:startTime)</span>以降の) 秒で表される期間の長さであり、
        この期間、値は<em>values</em>に従って計算されます。</p>
      <p><em>startTime</em> &lt;= t &lt; <em>startTime</em> + <em>duration</em> の期間内、値は次のように計算されます:
      </p>
      <pre>
      v(t) = values[N * (t - startTime) / duration]、 ここで、<em>N</em>は<em>values</em>配列の長さです。
      </pre>
      <p>曲線の期間の後(t >= <em>startTime</em> + <em>duration</em>)、値は(あれば)次のオートメーションイベントまで曲線の最後の値を保持します。
      </p>
    </dd>
</dl>
<dl>
  <dt id="dfn-cancelScheduledValues"><code>cancelScheduledValues</code> メソッド</dt>
    <dd><p>startTimeと同じかそれ以降にスケジュールされているすべてのパラメータ変化をキャンセルします。</p>
      <p><dfn>startTime</dfn>はそれと同じかそれ以降に設定されていたパラメータ変化のスケジュールをキャンセルする時刻です。
        これはAudioContext.currentTimeと同じ時間軸の時刻を使用します。</p>
    </dd>
</dl>
</div>


<div id="computedValue-AudioParam-section" class="section">
<h3>4.5.3. 値の計算 <button onclick="OpenOriginal('computedValue-AudioParam-section')">原文</button></h3>

<p>
<dfn>computedValue</dfn>はオーディオDSPを制御する最終的な値であり、オーディオレンダリングスレッドがそれぞれのレンダリング時刻に計算します。
内部的には次のように計算されなくてはなりません:
</p>

<ol>
<li><code>value</code>属性に直接設定されるか、あらかじめ、またはこの時刻に値の変化スケジュールが設定(オートメーションイベント)されていればこれらのイベントから<em>固有</em>の値が計算されます。
  もし、オートメーションイベントがスケジュールされた後に<code>value</code>属性が設定された場合、これらのイベントは削除されます。
  <code>value</code>属性を読みだした場合は常に現在の<em>固有</em>の値を返します。
  もしオートメーションイベントがある期間から削除された場合、<em>固有</em>の値は<code>value</code>属性が直接設定されるか、その期間を対象にオートメーションイベントが追加されるまで、直前の値を保持したままになります。
</li>

<li>AudioParamはAudioNodeの出力が接続されている場合、その出力がモノラルでなければダウンミックスして<a href="#down-mix">モノラルに変換</a>し、他の同様に接続されている出力とミックスします。
  もしAudioNodeが接続されていない場合はこの値は0となり、<em>computedValue</em>には影響を及ぼしません。
</li>

<li>
<em>computedValue</em>は<em>固有</em>の値と(2)で計算された値の和となります。
</li>

</ol>

</div>


<div id="example1-AudioParam-section" class="section">
<h3 id="example1-AudioParam">4.5.4. AudioParam オートメーションの例 <button onclick="OpenOriginal('example1-AudioParam')">原文</button></h3>



<div class="example">

<div class="exampleHeader">
例</div>
<img alt="AudioParam automation" src="images/audioparam-automation1.png" />

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">ECMAScript</span></div>

<div class="blockContent">
<pre class="code"><code class="es-code">
var t0 = 0;
var t1 = 0.1;
var t2 = 0.2;
var t3 = 0.3;
var t4 = 0.4;
var t5 = 0.6;
var t6 = 0.7;
var t7 = 1.0;

var curveLength = 44100;
var curve = new Float32Array(curveLength);
for (var i = 0; i &lt; curveLength; ++i)
    curve[i] = Math.sin(Math.PI * i / curveLength);

param.setValueAtTime(0.2, t0);
param.setValueAtTime(0.3, t1);
param.setValueAtTime(0.4, t2);
param.linearRampToValueAtTime(1, t3);
param.linearRampToValueAtTime(0.15, t4);
param.exponentialRampToValueAtTime(0.75, t5);
param.exponentialRampToValueAtTime(0.05, t6);
param.setValueCurveAtTime(curve, t6, t7 - t6);
</code></pre>
</div>
</div>
</div>
</div>
</div>

<div id="GainNode-section" class="section">
<h2 id="GainNode">4.7. GainNodeインターフェース <button onclick="OpenOriginal('GainNode')">原文</button></h2>

<p>オーディオ信号のゲインを変える事はオーディオアプリケーションでは基本的な処理です。
  <code>GainNode</code>は<a href="#MixerGainStructure-section">ミキサー</a>の構成ブロックの1つとなります。
  このインターフェースは1つの信号入力と1つの信号出力を持つAudioNodeです:</p>
<pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>

<p>これは入力のオーディオ信号を(場合によっては時間と共に変化する)<code>gain</code>属性で乗算し、結果を出力に渡します。
デフォルトではゲインは1であり、受け取った入力を変化させずにそのまま出力します。
</p>

<p>他の<code>AudioParam</code>と同じように<code>gain</code>パラメータは(AudioContext.currentTimeによる時間軸で)時間に対して浮動小数値を割り付けられます。

入力のPCMオーディオの各サンプルはそのサンプルに対応する<code>gain</code>パラメータの値で乗算されます。
この乗算された値は出力されるPCMオーディオのサンプルの値となります。
</p>

<p>出力のチャンネル数は常に入力のチャンネル数と同じになり、それぞれのチャンネルの入力は<code>gain</code>の値で乗算されて対応するチャンネルの出力となります。
</p>

<p>実装はクリックやグリッジが目立たないようにオーディオストリームに対するゲインの変更を滑らかに行わなくてはなりません。この処理は"de-zippering"と呼ばれます。</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="gain-node-idl">

interface <dfn id="dfn-GainNode">GainNode</dfn> : AudioNode {

    readonly attribute AudioParam gain;

};
</code></pre>
</div>
</div>

<div id="attributes-GainNode-section" class="section">
<h3 id="attributes-GainNode">4.7.1. 属性 <button onclick="OpenOriginal('attributes-GainNode')">原文</button></h3>
<dl>
  <dt id="dfn-gain"><code>gain</code></dt>
    <dd><p>適用されるゲインの量を表します。デフォルトの<code>value</code>は1です(ゲイン変更なし)。名目上の<code>minValue</code>は0ですが、位相反転のために負の値に設定する事もできます。
      名目上の<code>maxValue</code>は1ですが(例外が発生する事なく)より大きな値を設定する事もできます。このパラメータは<em>a-rate</em>です。</p>
    </dd>
</dl>
</div>
</div>

<div id="DelayNode-section" class="section">
<h2 id="DelayNode">4.8. DelayNodeインターフェース <button onclick="OpenOriginal('DelayNode')">原文</button></h2>

<p>ディレイ機能はオーディオアプリケーションの基本的な構成要素です。このインターフェースは単一の入力と単一の出力を持つAudioNodeです。</p>
<pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>

<p>出力のチャンネル数は常に入力のチャンネル数と同じになります。</p>

<p>これは入力されるオーディオ信号を一定の量だけ遅延させます。
  具体的には各時刻<em>t</em>において、入力信号<em>input(t)</em>に対して、遅延時間<em>delayTime(t)</em>、出力信号<em>output(t)</em>とすると
  出力は<em>output(t) = input(t - delayTime(t))</em>となります。
  デフォルトの<code>delayTime</code>は0秒(遅延なし)です。
  遅延時間が変更された場合、実装はオーディオストリームにクリックやグリッジが発生しないように変化を滑らかに行わなくてはなりません。</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="delay-node-idl">

interface <dfn id="dfn-DelayNode">DelayNode</dfn> : AudioNode {

    readonly attribute AudioParam delayTime;

};
</code></pre>
</div>
</div>

<div id="attributes-GainNode-section_2" class="section">
<h3 id="attributes-GainNode_2">4.8.1. 属性 <button onclick="OpenOriginal('attributes-GainNode_2')">原文</button></h3>
<dl>
  <dt id="dfn-delayTime_2"><code>delayTime</code></dt>
    <dd><p>遅延の量(単位は秒)を表すAudioParamオブジェクトです。
      デフォルトでは<code>value</code>は0(遅延なし)です。
      最小値は0であり、最大値は<code>AudioContext</code>の<code>createDelay</code>メソッドの引数、<code>maxDelayTime</code>で決定されます。
      このパラメータは<em>a-rate</em>です。</p>
    </dd>
</dl>
</div>
</div>

<div id="AudioBuffer-section" class="section">
<h2 id="AudioBuffer">4.9. AudioBufferインターフェース <button onclick="OpenOriginal('AudioBuffer')">原文</button></h2>

<p>このインターフェースはメモリ上に保持されるオーディオのリソース(ワンショットの音やその他の短いオーディオクリップ)を表します。
  そのフォーマットは-1 ～ +1の名目上の範囲を持つ非インターリーブのIEEE 32ビット・リニアPCMです。
  1つ以上のチャンネルを持つ事ができます。
  典型的には、そのPCMデータは適度に(通常1分以内程度に)短いと見込まれます。
  音楽の1曲分のような長時間の音に関しては<code>audio</code>要素と<code>MediaElementAudioSourceNode</code>によるストリーミングを使うべきです。</p>

<p>AudioBufferは1つ以上のAudioContextで使用される場合があります。
</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="audio-buffer-idl">

interface <dfn id="dfn-AudioBuffer">AudioBuffer</dfn> {

    readonly attribute float sampleRate;
    readonly attribute long length;

    <span class="comment">// in seconds </span>
    readonly attribute double duration;

    readonly attribute long numberOfChannels;

    Float32Array getChannelData(unsigned long channel);

};
</code></pre>
</div>
</div>

<div id="attributes-AudioBuffer-section" class="section">
<h3 id="attributes-AudioBuffer">4.9.1. 属性 <button onclick="OpenOriginal('attributes-AudioBuffer')">原文</button></h3>
<dl>
  <dt id="dfn-sampleRate_AudioBuffer"><code>sampleRate</code></dt>
    <dd><p>1秒あたりのサンプル数で表されるPCMオーディオデータのサンプルレートです。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-length_AudioBuffer"><code>length</code></dt>
    <dd><p>サンプルフレーム数で表されるPCMオーディオデータの長さです。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-duration_AudioBuffer"><code>duration</code></dt>
    <dd><p>秒で表されるPCMオーディオデータの再生時間です。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-numberOfChannels_AudioBuffer"><code>numberOfChannels</code></dt>
    <dd><p>オーディオのチャンネル数です。</p>
    </dd>
</dl>
</div>

<div id="methodsandparams-AudioBuffer-section" class="section">
<h3 id="methodsandparams-AudioBuffer">4.9.2. メソッドとパラメータ <button onclick="OpenOriginal('methodsandparams-AudioBuffer')">原文</button></h3>
<dl>
  <dt id="dfn-getChannelData"><code>getChannelData</code>メソッド</dt>
    <dd><p>指定されたチャンネルの<code>Float32Array</code>型のPCMオーディオデータを返します。
      <dfn id="dfn-channel">channel</dfn>パラメータはデータを取得するチャンネルのインデックスを指定します。
      インデックス値0は最初のチャンネルを表します。このインデックスは<code>numberOfChannels</code>未満でなくてはならず(MUST)、そうでない場合
      INDEX_SIZE_ERR例外が発生しなくてはなりません(MUST)。</p>
      <div class="annotate">(最新仕様では同様の機能を持つcopyFromChannel()が定められました。大きなデータの一部として複数回読み出すような場合、パフォーマンス面からgetChannelData()よりもcopyFromChannel()が推奨されます。
        <a href="http://webaudio.github.io/web-audio-api/#the-audiobuffer-interface" target="_blank">Editor's Draft : AudioBuffer Interface</a>
        )</div>
    </dd>
</dl>
</div>
</div>

<div id="AudioBufferSourceNode-section" class="section">
<h2 id="AudioBufferSourceNode">4.10. AudioBufferSourceNodeインターフェース <button onclick="OpenOriginal('AudioBufferSourceNode')">原文</button></h2>

<p>このインターフェースは<code>AudioBuffer</code>によってメモリ上に保持されているオーディオデータからのオーディオソースを表します。
  それは通常、(完全なリズムを刻むような)再生タイミングに高度な柔軟性が要求される短時間のオーディオのリソースです。
start()メソッドはいつ再生されるかをスケジュールするために使用されます。
再生は(もし<code>loop</code>属性が指定されていなければ)バッファのオーディオデータがすべて再生されると自動的に、
あるいはstop()メソッドが呼び出されて指定された時刻になると停止します。
より詳しくはstart()およびstop()の説明を参照してください。
start()とstop()はAudioBufferSourceNodeに対して複数回呼び出す事はできません。</p>
<pre>    numberOfInputs  : 0
    numberOfOutputs : 1
    </pre>

<p>出力のチャンネル数は常に.buffer属性に指定されたAudioBufferのチャンネル数と同じになります。
  もし.bufferがNULLの場合、チャンネルは無音の1チャンネルとなります。
</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="audio-buffer-source-node-idl">

interface <dfn id="dfn-AudioBufferSourceNode">AudioBufferSourceNode</dfn> : AudioNode {

    attribute AudioBuffer? buffer;

    readonly attribute AudioParam playbackRate;

    attribute boolean loop;
    attribute double loopStart;
    attribute double loopEnd;

    void start(optional double when = 0, optional double offset = 0, optional double duration);
    void stop(optional double when = 0);

    attribute EventHandler onended;

};
</code></pre>
</div>
</div>

<div id="attributes-AudioBufferSourceNode-section" class="section">
<h3 id="attributes-AudioBufferSourceNode">4.10.1. 属性 <button onclick="OpenOriginal('attributes-AudioBufferSourceNode')">原文</button></h3>
<dl>
  <dt id="dfn-buffer_AudioBufferSourceNode"><code>buffer</code></dt>
    <dd><p>再生されるオーディオのリソースを指定します。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-playbackRate_AudioBufferSourceNode"><code>playbackRate</code></dt>
    <dd><p>オーディオストリームを出力する再生速度です。デフォルトの<code>value</code>は1です。このパラメータは<em>a-rate</em>です。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-loop_AudioBufferSourceNode"><code>loop</code></dt>
    <dd><p>オーディオデータをループ再生する事を指定します。デフォルトはfalseです。</p>
    </dd>
</dl>

<dl>
  <dt id="dfn-loopStart_AudioBufferSourceNode"><code>loopStart</code></dt>
    <dd><p>オプションのパラメータで、<code>loop</code>属性がtrueの場合、ループの開始位置を秒で表します。
      デフォルトの<code>value</code>は0で、これは通常0とバッファのdurationの間の任意の値に設定されます。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-loopEnd_AudioBufferSourceNode"><code>loopEnd</code></dt>
    <dd><p>オプションのパラメータで、<code>loop</code>属性がtrueの場合、ループの終了位置を秒で表します。
      デフォルトの<code>value</code>は0で、通常0とバッファのdurationの間の任意に値に設定されます。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-onended_AudioBufferSourceNode"><code>onended</code></dt>
    <dd><p><a href="#AudioBufferSourceNode-section"><code>AudioBufferSourceNode</code></a>ノードにディスパッチされる終了イベントに対する
      (<cite><a href="http://www.whatwg.org/specs/web-apps/current-work/#eventhandler">HTML</a></cite>で記述されている)<code>EventHandler</code>を設定するための属性です。
      <code>AudioBufferSourceNode</code>のバッファの再生が終わった時、(<cite><a href="http://www.whatwg.org/specs/web-apps/current-work/#event">HTML</a></cite>で記述されている)
      イベントタイプ<code>Event</code>がイベントハンドラにディスパッチされます。</p>
    </dd>
</dl>


</div>

<div id="methodsandparams-AudioBufferSourceNode-section" class="section">
<h3 id="methodsandparams-AudioBufferSourceNode">4.10.2. メソッドとパラメータ <button onclick="OpenOriginal('methodsandparams-AudioBufferSourceNode')">原文</button></h3>
<dl>
  <dt id="dfn-start"><code>start</code>メソッド</dt>
    <dd><p>指定の時刻に音の再生開始をスケジュールします。</p>
      <p><dfn id="dfn-when">when</dfn>パラメータは、再生の開始時刻を(秒で)指定します。
        これはAudioContext.currentTimeと同じ時間軸の時刻を使用します。
        もしこの値に0、あるいは<b>currentTime</b>よりも小さな値を渡した場合、音は即時に再生されます。
        <code>start</code>は<code>stop</code>よりも前に一度だけ呼ばれなくてはなりません。
        そうでない場合、INVALID_STATE_ERR例外が発生しなくてはなりません(MUST)。</p>
      <p><dfn id="dfn-offset">offset</dfn>パラメータはバッファ中の再生開始位置を(秒で)指定します。
        もしこの値に0が渡された場合、再生はバッファの先頭から開始されます。</p>
      <p><dfn id="dfn-duration">duration</dfn>パラメータは再生される部分の長さを(秒で)指定します。
        もしこの値が渡されなかった場合、再生の長さはAudioBuffer全体の長さから<code>offset</code>を引いたものになります。
        即ち、<code>offset</code> も <code>duration</code>も指定されなかった場合、暗黙的にdurationは
        AudioBuffer全体の長さとなります。
      </p>

    </dd>
</dl>
<dl>
  <dt id="dfn-stop"><code>stop</code>メソッド</dt>
    <dd><p>指定の時刻に再生停止をスケジュールします。
      <dfn id="dfn-when_AudioBufferSourceNode_2">when</dfn>パラメータは再生を停止する時刻を(秒で)指定します。
      これはAudioContext.currentTimeと同じ時間軸の時刻を使用します。
      もしこの値に0、あるいは <b>currentTime</b>よりも小さな値を渡した場合、音の再生は即時に停止します。
      <code>stop</code>は<code>start</code>が呼ばれた後で一度だけ呼ばれなくてはなりません。
      そうでない場合、INVALID_STATE_ERR例外が発生しなくてはなりません(MUST)。</p>
    </dd>
</dl>
</div>

<div id="looping-AudioBufferSourceNode-section" class="section">
<h3 id="looping-AudioBufferSourceNode">4.10.3. ループ再生 <button onclick="OpenOriginal('looping-AudioBufferSourceNode')">原文</button></h3>
<p>
もし<code>start()</code>が呼ばれた時に<code>loop</code>属性がtrueであれば、再生は<code>stop()</code>が呼ばれて停止時刻になるまで無限に継続します。
これを"ループ"モードと呼びます。
再生は常に<code>start()</code>の<code>offset</code>引数で指定されたバッファ中の位置から開始され、<em>ループ</em>モード中、<em>actualLoopEnd</em>のバッファ位置(あるいはバッファの終端位置)
まで再生され、<em>actualLoopStart</em>のバッファ位置に戻る事が繰り返されます。
</p>

<p><em>ループ</em>モードでは<em>実際の</em>ループ位置は<code>loopStart</code>および<code>loopEnd</code>属性から次のように計算されます:
</p>

<blockquote>
<pre>
    if ((loopStart || loopEnd) &amp;&amp; loopStart >= 0 &amp;&amp; loopEnd > 0 &amp;&amp; loopStart &lt; loopEnd) {
        actualLoopStart = loopStart;
        actualLoopEnd = min(loopEnd, buffer.length);
    } else {
        actualLoopStart = 0;
        actualLoopEnd = buffer.length;
    }
</pre>
</blockquote>

<p><code>loopStart</code>および<code>loopEnd</code>のデフォルト値はどちらも0である事に注意が必要です。これはループがバッファの先頭から始まり、バッファの終端位置で終わる事を意味します。
</p>

<p>低レベルの実装の詳細では、特定のサンプルレート(通常はAudioContextのサンプルレートと同じですが)のAudioBufferでは、
  ループの時間(秒)がサンプルレートに従って適切なサンプルフレームの位置に変換されなくてはならない事に注意が必要です。
</p>

</div>
</div>

<div id="MediaElementAudioSourceNode-section" class="section">
<h2 id="MediaElementAudioSourceNode">4.11. MediaElementAudioSourceNodeインターフェース <button onclick="OpenOriginal('MediaElementAudioSourceNode')">原文</button></h2>

<p>このインターフェースは<code>audio</code>または<code>video</code>要素からの音声ソースを表します。</p>
<pre>    numberOfInputs  : 0
    numberOfOutputs : 1
    </pre>

<p>出力のチャンネル数はHTMLMediaElementで参照されるメディアのチャンネル数に対応します。
  そのため、メディア要素の.src属性の変更によって、このノードの出力チャンネル数が変化します。
  もし.src属性が設定されていない場合、出力のチャンネル数は1になり、無音を出力します。
</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="media-element-audio-source-node-idl">

interface <dfn id="dfn-MediaElementAudioSourceNode">MediaElementAudioSourceNode</dfn> : AudioNode {

};
</code></pre>
</div>
</div>

<p>MediaElementAudioSourceNodeはHTMLMediaElementからAudioContextの<a href="#dfn-createMediaElementSource">createMediaElementSource()</a>メソッドを使用して作成されます。</p>

<p>出力のチャンネル数はcreateMediaElementSource()の引数として渡されたHTMLMediaElementのオーディオのチャンネル数と同じになります。
  もしそのHTMLMediaElementがオーディオを持っていない場合、1となります。
</p>

<p>HTMLMediaElementはMediaElementAudioSourceNodeが作成された後、オーディオが直接、音として再生されなくなる代わりにMediaElementAudioSourceNodeからルーティンググラフを通して再生されるようになる事を<em>除けば</em>、MediaElementAudioSourceNodeを使わ<em>ない</em>場合と全く同じように振る舞わなくてはなりません。
</p>

<div class="example">

<div class="exampleHeader">
Example</div>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">ECMAScript</span></div>

<div class="blockContent">
<pre class="code"><code class="es-code">
var mediaElement = document.getElementById('mediaElementID');
var sourceNode = context.createMediaElementSource(mediaElement);
sourceNode.connect(filterNode);
 </code></pre>
</div>
</div>
</div>
</div>


<div id="ScriptProcessorNode-section" class="section">
<h2 id="ScriptProcessorNode">4.12. ScriptProcessorNodeインターフェース <button onclick="OpenOriginal('ScriptProcessorNode')">原文</button></h2>
<div class="annotate">(最新仕様ではScriptProcessorはDEPRECATEDとなりました。AudioWorkerが代替となります。<a href="http://webaudio.github.io/web-audio-api/#the-audioworker-interface" target="_blank">Editor's Draft : AudioWorker</a>)</div>
<p>このインターフェースはJavaScriptによってオーディオを直接、合成、加工、分析する事ができるAudioNodeです。</p>
<pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCount = numberOfInputChannels;
    channelCountMode = "explicit";
    channelInterpretation = "speakers";
</pre>

<p>ScriptProcessorNodeは<code>bufferSize</code>を指定して作成され、そのサイズは次の値のどれかでなくてはなりません:
  256、512、1024、2048、4096、8192、16384。
  この値は<code>audioprocess</code>イベントが発生する頻度と、それぞれの呼び出しで処理されるべきサンプルフレームの数
  を制します。
  <code>audioprocess</code>イベントは<a href="#ScriptProcessorNode-section"><code>ScriptProcessorNode</code></a>が
  少なくとも1つの入力か出力を接続している場合にのみディスパッチされます。
  <code>bufferSize</code>を小さい値にすると<a href="#Latency-section">レイテンシー</a>は低く(良く)なります。
  オーディオが途切れたり<a href="#Glitching-section">グリッジ</a>が発生するのを避けるためには大きな値が必要になります。
  <code>createScriptProcessor</code>のbufferSize引数が渡されない、あるいは0の場合、この値は実装によって
  自動的に設定されます。</p>

<p><code>numberOfInputChannels</code> および <code>numberOfOutputChannels</code>は入力と出力の
  チャンネル数を指定します。<code>numberOfInputChannels</code> と <code>numberOfOutputChannels</code>の両方が0で
  あってはいけません。</p>
<pre>    var node = context.createScriptProcessor(bufferSize, numberOfInputChannels, numberOfOutputChannels);
    </pre>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="script-processor-node-idl">

interface <dfn id="dfn-ScriptProcessorNode">ScriptProcessorNode</dfn> : AudioNode {

    attribute EventHandler onaudioprocess;

    readonly attribute long bufferSize;

};
</code></pre>
</div>
</div>

<div id="attributes-ScriptProcessorNode-section" class="section">
<h3 id="attributes-ScriptProcessorNode">4.12.1. 属性 <button onclick="OpenOriginal('attributes-ScriptProcessorNode')">原文</button></h3>
<dl>
  <dt id="dfn-onaudioprocess"><code>onaudioprocess</code></dt>
    <dd><p><a href="#ScriptProcessorNode-section"><code>ScriptProcessorNode</code></a>にディスパッチされるオーディオプロセスイベントの
      (<cite><a href="http://www.whatwg.org/specs/web-apps/current-work/#eventhandler">HTML</a></cite>で記述されている)<code>EventHandler</code>
      を設定するための属性です。
      <a href="#AudioProcessingEvent-section"><code>AudioProcessingEvent</code></a>型のイベントがイベントハンドラにディスパッチされます。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-bufferSize_ScriptProcessorNode"><code>bufferSize</code></dt>
    <dd><p><code>onaudioprocess</code>が呼ばれる度に処理が必要される(サンプルフレーム数で表される)バッファのサイズです。
      適正な値は次のどれかです(256, 512, 1024, 2048, 4096, 8192, 16384)。
    </p>
    </dd>
</dl>
</div>
</div>

<div id="AudioProcessingEvent-section" class="section">
<h2 id="AudioProcessingEvent">4.13. AudioProcessingEvent インターフェース <button onclick="OpenOriginal('AudioProcessingEvent')">原文</button></h2>

<p>これは<a href="#ScriptProcessorNode-section"><code>ScriptProcessorNode</code></a>ノードにディスパッチされる
  <code>Event</code>オブジェクトです。</p>

<p>イベントハンドラは(もしあれば)入力からのオーディオを<code>inputBuffer</code>属性からデータにアクセスして処理します。
  処理結果(あるいは入力がなければ合成した)オーディオデータは<code>outputBuffer</code>に出力します。</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="audio-processing-event-idl">

interface <dfn id="dfn-AudioProcessingEvent">AudioProcessingEvent</dfn> : Event {

    readonly attribute double playbackTime;
    readonly attribute AudioBuffer inputBuffer;
    readonly attribute AudioBuffer outputBuffer;

};
</code></pre>
</div>
</div>

<div id="attributes-AudioProcessingEvent-section" class="section">
<h3 id="attributes-AudioProcessingEvent">4.13.1. 属性 <button onclick="OpenOriginal('attributes-AudioProcessingEvent')">原文</button></h3>
<dl>
  <dt id="dfn-playbackTime"><code>playbackTime</code></dt>
    <dd><p>AudioContext.currentTimeと同じ時間軸で表された、そのオーディオが再生される時刻です。
      <code>playbackTime</code>によってJavaScriptでの処理をコンテキストのレンダリンググラフ中の他のイベントと非常に緊密に同期させる事が可能になります。
      </p>
    </dd>
</dl>
<dl>
  <dt id="dfn-inputBuffer"><code>inputBuffer</code></dt>
    <dd><p>入力データを含むAudioBufferです。 これはcreateScriptProcessor()メソッドの<code>numberOfInputChannels</code>と同じチャンネル数を持ちます。
      このAudioBufferは<code>onaudioprocess</code>関数のスコープ中でのみ有効です。
      その値はスコープの外では意味を持ちません。
      </p>
    </dd>
</dl>
<dl>
  <dt id="dfn-outputBuffer"><code>outputBuffer</code></dt>
    <dd><p>出力するオーディオデータを書き込むためのAudioBufferです。
      これはcreateScriptProcessor()メソッドの<code>numberOfOutputChannels</code>と同じチャンネル数を持ちます。
      <code>onaudioprocess</code>関数スコープ中のスクリプトコードはこのAufioBuffer中のチャンネルデータが表す<code>Float32Array</code>配列に
      書き込む事が期待されます。
      このスコープ外での、このAudioBufferに対するスクリプトによる変更は何も効果を持ちません。</p>
    </dd>
</dl>
</div>
</div>

<div id="PannerNode-section" class="section">
<h2 id="PannerNode">4.14. PannerNode インターフェース <button onclick="OpenOriginal('PannerNode')">原文</button></h2>
<div class="annotate">(最新仕様では2chステレオ用にはStereoPannerが新たに定められています。<a href="http://webaudio.github.io/web-audio-api/#the-stereopannernode-interface" target="_blank">Editor's Draft: StereoPanner</a>)</div>
<p>このインターフェースは入力されるオーディオストリームの3D空間での<a href="#Spatialization-section">定位 / 空間音響</a>を処理するためのノードを表します。
  空間音響は<a href="#AudioContext-section"><code>AudioContext</code></a>の
  <a href="#AudioListener-section"><code>AudioListener</code></a>(<code>listener</code> 属性)
  に関連して処理されます。
</p>

<pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCount = 2;
    channelCountMode = "clamped-max";
    channelInterpretation = "speakers";
</pre>

<p>入力からのオーディオストリームは入力への接続に依存してモノラルまたはステレオとなります。
</p>

<p>このノードの出力はステレオ(2チャンネル)に固定され、<em>現在の所</em>変更する事はできません。
</p>


<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="panner-node-idl">

enum <dfn>PanningModelType</dfn> {
  "equalpower",
  "HRTF"
};

enum <dfn>DistanceModelType</dfn> {
  "linear",
  "inverse",
  "exponential"
};

interface <dfn id="dfn-PannerNode">PannerNode</dfn> : AudioNode {

    <span class="comment">// Default for stereo is HRTF </span>
    attribute PanningModelType panningModel;

    <span class="comment">// Uses a 3D cartesian coordinate system </span>
    void setPosition(double x, double y, double z);
    void setOrientation(double x, double y, double z);
    void setVelocity(double x, double y, double z);

    <span class="comment">// Distance model and attributes </span>
    attribute DistanceModelType distanceModel;
    attribute double refDistance;
    attribute double maxDistance;
    attribute double rolloffFactor;

    <span class="comment">// Directional sound cone </span>
    attribute double coneInnerAngle;
    attribute double coneOuterAngle;
    attribute double coneOuterGain;

};
</code></pre>
</div>
</div>

<div id="attributes-PannerNode_attributes-section" class="section">
<h3 id="attributes-PannerNode_attributes">4.14.2. 属性 <button onclick="OpenOriginal('attributes-PannerNode_attributes')">原文</button></h3>
<dl>
  <dt id="dfn-panningModel"><code>panningModel</code></dt>
    <dd><p>オーディオを3D空間に定位させるために使用する空間音響アルゴリズムを指定します。
      デフォルトは"HRTF"です。</p>

      <dl>
        <dt id="dfn-EQUALPOWER"><code>"equalpower"</code></dt>
          <dd><p>単純で効率的な空間音響アルゴリズムで、等価パワーによるパンニングを行います。
          </p>
          </dd>
      </dl>
      <dl>
        <dt id="dfn-HRTF"><code>"HRTF"</code></dt>
          <dd><p>高品質な空間音響アルゴリズムで、被験者を使ったインパルスレスポンス測定からのコンボリューション処理を使用します。
            このパンニング方法はステレオ出力にレンダリングされます。</p>
          </dd>
      </dl>
    </dd>
</dl>
<dl>
  <dt id="dfn-distanceModel"><code>distanceModel</code></dt>
    <dd><p>音源がリスナーから離れていった時、音量を減衰させるためにどのアルゴリズムを使用するかを決定します。
      デフォルトは"inverse"です。</p>

<dl>
  <dt id="dfn-LINEAR_DISTANCE"><code>"linear"</code></dt>
    <dd><p><em>distanceGain</em>が次のように計算される直線距離モデルです:</p>
    <pre>
1 - rolloffFactor * (distance - refDistance) / (maxDistance - refDistance)
    </pre>
    </dd>
</dl>
<dl>
  <dt id="dfn-INVERSE_DISTANCE"><code>"inverse"</code></dt>
  <dd><p><em>distanceGain</em>が次のように計算される逆数距離モデルです:</p>

    <pre>
refDistance / (refDistance + rolloffFactor * (distance - refDistance))
    </pre>
    </dd>
</dl>
<dl>
  <dt id="dfn-EXPONENTIAL_DISTANCE"><code>"exponential"</code></dt>
  <dd><p><em>distanceGain</em>が次のように計算される指数距離モデルです:</p>
  <pre>
pow(distance / refDistance, -rolloffFactor)
  </pre>
    </dd>
</dl>

    </dd>
</dl>
<dl>
  <dt id="dfn-refDistance"><code>refDistance</code></dt>
    <dd><p>音源がリスナーから離れていった時の音量減衰のリファレンスとなる基準距離です。
      デフォルトの値は1です。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-maxDistance"><code>maxDistance</code></dt>
    <dd><p>音量減衰の最大距離で、これ以上音源とリスナー間が離れても音量が減衰しません。
      デフォルトの値は10000です。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-rolloffFactor"><code>rolloffFactor</code></dt>
    <dd><p>音源がリスナーが離れていった時の音量減衰の速さを表します。
      デフォルトの値は1です。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-coneInnerAngle"><code>coneInnerAngle</code></dt>
    <dd><p>音源の指向性パラメータで、角度で表します。この角度の内部では音量減衰が生じません。
      デフォルトの値は360です。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-coneOuterAngle"><code>coneOuterAngle</code></dt>
    <dd><p>音源の指向性パラメータで角度で表します。この角度の外側では音量の減衰率が定数値の<b>coneOuterGain</b>となります。
      デフォルトの値は360です。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-coneOuterGain"><code>coneOuterGain</code></dt>
    <dd><p>音源の指向性パラメータで、角度が<b>coneOuterAngle</b>の外側の場合の減衰率です。
      デフォルトの値は0です。</p>
    </dd>
</dl>
</div>

<h3 id="Methods_and_Parameters">4.14.3. メソッドとパラメータ <button onclick="OpenOriginal('Methods_and_Parameters')">原文</button></h3>
<dl>
  <dt id="dfn-setPosition"><code>setPosition</code> メソッド</dt>
    <dd><p><b>listener</b>属性に相対する音源の位置を設定します。
      3Dのデカルト座標系が使用されます。</p>
      <p><dfn id="dfn-x">x, y, z</dfn>パラメータは3D空間中の座標を表します</p>
      <p>デフォルトの値は (0,0,0)です。
      </p>
    </dd>
</dl>
<dl>
  <dt id="dfn-setOrientation"><code>setOrientation</code> メソッド</dt>
    <dd><p>3D空間のデカルト座標系で音源の向いている方向を表します。
      音がどれくらいの指向性(<b>cone</b>属性で制御されます)を持っているかによって音が
      リスナーからはずれると小さくなったり全く聴こえなくなったりします。</p>
      <p><dfn id="dfn-x_2">x, y, z</dfn>パラメータは3D空間内での方向を表します。</p>
      <p>デフォルトの値は(1,0,0)です。
      </p>
    </dd>
</dl>
<dl>
  <dt id="dfn-setVelocity"><code>setVelocity</code> メソッド</dt>
    <dd><p>音源の速度ベクトルを設定します。
      このベクトルは3D空間内での移動する方向と速度の両方を制御します。
      この速度とリスナーの速度の相対値によってどれくらいのドップラー効果(ピッチの変化)が適用されるかが決定します。
      このベクトルの単位は<em>メートル / 秒</em>で、位置や方向ベクトルで使われる単位とは独立しています。</p>
      <p><dfn id="dfn-x_3">x, y, z</dfn>パラメータは移動の方向と大きさを表しています。</p>
      <p>デフォルトの値は(0,0,0)です。
      </p>
    </dd>
</dl>
</div>

<div id="AudioListener-section" class="section">
<h2 id="AudioListener">4.15. AudioListener インターフェース <button onclick="OpenOriginal('AudioListener')">原文</button></h2>

<p>このインターフェースはオーディオシーンを聴く人の位置と方向を表します。
  全ての<a href="#PannerNode-section"><code>PannerNode</code></a>オブジェクトは
  AudioContextの<code>listener</code>との関係で空間に配置されます。
  空間音響についての詳細は<a href="#Spatialization-section">この</a>セクションを見てください。
</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="audio-listener-idl">

interface <dfn id="dfn-AudioListener">AudioListener</dfn> {

    attribute double dopplerFactor;
    attribute double speedOfSound;

    <span class="comment">// Uses a 3D cartesian coordinate system </span>
    void setPosition(double x, double y, double z);
    void setOrientation(double x, double y, double z, double xUp, double yUp, double zUp);
    void setVelocity(double x, double y, double z);

};
</code></pre>
</div>
</div>

<div id="attributes-AudioListener-section" class="section">
<h3 id="attributes-AudioListener">4.15.1. 属性 <button onclick="OpenOriginal('attributes-AudioListener')">原文</button></h3>
<dl>
  <dt id="dfn-dopplerFactor"><code>dopplerFactor</code></dt>
    <dd><p>ドップラー効果をレンダリングする際に使用するピッチ偏移の量を決定する定数です。
      デフォルトの値は1です。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-speedOfSound"><code>speedOfSound</code></dt>
    <dd><p>ドップラー効果の計算の際に使用する音速です。
      デフォルトの値は343.3です。</p>
    </dd>
</dl>
</div>

<h3 id="L15842">4.15.2. メソッドとパラメータ</h3>
<dl>
  <dt id="dfn-setPosition_2"><code>setPosition</code> メソッド</dt>
    <dd><p>3Dデカルト座標空間でのリスナーの位置を設定します。
      <code>PannerNode</code>オブジェクトはこの位置と音源との相対的な位置を空間音響のために使用します。</p>
      <p><dfn id="dfn-x_AudioListener">x, y, z</dfn>パラメータは3D空間内の座標を表します。</p>
      <p>デフォルトの値は(0,0,0)です。
      </p>
    </dd>
</dl>
<dl>
  <dt id="dfn-setOrientation_2"><code>setOrientation</code> メソッド</dt>
    <dd><p>3Dデカルト座標空間でリスナーが向いている方向を表します。
      <b>front</b>ベクトルと<b>up</b>ベクトルの両方が与えられます。
      単純に人間について言えば、<b>front</b>ベクトルはその人の鼻が向いている方向を表します。
      <b>up</b>ベクトルはその人の頭のてっぺんが向いている方向です。
      これらは線形独立(互いに90度)の関係になります。
      これらの値がどのように解釈されるかの基準としての要件は<a href="#Spatialization-section">空間音響</a>セクションを見てください。
      </p>
      <p><dfn id="dfn-x_setOrientation">x, y, z</dfn>パラメータは3D空間中の<b>front</b>方向ベクトルであり、デフォルトの値は
        (0,0,-1)です。</p>
      <p><dfn id="dfn-x_setOrientation_2">xUp, yUp, zUp</dfn>パラメータは3D空間中の<b>up</b>方向ベクトルであり、デフォルトの値は
        (0,1,0)になります。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-setVelocity_4"><code>setVelocity</code> メソッド</dt>
    <dd><p>リスナーの速度ベクトルを設定します。
      このベクトルは3D空間内での移動の方向と速度の両方を制御します。
      この速度と音源の速度との相対値がドップラー効果(ピッチ変化)がどれだけ起こるかの決定に使用されます。
      このベクトルの単位は<em>メートル / 秒</em>であり、位置と方向のベクトルの単位とは独立しています。
      </p>
      <p><dfn id="dfn-x_setVelocity_5">x, y, z</dfn>パラメータは移動の方向と大きさの方向ベクトルを表します。</p>
      <p>デフォルトの値は(0,0,0)です。
      </p>
    </dd>
</dl>
</div>

<div id="ConvolverNode-section" class="section">
<h2 id="ConvolverNode">4.16. ConvolverNode インターフェース <button onclick="OpenOriginal('ConvolverNode')">原文</button></h2>

<p>このインターフェースはインパルスレスポンスによって<a href="#Convolution-section">線形コンボリューションエフェクト</a>を
  適用する処理ノードを表すインターフェースです。
  マルチチャンネルのコンボリューションマトリックスに対する基準としての要件は
  <a href="#Convolution-reverb-effect">ここ</a>に記述されています。</p>
<pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCount = 2;
    channelCountMode = "clamped-max";
    channelInterpretation = "speakers";
</pre>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="convolver-node-idl">

interface <dfn id="dfn-ConvolverNode">ConvolverNode</dfn> : AudioNode {

    attribute AudioBuffer? buffer;
    attribute boolean normalize;

};
</code></pre>
</div>
</div>

<div id="attributes-ConvolverNode-section" class="section">
<h3 id="attributes-ConvolverNode">4.16.1. 属性 <button onclick="OpenOriginal('attributes-ConvolverNode')">原文</button></h3>
<dl>
  <dt id="dfn-buffer_ConvolverNode"><code>buffer</code></dt>
    <dd><p>ConvolverNodeで使用される(マルチチャンネルの場合もある)インパルスレスポンスを含む
      モノラル、ステレオ、または4チャンネルの<code>AudioBuffer</code>です。
      この<code>AudioBuffer</code>はAudioContextと同じサンプルレートでなくてはなりません。
      そうでない場合NOT_SUPPORTED_ERR例外が発生しなくてはなりません(MUST)。
      この属性が設定される際に、<em>buffer</em>と<em>normalize</em>属性の状態がこのインパルスレスポンスが正規化されるかどうかのConvolverNodeの設定に使われます。
      この属性の初期値はnullです。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-normalize"><code>normalize</code></dt>
    <dd><p><code>buffer</code>属性がセットされた時に等価パワーで正規化してインパルスレスポンスをスケーリングされるかどうかを制御します。
      このデフォルトの値は<code>true</code>で、様々なインパルスレスポンスをロードした時にコンボルバーからの出力レベルを均一化するようになっています。
      もし <code>normalize</code>が<code>false</code>に設定された場合、インパルスレスポンスの前処理/スケーリングなしでコンボリューションが行われます。
      この値を変更した場合、次回に<em>buffer</em>属性をセットするまで効果は現れません。
      </p>
    </dd>
</dl>

      <p>もし<em>buffer</em>属性を設定した時に<em>normalize</em>属性がfalseであればConvolverNodeは<em>buffer</em>内にあるインパルスレスポンス
        をそのまま使用して線形コンボリューション処理を行います。
      </p>
      <p>
        そうでなく、<em>buffer</em>属性を設定した時に<em>normalize</em>属性がtrueであれば、ConvolverNodeは次のアルゴリズムによって、
        まず<em>buffer</em>内のデータのスケールドRMS-パワー解析を行い、<em>normalizationScale</em>を計算します:
      </p>


      <div class="block">

      <div class="blockTitleDiv">

      <div class="blockContent">
      <pre class="code"><code class="es-code">

float calculateNormalizationScale(buffer)
{
    const float GainCalibration = 0.00125;
    const float GainCalibrationSampleRate = 44100;
    const float MinPower = 0.000125;

    // Normalize by RMS power.
    size_t numberOfChannels = buffer->numberOfChannels();
    size_t length = buffer->length();

    float power = 0;

    for (size_t i = 0; i &lt; numberOfChannels; ++i) {
        float* sourceP = buffer->channel(i)->data();
        float channelPower = 0;

        int n = length;
        while (n--) {
            float sample = *sourceP++;
            channelPower += sample * sample;
        }

        power += channelPower;
    }

    power = sqrt(power / (numberOfChannels * length));

    // Protect against accidental overload.
    if (isinf(power) || isnan(power) || power &lt; MinPower)
        power = MinPower;

    float scale = 1 / power;

    // Calibrate to make perceived volume same as unprocessed.
    scale *= GainCalibration;

    // Scale depends on sample-rate.
    if (buffer->sampleRate())
        scale *= GainCalibrationSampleRate / buffer->sampleRate();

    // True-stereo compensation.
    if (buffer->numberOfChannels() == 4)
        scale *= 0.5;

    return scale;
}
          </code></pre>

      </div>
      </div>
      </div>

<p>処理の間ConvolverNodeはこの計算された<em>normalizationScale</em>値を
  最終出力を得るために入力と(<em>buffer</em>で表される)インパルスレスポンスを処理した線形コンボリューションの結果と掛け合わせます。
  あるいは、例えば入力に事前に<em>normalizationScale</em>をかけ合わせたり、<em>normalizationScale</em>を掛け合わせたバージョンのインパルスレスポンスを作るなど、
  何らかの数学的に等価な演算が使用されるかも知れません。
</p>

</div>
</div>

<div id="AnalyserNode-section" class="section">
<h2 id="AnalyserNode">4.17. AnalyserNode インターフェース <button onclick="OpenOriginal('AnalyserNode')">原文</button></h2>

<p>このインターフェースはリアルタイムの周波数領域および時間領域の<a href="#AnalyserNode">解析</a>を行う事ができるノードを表しています。
  オーディオストリームは入力から出力にそのまま渡されます。
</p>
<pre>
    numberOfInputs  : 1
    numberOfOutputs : 1    <em>Note that this output may be left unconnected.</em>

    channelCount = 1;
    channelCountMode = "explicit";
    channelInterpretation = "speakers";
</pre>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="analyser-node-idl">

interface <dfn id="dfn-AnalyserNode">AnalyserNode</dfn> : AudioNode {

    <span class="comment">// Real-time frequency-domain data </span>
    void getFloatFrequencyData(Float32Array array);
    void getByteFrequencyData(Uint8Array array);

    <span class="comment">// Real-time waveform data </span>
    void getByteTimeDomainData(Uint8Array array);

    attribute unsigned long fftSize;
    readonly attribute unsigned long frequencyBinCount;

    attribute double minDecibels;
    attribute double maxDecibels;

    attribute double smoothingTimeConstant;

};
</code></pre>
</div>
</div>

<div id="attributes-ConvolverNode-section_2" class="section">
<h3 id="attributes-ConvolverNode_2">4.17.1. 属性 <button onclick="OpenOriginal('attributes-ConvolverNode_2')">原文</button></h3>
<dl>
  <dt id="dfn-fftSize"><code>fftSize</code></dt>
    <dd><p>周波数領域の解析で使用されるFFTのサイズです。2の累乗(0を除く)で32から2048の範囲でなくてはなりません。
      そうでない場合、INDEX_SIZE_ERR例外が発生しなくてはなりません(MUST)。
      デフォルトの値は2048です。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-frequencyBinCount"><code>frequencyBinCount</code></dt>
    <dd><p>FFTサイズの1/2です</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-minDecibels"><code>minDecibels</code></dt>
    <dd><p>FFT解析データをunsigned byte値へ変換するスケーリングの際の最少パワー値です。
      デフォルトの値は-100です。
      もしこの属性の値が<code>maxDecibels</code>かそれ以上に設定された場合、INDEX_SIZE_ERR例外が発生しなくてはなりません(MUST)。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-maxDecibels"><code>maxDecibels</code></dt>
    <dd><p>FFT解析データをunsigned byte値へ変換するスケーリングの際の最大パワー値です。
      デフォルトの値は-30です。
      もしこの属性の値が<code>minDecibels</code>かそれ以上の値に設定された場合、INDEX_SIZE_ERR例外が発生しなくてはなりません(MUST)。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-smoothingTimeConstant"><code>smoothingTimeConstant</code></dt>
    <dd><p>0 -&gt; 1 の範囲の値で、0ならば最後の解析フレームに対して時間平均が取られない事を表します。
      デフォルトの値は0.8です。
      もしこの属性の値が0より小さいか1より大きい値が設定された場合、INDEX_SIZE_ERR例外が発生しなくてはなりません(MUST)。</p>
    </dd>
</dl>
</div>

<h3 id="methods-and-parameters">4.17.2. メソッドとパラメータ <button onclick="OpenOriginal('methods-and-parameters')">原文</button></h3>
<dl>
  <dt id="dfn-getFloatFrequencyData"><code>getFloatFrequencyData</code> メソッド</dt>
    <dd><p>現在の周波数データを渡された浮動小数値配列にコピーします。
      もし配列の要素数がfrequencyBinCountよりも小さい場合、はみ出した部分は削られます。
      もし配列がfrequencyBinCountよりも大きい要素数を持つ場合、余分な要素は無視されます。</p>
      <p><dfn id="dfn-array">array</dfn>パラメータは周波数領域の解析結果を格納する場所を示します。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-getByteFrequencyData"><code>getByteFrequencyData</code> メソッド</dt>
    <dd><p>現在の周波数データを渡されたunsigned byte配列にコピーします。
      もし配列の要素数がfrequencyBinCountよりも小さい場合、はみ出した部分は削られます。
      もし配列がfrequencyBinCountよりも大きい要素数を持つ場合、余分な要素は無視されます</p>
      <p><dfn id="dfn-array_2">array</dfn>パラメータは周波数領域の解析結果の格納する場所を示します。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-getByteTimeDomainData"><code>getByteTimeDomainData</code> メソッド</dt>
    <dd><p>現在の時間領域データ(波形データ)を渡されたunsigned byte配列にコピーします。
      もし配列の要素数がfftSizeよりも小さい場合、はみ出た部分は削られます。
      もし配列がfftSizeよりも大きな要素数を持つ場合、余分な要素は無視されます。</p>
      <p><dfn id="dfn-array_3">array</dfn>パラメータは時間領域解析データを格納する場所を示します。</p>
    </dd>
</dl>
</div>

<div id="ChannelSplitterNode-section" class="section">
<h2 id="ChannelSplitterNode">4.18. ChannelSplitterNode インターフェース <button onclick="OpenOriginal('ChannelSplitterNode')">原文</button></h2>

<p><code>ChannelSplitterNode</code>は高度なアプリケーションで、<a href="#ChannelMergerNode-section"><code>ChannelMergerNode</code></a>
  と組み合わせて使われます。</p>
<pre>
    numberOfInputs  : 1
    numberOfOutputs : Variable N (defaults to 6) // number of "active" (non-silent) outputs is determined by number of channels in the input

    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>

<p>このインターフェースはルーティンググラフ中のオーディオストリームの個別のチャンネルにアクセスするAudioNodeを表しています。
  これは1つの入力と入力のオーディオストリームのチャンネル数と同じ数の"アクティブ"な出力を持ちます。
  例えば、ステレオの入力ストリームが<code>ChannelSplitterNode</code>に接続された場合、アクティブな出力は2
  (1つは左チャンネルから、もう1つは右チャンネルから)になります。
  常に合計N個の出力(AudioContextの<code>createChannelSplitter()</code>の<code>numberOfOutputs</code>パラメータで決まります)があり、
  この値が渡されない場合のデフォルトの数は6になります。
  "アクティブ"でないどの出力も無音を出力し、通常はどこにも接続されません。</p>

<h3 id="example-1">例:</h3>
<img alt="channel splitter" src="images/channel-splitter.png" />

<p>この例ではスプリッターはチャンネルの(例えば左チャンネル、右チャンネルなどの)識別は<b>せず</b>、単純に入力チャンネルの順序に従って出力チャンネルを分割する事に注意してください。</p>

<p><code>ChannelSplitterNode</code>を使うアプリケーションの1つは個別のチャンネルのゲインの制御を必要とする"マトリックス・ミキシング"を行うものです。
</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="channel-splitter-node-idl">

interface <dfn id="dfn-ChannelSplitterNode">ChannelSplitterNode</dfn> : AudioNode {

};
</code></pre>
</div>
</div>
</div>

<div id="ChannelMergerNode-section" class="section">
<h2 id="ChannelMergerNode">4.19. ChannelMergerNode インターフェース <button onclick="OpenOriginal('ChannelMergerNode')">原文</button></h2>

<p><code>ChannelMergerNode</code>は高度なアプリケーションで、<a href="#ChannelSplitterNode-section"><code>ChannelSplitterNode</code></a>
  と組み合わせて使われます。</p>
<pre>
    numberOfInputs  : Variable N (default to 6)  // number of connected inputs may be less than this
    numberOfOutputs : 1

    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>

<p>このインターフェースは複数のオーディオストリームからチャンネルを結合して1つのオーディオストリームにするAudioNodeを表します。
  これは可変数の入力(デフォルトは6)の入力を持ちますが、全ての入力を接続する必要はありません。
  出力は1つでそのオーディオストリームは、接続された入力の全てのチャンネルの合計のチャンネル数を持ちます。
  例えば、<code>ChannelMergerNode</code>が2つの入力(どちらもステレオ)が接続された場合、出力は4チャンネルになり、
  最初の2つは最初の入力から、次の2つは2番目の入力からになります。
  別の例として、2つの入力(どちらもモノラル)の場合は、出力は2チャンネル(ステレオ)で、左チャンネルが最初の入力から、右チャンネルは2番目の入力から取られます。</p>

<h3 id="example-2">例:</h3>
<img alt="channel merger" src="images/channel-merger.png" />

<p>この例ではマージャーはチャンネルの(例えば左チャンネル、右チャンネルなどの)識別は<b>せず</b>、単純に入力チャンネルの順序に従って出力チャンネルを結合する事に注意してください。</p>

<p><code>ChannelMergerNode</code>はオーディオハードウェアがサポートする最大チャンネル数を超える多くのチャンネル数を持つ
  オーディオストリームを出力するように接続する事が可能な事に注意してください。
  この場合、その出力はAudioContext.destination (オーディオハードウェア)に接続されると、余分なチャンネルは無視されます。
  そのため、<code>ChannelMergerNode</code>はチャンネル数について認識した状況で使用されなくてはなりません。
</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="channel-merger-node-idl">

interface <dfn id="dfn-ChannelMergerNode">ChannelMergerNode</dfn> : AudioNode {

};
</code></pre>
</div>
</div>
</div>
<div id="DynamicsCompressorNode-section" class="section">
<h2 id="DynamicsCompressorNode">4.20. DynamicsCompressorNode インターフェース <button onclick="OpenOriginal('DynamicsCompressorNode')">原文</button></h2>

<p>DynamicsCompressorNodeはダイナミック・コンプレッション・エフェクトを実装したAudioNodeです。</p>

<p>ダイナミック・コンプレッションは音楽制作やゲーム・オーディオで非常に良く使われます。
  これは信号の音量が大きな部分を抑え、音量が小さな部分を持ち上げます。
  全体として、より大きく、豊かで隙間のない音を作る事ができます。
  これは特に、多くの個別サウンドを同時に再生するゲームと音楽アプリケーションで、全体の信号レベルを制御してスピーカーへの出力のクリッピング(歪み)を避けるために重要です。
</p>
<pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCount = 2;
    channelCountMode = "explicit";
    channelInterpretation = "speakers";
</pre>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="dynamics-compressor-node-idl">

interface <dfn id="dfn-DynamicsCompressorNode">DynamicsCompressorNode</dfn> : AudioNode {

    readonly attribute AudioParam threshold; // in Decibels
    readonly attribute AudioParam knee; // in Decibels
    readonly attribute AudioParam ratio; // unit-less
    readonly attribute AudioParam reduction; // in Decibels
    readonly attribute AudioParam attack; // in Seconds
    readonly attribute AudioParam release; // in Seconds

};
</code>
</pre>
</div>
</div>

<div id="attributes-DynamicsCompressorNode-section" class="section">
<h3 id="attributes-DynamicsCompressorNode">4.20.1. 属性 <button onclick="OpenOriginal('attributes-DynamicsCompressorNode')">原文</button></h3>
<p>
全てのパラメータは<em>k-rate</em>です。
</p>

<dl>
  <dt id="dfn-threshold"><code>threshold</code></dt>
    <dd><p>これを超えた時にコンプレッション動作を開始するデシベル値です。
      デフォルトの<code>value</code>は-100から0の範囲中、-24になっています。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-knee"><code>knee</code></dt>
    <dd><p>threadholdを超えた部分の、"ratio"の変化が滑らかにカーブする範囲を表すデシベル値です。
      このデフォルトの<code>value</code>は0から40の範囲中、30になっています。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-ratio"><code>ratio</code></dt>
    <dd><p>出力が1dB変化するための入力のdBの変化量です。
      このデフォルトの<code>value</code>は、1から20の範囲中、12になっています。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-reduction"><code>reduction</code></dt>
    <dd><p>メーターの表示のために使用するリードオンリーのデシベル値で、
      信号に対するコンプレッサーの動作による、現在のゲインの減衰量を表します。
      もし信号が供給されていない場合この値は0(ゲイン減衰なし)になります。
      名目上の範囲は-20から0になります。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-attack"><code>attack</code></dt>
    <dd><p>ゲインを10dB減衰させるために必要な時間(秒)です。
      これは名目上0から1の範囲を持ち、デフォルトの<code>value</code>は0.003です。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-release"><code>release</code></dt>
    <dd><p>ゲインを10dB増加させるために必要な時間(秒)です。
      これは名目上0から1の範囲を持ち、デフォルトの<code>value</code>は0.25です。</p>
    </dd>
</dl>
</div>
</div>

<div id="BiquadFilterNode-section" class="section">
<h2 id="BiquadFilterNode">4.21. BiquadFilterNode インターフェース <button onclick="OpenOriginal('BiquadFilterNode')">原文</button></h2>

<p>BiquadFilterNodeは非常に一般的な低次フィルタを実装したAudioNodeです。</p>

<p>低次フィルタは基本的なトーンコントロール(バス、ミドル、トレブル)やグラフィックイコライザーやより高度なフィルタを構成するブロックです。
  複数のBiquadFilterNodeフィルタを組み合わせてより複雑なフィルタを作る事もできます。
  フィルタのパラメータの"frequency"などを時間と共に変化させてフィルタスイープやその他の効果を得る事もできます。
  それぞれのBiquadFilterNodeは下のIDLで紹介する一般的なフィルタの型のうちの1つに設定する事ができます。
  デフォルトのフィルタの型は"lowpass"です。</p>

<pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>
<p>
  出力のチャンネル数は常に入力のチャンネル数と同じになります。
</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="biquad-filter-node-idl">

enum <dfn>BiquadFilterType</dfn> {
  "lowpass",
  "highpass",
  "bandpass",
  "lowshelf",
  "highshelf",
  "peaking",
  "notch",
  "allpass"
};

interface <dfn id="dfn-BiquadFilterNode">BiquadFilterNode</dfn> : AudioNode {

    attribute BiquadFilterType type;
    readonly attribute AudioParam frequency; // in Hertz
    readonly attribute AudioParam detune; // in Cents
    readonly attribute AudioParam Q; // Quality factor
    readonly attribute AudioParam gain; // in Decibels

    void getFrequencyResponse(Float32Array frequencyHz,
                              Float32Array magResponse,
                              Float32Array phaseResponse);

};
</code></pre>
</div>
</div>

<p>フィルタの型について以下で簡単に説明します。
  我々はこれらのフィルタがオーディオ処理で非常に良く使われている事に気づきます。
  実装的に言えばこれらはすべて標準的なアナログフィルタのプロトタイプから派生したものです。
  より技術的な詳細については、我々はRobert Bristow-Johnsonによる素晴らしい解説文書を参照します。
  <a href="http://www.musicdsp.org/files/Audio-EQ-Cookbook.txt">参照</a>
</p>

<p>
  全てのパラメータは<em>k-rate</em>であり、次のデフォルト値を持ちます:
</p>

<blockquote>
<dl>
  <dt>frequency</dt>
    <dd>350Hz、 名目上の範囲は10からナイキスト周波数(サンプルレートの半分)。
    </dd>
  <dt><a href="http://en.wikipedia.org/wiki/Q_factor">Q</a></dt>
    <dd>1、 名目上の範囲は0.0001から1000。</dd>
  <dt>gain</dt>
    <dd>0、名目上の範囲は-40から40。</dd>
</dl>
</blockquote>



<div id="BiquadFilterNode-description-section" class="section">
<h3 id="BiquadFilterNode-description">4.21.1 "lowpass" <button onclick="OpenOriginal('BiquadFilterNode-description')">原文</button></h3>

<p><a href="http://en.wikipedia.org/wiki/Low-pass_filter">ローパスフィルタ</a>は
  カットオフ周波数より低い周波数をそのまま通し、カットオフよりも高い周波数を減衰させます。これは標準的な2次の
  レゾナントローパスフィルタの実装で、12dB/オクターブのロールオフを持ちます。
</p>

<blockquote>
  <dl>
    <dt>frequency</dt>
      <dd>カットオフ周波数</dd>
    <dt>Q</dt>
      <dd>カットオフ周波数にどれだけピークを付けて共振させるかを制御します。
        大きな値はより強く共振させます。
        このフィルタタイプではこの値は伝統的な従来のQではなく、デシベルで表される共振の値である事に注意してください。</dd>
    <dt>gain</dt>
      <dd>このフィルタのタイプでは使用しません。</dd>
  </dl>
</blockquote>

<h3 id="HIGHPASS">4.21.2 "highpass" <button onclick="OpenOriginal('HIGHPASS')">原文</button></h3>

<p><a href="http://en.wikipedia.org/wiki/High-pass_filter">ハイパスフィルタ</a>はローパスフィルタの反対の機能を持ちます。
  カットオフ周波数よりも高い周波数をそのまま通し、カットオフよりも低い周波数を減衰させます。
  これは標準的な2次レゾナントハイパスフィルタの実装で、12dB/オクターブのロールオフを持ちます。</p>

<blockquote>
  <dl>
    <dt>frequency</dt>
      <dd>これより低い周波数を減衰させるカットオフ周波数です。</dd>
    <dt>Q</dt>
      <dd>カットオフ周波数にどれだけピークを付けて共振させるかを制御します。
        大きな値はより強く共振させます。
        このフィルタタイプではこの値は伝統的な従来のQではなく、デシベルで表される共振の値である事に注意してください。</dd>
    <dt>gain</dt>
      <dd>このフィルタのタイプでは使用しません。</dd>
  </dl>
</blockquote>

<h3 id="BANDPASS">4.21.3 "bandpass" <button onclick="OpenOriginal('BANDPASS')">原文</button></h3>

<p>A <a href="http://en.wikipedia.org/wiki/Band-pass_filter">バンドパスフィルタ</a>はある範囲の周波数をそのまま通し、この周波数範囲より上または下の周波数を減衰させます。
  これは2次のバンドパスフィルタを実装しています。</p>

<blockquote>
  <dl>
    <dt>frequency</dt>
      <dd>周波数範囲の中心周波数です。</dd>
    <dt><a href="http://en.wikipedia.org/wiki/Q_factor">Q</a></dt>
      <dd>周波数範囲の幅を制御します。この幅はQが増加すると狭くなります。</dd>
    <dt>gain</dt>
      <dd>このフィルタのタイプでは使用しません。</dd>
  </dl>
</blockquote>

<h3 id="LOWSHELF">4.21.4 "lowshelf" <button onclick="OpenOriginal('LOWSHELF')">原文</button></h3>

<p>ローシェルフフィルタは全ての周波数を通しますが、低い周波数だけを増幅(または減衰)させます。
  これは2次のローシェルフフィルタを実装しています。</p>

<blockquote>
  <dl>
    <dt>frequency</dt>
      <dd>増幅(または減衰)させる上限の周波数です。</dd>
    <dt><a href="http://en.wikipedia.org/wiki/Q_factor">Q</a></dt>
      <dd>このフィルタのタイプでは使用しません。</dd>
    <dt>gain</dt>
      <dd>dBで表した増幅率です。もしこの値が負ならばその周波数は減衰されます。</dd>
  </dl>
</blockquote>

<h3 id="L16352">4.21.5 "highshelf" <button onclick="OpenOriginal('L16352')">原文</button></h3>

<p>ハイシェルフフィルタはローシェルフフィルタとは反対に、すべての周波数を通しますが高い周波数だけを増幅します。
  これは2次のハイシェルフフィルタを実装しています。</p>

<blockquote>
  <dl>
    <dt>frequency</dt>
      <dd>増幅(または減衰)させる下限の周波数です。</dd>
    <dt><a href="http://en.wikipedia.org/wiki/Q_factor">Q</a></dt>
      <dd>このフィルタのタイプでは使用しません。</dd>
    <dt>gain</dt>
      <dd>dBで表した増幅率です。もしこの値が負ならばその周波数は減衰されます。</dd>
  </dl>
</blockquote>

<h3 id="PEAKING">4.21.6 "peaking" <button onclick="OpenOriginal('PEAKING')">原文</button></h3>

<p>ピーキングフィルタは全ての周波数を通しますが、ある周波数の範囲だけが増幅(または減衰)されます。</p>

<blockquote>
  <dl>
    <dt>frequency</dt>
      <dd>増幅される中心の周波数です。</dd>
    <dt><a href="http://en.wikipedia.org/wiki/Q_factor">Q</a></dt>
      <dd>増幅される周波数の幅を制御します。値が大きいと幅は狭くなります。</dd>
    <dt>gain</dt>
      <dd>dBで表した増幅率です。もしこの値が負ならばその周波数は減衰されます。</dd>
  </dl>
</blockquote>

<h3 id="NOTCH">4.21.7 "notch" <button onclick="OpenOriginal('NOTCH')">原文</button></h3>

<p>ノッチフィルタ(<a href="http://en.wikipedia.org/wiki/Band-stop_filter">バンドストップまたはバンドリジェクション・フィルタ</a>とも呼ばれます)は、バンドパスフィルタの逆です。
  ある周波数を除く全ての周波数を通します。</p>

<blockquote>
  <dl>
    <dt>frequency</dt>
      <dd>ノッチを適用する中心の周波数です。</dd>
    <dt><a href="http://en.wikipedia.org/wiki/Q_factor">Q</a></dt>
      <dd>減衰させる周波数の幅を制御します。大きな値は幅が狭い事を意味します。</dd>
    <dt>gain</dt>
      <dd>このフィルタのタイプでは使用しません。</dd>
  </dl>
</blockquote>

<h3 id="ALLPASS">4.21.8 "allpass" <button onclick="OpenOriginal('ALLPASS')">原文</button></h3>

<p><a href="http://en.wikipedia.org/wiki/All-pass_filter#Digital_Implementation">オールパスフィルタ</a>は全ての周波数を通しますが、周波数の変化に対して位相が変化します。
  これは2次のオールパスフィルタを実装しています。</p>

<blockquote>
  <dl>
    <dt>frequency</dt>
      <dd>位相変化が発生する中心の周波数です。別の見方では、最大の<a href="http://en.wikipedia.org/wiki/Group_delay">群遅延</a>になる周波数です。</dd>
    <dt><a href="http://en.wikipedia.org/wiki/Q_factor">Q</a></dt>
      <dd>中心周波数での位相変化がどれくらい急峻であるかを制御します。値が大きいと、より急峻な位相変化で大きな群遅延である事を意味します。</dd>
    <dt>gain</dt>
      <dd>このフィルタのタイプでは使用しません。</dd>
  </dl>
</blockquote>

<h3 id="Methods">4.21.9. メソッド <button onclick="OpenOriginal('Methods')">原文</button></h3>
<dl>
  <dt id="dfn-getFrequencyResponse"><code>getFrequencyResponse</code> メソッド</dt>
    <dd><p>現在のフィルタパラメータの設定から指定の周波数に対する応答特性を計算します。</p>
      <p><dfn id="dfn-frequencyHz">frequencyHz</dfn>パラメータは応答特性を計算する周波数の配列を指定します。</p>
      <p><dfn id="dfn-magResponse">magResponse</dfn>パラメータはリニア振幅特性の値を受け取る配列を指定します。</p>
      <p><dfn id="dfn-phaseResponse">phaseResponse</dfn>パラメータはラジアン単位の位相特性を受け取る配列を指定します。</p>
    </dd>
</dl>
</div>
</div>

<div id="WaveShaperNode-section" class="section">
<h2 id="WaveShaperNode">4.22. WaveShaperNode インターフェース <button onclick="OpenOriginal('WaveShaperNode')">原文</button></h2>

<p>WaveShaperNodeは非線形のディストーションエフェクトを実装したAudioNodeプロセッサです。</p>

<p>非線形ウェーブシェイピング歪みは微妙な非線形ウォーミングやはっきりしたディストーションの両方のエフェクトで一般的に使用されています。
  任意の非線形シェイピング曲線を指定する事ができます。
</p>
<pre>
    numberOfInputs  : 1
    numberOfOutputs : 1

    channelCountMode = "max";
    channelInterpretation = "speakers";
</pre>

<p>
  出力のチャンネル数は常に入力のチャンネル数に同じです。
</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="wave-shaper-node-idl">

enum <dfn>OverSampleType</dfn> {
    "none",
    "2x",
    "4x"
};

interface <dfn id="dfn-WaveShaperNode">WaveShaperNode</dfn> : AudioNode {

    attribute Float32Array? curve;
    attribute OverSampleType oversample;

};
</code></pre>
</div>
</div>

<div id="attributes-WaveShaperNode-section" class="section">
<h3 id="attributes-WaveShaperNode">4.22.1. 属性 <button onclick="OpenOriginal('attributes-WaveShaperNode')">原文</button></h3>
<dl>
  <dt id="dfn-curve"><code>curve</code></dt>
    <dd><p>ウェーブシェイピング・エフェクトで使用されるシェイピング曲線です。
      入力信号は名目上-1 -&gt; +1 の範囲になります。
      この範囲内のそれぞれの入力サンプルは信号レベル0を配列の中心として、シェイピング曲線の配列
      インデックスに対応付けられます。
      -1よりも小さい全てのサンプルは曲線配列の最初の値に対応します。
      +1よりも大きい全てのサンプルは曲線配列の最後の値に対応します。
      実装は曲線配列の隣接した値から直線補間を行わなくてはなりません。
      curve属性の初期値はnullで、これはWaveShaperNodeは入力を変更せずにそのまま出力する事を意味します。</p>
    </dd>
</dl>

<dl>
  <dt id="dfn-oversample"><code>oversample</code></dt>
    <dd><p>シェイピング曲線を適用する際に(もし必要なら)使用するオーバーサンプリングのタイプを指定します。
      デフォルトの値は"none"で、曲線はそのまま入力サンプルに適用されます。
      "2x"または"4x"を指定すると、ある程度エイリアスを避けて品質を向上する事ができ、"4x"の場合にもっとも高い品質になります。
      アプリケーションによっては非常に高精度のシェイピング曲線を使うため、オーバーサンプリングを使用しない方が良い場合があります。
    </p>
    <p>
      "2x"または"4x"の値は次のステップが行われなくてはならない事を意味します:
    </p>
    <ol>
    <li>入力サンプルをAudioContextのサンプルレートの2倍または4倍にアップサンプリングします。
      つまりそれぞれの128サンプルのブロックは、256(2xの時)または512(4xの時)サンプルになります。</li>
    <li>シェイピング曲線を適用します。</li>
    <li>結果をAudioContextのサンプルレートにダウンサンプリングします。
      つまり処理された256(あるいは512)サンプルは最終的な結果として128サンプルを生成します。</li>
    </ol>アップサンプリングとダウンサンプリングフィルタの詳細は定められておらず、(エイリアス低減などの)音の品質向上、低レイテンシー性、パフォーマンス面でのチューニングが可能です。
    </dd>
</dl>
</div>
</div>

<div id="OscillatorNode-section" class="section">
<h2 id="OscillatorNode">4.23. OscillatorNode インターフェース <button onclick="OpenOriginal('OscillatorNode')">原文</button></h2>

<p>OscillatorNodeは周期的な波形を発生するオーディオソースを表しています。これは一般的に使われる幾つかの波形に設定する事ができます。
  更に、これは<a href="#PeriodicWave-section"><code>PeriodicWave</code></a>オブジェクトを使って任意の周期波形に設定する事が可能です。
</p>

<p>オシレーターは音の合成において一般的な基本構成ブロックです。
  OscillatorNodeは<code>start()</code>メソッドで指定された時刻に音の発生を開始します。</p>

<p>数学的に言えば、<em>連続した時間</em>の周期波形は周波数領域で考えた場合、非常に高い(あるいは無限に高い)周波数情報を持つ事ができます。
  この波形があるサンプルレートの離散時間のデジタルオーディオ信号としてサンプリングされる場合、波形をデジタル化する前に<em>ナイキスト</em>周波数(サンプリング周波数の半分)よりも高い高周波数成分の除去(フィルタで取り除く事)を考慮しなくてはなりません。
  これを行わない場合、(ナイキスト周波数よりも)高い周波数の<em>エイリアス</em>がナイキスト周波数よりも低い周波数に鏡像として折り返されます。
  多くの場合、これは音として聴こえる好ましくないノイズを引き起こします。
  これはオーディオDSPにおける基本的で良く知られている原理です。
</p>

<p>このエイリアスを避けるため、実装に使う事のできる幾つかの実践的な手段があります。しかし、これらの手段によらず、<em>理想的な</em>離散時間のデジタルオーディオ信号は数学的には完全に定義されます。
(CPUの負荷という意味で)実装のコスト対、理想への忠実性というトレードオフが実装上の問題になります。
</p>

<p>実装はこの理想を達成するためにいくらかの考慮をする事が期待されますが、ローエンド・ハードウェアでは低品質ローコストな手段を考慮する事も合理的です。
</p>

<p>
  .frequencyと.detuneはどちらも<em>a-rate</em>パラメータで、<em>computedFrequency</em>の値を決定するために一緒に使われます:
</p>

<pre>
computedFrequency(t) = frequency(t) * pow(2, detune(t) / 1200)
</pre>

<p>
  OscillatorNodeの各時刻における位相は<em>computedFrequency</em>を時間で積分したものになります。
</p>

<pre>    numberOfInputs  : 0
    numberOfOutputs : 1 (mono output)
    </pre>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="oscillator-node-idl">

enum <dfn>OscillatorType</dfn> {
  "sine",
  "square",
  "sawtooth",
  "triangle",
  "custom"
};

interface <dfn id="dfn-OscillatorNode">OscillatorNode</dfn> : AudioNode {

    attribute OscillatorType type;

    readonly attribute AudioParam frequency; // in Hertz
    readonly attribute AudioParam detune; // in Cents

    void start(double when);
    void stop(double when);
    void setPeriodicWave(PeriodicWave periodicWave);

    attribute EventHandler onended;

};
</code></pre>
</div>
</div>

<div id="attributes-OscillatorNode-section" class="section">
<h3 id="attributes-OscillatorNode">4.23.1. 属性 <button onclick="OpenOriginal('attributes-OscillatorNode')">原文</button></h3>
<dl>
  <dt id="dfn-type"><code>type</code></dt>
    <dd><p>周期波形の形状です。これは"custom"を除いて、波形の定数値を直接設定する事ができます。
      <a href="#dfn-setPeriodicWave"><code>setPeriodicWave()</code></a>メソッドはカスタム波形を設定するために使われ、それによってこの属性は"custom"に設定されます。
      デフォルトの値は"sine"です。</p>
    </dd>
</dl>

<dl>
  <dt id="dfn-frequency"><code>frequency</code></dt>
    <dd><p>(Hz:ヘルツで表される)周期波形の周波数で、そのデフォルトの<code>value</code>は440です。このパラメータは<em>a-rate</em>です。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-detune"><code>detune</code></dt>
    <dd><p>(セントで表される)デチューン値で、これは <code>frequency</code>を与えられた量だけオフセットします。
      デフォルトの<code>value</code>は0です。
      このパラメータは<em>a-rate</em>です。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-onended"><code>onended</code></dt>
    <dd><p><a href="#OscillatorNode-section"><code>OscillatorNode</code></a>にディスパッチされるendedイベントに対する
      (<cite><a href="http://www.whatwg.org/specs/web-apps/current-work/#eventhandler">HTML</a></cite>で記述される)<code>EventHandler</code>
      を設定するために使われる属性です。
      <code>OscillatorNode</code>のバッファ再生が終了した時、
      (<cite><a href="http://www.whatwg.org/specs/web-apps/current-work/#event">HTML</a></cite>で記述される)イベントタイプ<code>Event</code>が
      イベントハンドラにディスパッチされます。<span class="annotate">(訳注:この部分の記述はAudioBufferSourceNodeとの混同があります)</span></p>
    </dd>
</dl>
</div>

<div id="methodsandparams-OscillatorNode-section" class="section">
<h3 id="methodsandparams-OscillatorNode">4.23.2. メソッドとパラメータ <button onclick="OpenOriginal('methodsandparams-OscillatorNode')">原文</button></h3>
<dl>
  <dt id="dfn-setPeriodicWave"><code>setPeriodicWave</code> メソッド</dt>
    <dd><p>
      <a href="#PeriodicWave-section"><code>PeriodicWave</code></a>で与えられる任意のカスタム周期波形を設定します。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-start-AudioBufferSourceNode"><code>start</code> メソッド</dt>
    <dd><p><a href="#AudioBufferSourceNode-section"><code>AudioBufferSourceNode</code></a>で定義されているものと同じです。</p>
    </dd>
</dl>
<dl>
  <dt id="dfn-stop-AudioBufferSourceNode"><code>stop</code> メソッド</dt>
    <dd><p><a href="#AudioBufferSourceNode-section"><code>AudioBufferSourceNode</code></a>で定義されているものと同じです。</p>
    </dd>
</dl>
</div>
</div>


<div id="PeriodicWave-section" class="section">
<h2 id="PeriodicWave">4.24. PeriodicWave インターフェース <button onclick="OpenOriginal('PeriodicWave')">原文</button></h2>

<p>PeriodicWaveは<a href="#OscillatorNode-section"><code>OscillatorNode</code></a>で使用される任意の周期波形を表します。
  詳細は<a href="#dfn-createPeriodicWave">createPeriodicWave()</a>、および<a href="#dfn-setPeriodicWave">setPeriodicWave()</a>を見てください。
</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="wavetable-idl">

interface <dfn id="dfn-PeriodicWave">PeriodicWave</dfn> {

};
</code></pre>
</div>
</div>
</div>

<div id="MediaStreamAudioSourceNode-section" class="section">
<h2 id="MediaStreamAudioSourceNode">4.25. MediaStreamAudioSourceNode インターフェース <button onclick="OpenOriginal('MediaStreamAudioSourceNode')">原文</button></h2>

<p>このインターフェースは<code>MediaStream</code>からのオーディオソースを表します。
  <code>MediaStream</code>からの最初の<code>AudioMediaStreamTrack</code>がオーディオソースとして使用されます。</p>
<pre>    numberOfInputs  : 0
    numberOfOutputs : 1
</pre>

  <p>
    出力のチャンネル数は、<code>AudioMediaStreamTrack</code>のチャンネル数に対応します。
    もし有効なオーディオトラックがない場合、1チャンネルの無音が出力されます。
  </p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="media-stream-audio-source-node-idl">

interface <dfn id="dfn-MediaStreamAudioSourceNode">MediaStreamAudioSourceNode</dfn> : AudioNode {

};
</code></pre>
</div>
</div>
</div>

<div id="MediaStreamAudioDestinationNode-section" class="section">
<h2 id="MediaStreamAudioDestinationNode">4.26. MediaStreamAudioDestinationNode インターフェース <button onclick="OpenOriginal('MediaStreamAudioDestinationNode')">原文</button></h2>

<p>このインターフェースは1つの<code>AudioMediaStreamTrack</code>を持つ<code>MediaStream</code>を表すオーディオの出力地点となります。
  このMediaStreamはノードが作成された時点で作られ、<dfn>stream</dfn>属性からアクセスする事ができます。
  このストリームはgetUserMedia()によって得られたMediaStreamと同様の方法で使う事ができ、
  例えば、RTCPeerConnection addStream()メソッドを使って、リモート・ピアに送る事ができます。
</p>
<pre>
    numberOfInputs  : 1
    numberOfOutputs : 0

    channelCount = 2;
    channelCountMode = "explicit";
    channelInterpretation = "speakers";
</pre>

<p>
  入力のチャンネル数はデフォルトで2(ステレオ)です。
  入力への全ての接続は入力チャンネル数にアップミックス/ダウンミックスされます。
</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">Web IDL</span></div>

<div class="blockContent">
<pre class="code"><code class="idl-code" id="media-stream-audio-destination-node-idl">

interface <dfn id="dfn-MediaStreamAudioDestinationNode">MediaStreamAudioDestinationNode</dfn> : AudioNode {

    readonly attribute MediaStream stream;

};
</code></pre>
</div>
</div>

<div id="attributes-MediaStreamAudioDestinationNode-section" class="section">
<h3 id="attributes-MediaStreamAudioDestinationNode">4.26.1. 属性 <button onclick="OpenOriginal('attributes-MediaStreamAudioDestinationNode')">原文</button></h3>
<dl>
  <dt id="dfn-stream"><code>stream</code></dt>
    <dd><p>ノードと同じチャンネル数の1つのAudioMediaStreamTrackを持つMediaStreamです。</p>
    </dd>
</dl>
</div>

</div>
</div>
<div id="MixerGainStructure-section" class="section">
<h2 id="MixerGainStructure">6. ミキサーゲイン構成 <button onclick="OpenOriginal('MixerGainStructure')">原文</button></h2>

<p class="norm">このセクションは参考情報です</p>

<h3 id="background">背景</h3>

<p>オーディオ処理グラフについて考える時に重要な事の1つが、各ポイントにおけるゲイン(音量)をどのように調整するかです。
  例えば、標準的なミキサー卓モデルの場合、それぞれの入力バスは、プリゲイン、ポストゲイン、センドゲインを持っています。
  サブミックスとマスター出力バスもまたゲインコントロールを持っています。
  ここで述べるゲインコントロールは他のアーキテクチャーと同じように標準的なミキサー卓に使用する事ができます。
</p>

<div id="SummingJunction-section" class="section">
<h3 id="SummingJunction">サミング入力</h3>
</div>

<p><a href="#AudioNode-section"><code>AudioNode</code></a>の入力は複数の出力からの接続を受け付ける能力を持っています。
  入力はそれぞれの出力を他と足し合わせる、ユニティ・ゲインのサミング接続として振舞います<span class="annotate">(訳注:複数の信号をそのまま足し合わせる事を指します)</span>。:</p>
<img alt="unity gain summing junction"
src="images/unity-gain-summing-junction.png" />

<p>各出力のチャンネルレイアウトが一致しない場合は、<a href="#UpMix-section">ミキシング規則</a>に従ってミックス(通常はアップミックス)が行われます。
</p>

<h3 id="gain-Control">ゲインコントロール</h3>

<p>しかし多くの場合、それぞれの出力信号のゲインを制御できる事が重要です。
  <a href="#GainNode-section"><code>GainNode</code></a>によってこのコントロールが可能です:
</p>
<img alt="mixer architecture new" src="images/mixer-architecture-new.png" />

<p>ユニティ・ゲイン・サミング接続とGainNodeという2つの概念を使う事で、簡単な、あるいは複雑なミキシングのシナリオを構成する事が可能です。</p>

<h3 id="Example-mixer-with-send-busses">例: センドバス付ミキサー</h3>

<p>複数のセンドとサブミックスを含むルーティングのシナリオでは、ミキサーへのそれぞれの接続について音量または"ゲイン"のわかりやすい制御が必要です。
  基本的なレコーディングスタジオに置かれている電子機器の一番単純なものでさえ、そのようなルーティング・トポロジーは非常に一般的に使われます。</p>

<p>これは2つのセンド・ミキサーと1つののメインミキサーの例です。
  可能ではありますが、単純化のため、プリゲインコントロールとインサート・エフェクトは図示していません。:
</p>
<img alt="mixer gain structure" src="images/mixer-gain-structure.png" />

<p>この図では省略した書き方を使っています。"send 1"、"send 2"、"main bus"は実際にはAudioNodeに入力されますがここではサミングバスとして書かれており、交点にある g2_1、g3_1、などが
  あるミキサー上のあるソースの"gain"または音量を表します。
  このゲインに触れるようにするために<a href="#dfn-GainNode"><code>GainNode</code></a>が使われます。:
</p>

<p>上の図をJavaScriptで構築したものがこれになります: </p>

<div class="example">

<div class="exampleHeader">
Example</div>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">ECMAScript</span></div>

<div class="blockContent">
<pre class="code"><code class="es-code">

var context = 0;
var compressor = 0;
var reverb = 0;
var delay = 0;
var s1 = 0;
var s2 = 0;

var source1 = 0;
var source2 = 0;
var g1_1 = 0;
var g2_1 = 0;
var g3_1 = 0;
var g1_2 = 0;
var g2_2 = 0;
var g3_2 = 0;

<span class="comment">// Setup routing graph </span>
function setupRoutingGraph() {
    context = new AudioContext();

    compressor = context.createDynamicsCompressor();

    <span class="comment">// Send1 effect </span>
    reverb = context.createConvolver();
    <span class="comment">// Convolver impulse response may be set here or later </span>

    <span class="comment">// Send2 effect </span>
    delay = context.createDelay();

    <span class="comment">// Connect final compressor to final destination </span>
    compressor.connect(context.destination);

    <span class="comment">// Connect sends 1 &amp; 2 through effects to main mixer </span>
    s1 = context.createGain();
    reverb.connect(s1);
    s1.connect(compressor);

    s2 = context.createGain();
    delay.connect(s2);
    s2.connect(compressor);

    <span class="comment">// Create a couple of sources </span>
    source1 = context.createBufferSource();
    source2 = context.createBufferSource();
    source1.buffer = manTalkingBuffer;
    source2.buffer = footstepsBuffer;

    <span class="comment">// Connect source1 </span>
    g1_1 = context.createGain();
    g2_1 = context.createGain();
    g3_1 = context.createGain();
    source1.connect(g1_1);
    source1.connect(g2_1);
    source1.connect(g3_1);
    g1_1.connect(compressor);
    g2_1.connect(reverb);
    g3_1.connect(delay);

    <span class="comment">// Connect source2 </span>
    g1_2 = context.createGain();
    g2_2 = context.createGain();
    g3_2 = context.createGain();
    source2.connect(g1_2);
    source2.connect(g2_2);
    source2.connect(g3_2);
    g1_2.connect(compressor);
    g2_2.connect(reverb);
    g3_2.connect(delay);

    <span class="comment">// We now have explicit control over all the volumes g1_1, g2_1, ..., s1, s2 </span>
    g2_1.gain.value = 0.2; <span class="comment"> // For example, set source1 reverb gain </span>

    <span class="comment"> // Because g2_1.gain is an "AudioParam", </span>
    <span class="comment"> // an automation curve could also be attached to it. </span>
    <span class="comment"> // A "mixing board" UI could be created in canvas or WebGL controlling these gains. </span>
}

 </code></pre>
</div>
</div>
</div>
</div>
<br />


<div id="DynamicLifetime-section">
<h2 id="DynamicLifetime">7.  動的ライフタイム <button onclick="OpenOriginal('DynamicLifetime')">原文</button></h2>

<h3 id="DynamicLifetime-background">背景</h3>

<p class="norm">このセクションは参考情報です。 基準としての要件については<a href="#lifetime-AudioContext">AudioContext ライフタイム</a>
と<a href="#lifetime-AudioNode">AudioNode ライフタイム</a>を参照してください。
</p>

<p>静的なルーティング設定の構築が可能である事に加えて、動的に割り当てられて限られたライフタイムを持つ「ボイス」に対して特別なエフェクトのルーティングを行う事が可能である必要があります。
  この議論のためにこれらの短期間だけ存在する音を"ノート"と呼びます。
  多くのオーディオアプリケーションがこのノートという考え方を組み込んでおり、例として、ドラムマシン、シーケンサー、多くのワンショットの音がゲームプレイに従ってトリガーされる3Dゲームがあります。</p>

<p>従来のソフトウェアシンセサイザーでは、ノートは使用可能なリソースのプールから動的に割り当てられ、解放されます。
  ノートはMIDIノートオン・メッセージを受信すると割り当てられます。
  それはそのノートが発音を終了するか、(もしループ再生でなければ)サンプルデータの終わりに達した時に解放されます。
  それは、エンベロープで値が0のサスティンフェーズに達したり、MIDIノートオフ・メッセージによってエンベロープのリリースフェーズに達したりする事で発生します。
  MIDIノートオフの場合には、そのノートは即時ではなく、リリースエンベロープが終了した時点で解放されます。
  どの時間においても、多数のノートが再生中であり、常に新しいノートがルーティンググラフに追加され、古いノートが解放されながらそのノートの組は常に変化しています。</p>

<p>オーディオシステムはそれぞれの"ノート"イベントに対して、ルーティンググラフの一部分の切り落としを自動的に行います。
  1つの"ノート"は1つの<code>AudioBufferSourceNode</code>で表され、それは直接、他の処理ノードに接続さする事ができます。
  ノートが再生を終了した時、コンテキストは自動的にその<code>AudioBufferSourceNode</code>への参照を解放します。
  それによって、そのノードが接続されていた先の全てのノードへの参照が解放され、という風に続きます。
  そのノードは自動的にグラフから切断され、全ての参照が無くなった時点で破棄されます。
  グラフ内の、長時間存在して動的なボイスから共有されるノードは明示的に管理する事ができます。
  複雑なように聞こえますが、これらは全て、特にJavaScriptでハンドリングする必要なく、自動的に行われます。</p>

<h3 id="Example-DynamicLifetime">例</h3>

<div class="example">

<div class="exampleHeader">
Example</div>
<img alt="dynamic allocation" src="images/dynamic-allocation.png" />

<p>ローパスフィルタ、パンナー、2番目のゲインノードがワンショットの音から直接接続されています。
  そのため再生が終わった時、コンテキストは自動的にそれら(点線内の全て)を解放します。
  もしJavaScriptからワンショットの音とそれに接続されているノードへの参照がもう無ければ、それらは
  すぐにグラフから外され、破棄されます。
  ストリーミングのソースはグローバルな参照を持っており、それが明示的に切断されるまで、接続されたままで残ります。
  JavaScriptではどうなるのかがここにあります。
</p>

<div class="block">

<div class="blockTitleDiv">
<span class="blockTitle">ECMAScript</span></div>

<div class="blockContent">
<pre class="code"><code class="es-code">

var context = 0;
var compressor = 0;
var gainNode1 = 0;
var streamingAudioSource = 0;

<span class="comment">// Initial setup of the "long-lived" part of the routing graph </span>
function setupAudioContext() {
    context = new AudioContext();

    compressor = context.createDynamicsCompressor();
    gainNode1 = context.createGain();

    // Create a streaming audio source.
    var audioElement = document.getElementById('audioTagID');
    streamingAudioSource = context.createMediaElementSource(audioElement);
    streamingAudioSource.connect(gainNode1);

    gainNode1.connect(compressor);
    compressor.connect(context.destination);
}

<span class="comment">// Later in response to some user action (typically mouse or key event) </span>
<span class="comment">// a one-shot sound can be played. </span>
function playSound() {
    var oneShotSound = context.createBufferSource();
    oneShotSound.buffer = dogBarkingBuffer;

    <span class="comment">// Create a filter, panner, and gain node. </span>
    var lowpass = context.createBiquadFilter();
    var panner = context.createPanner();
    var gainNode2 = context.createGain();

    <span class="comment">// Make connections </span>
    oneShotSound.connect(lowpass);
    lowpass.connect(panner);
    panner.connect(gainNode2);
    gainNode2.connect(compressor);

    <span class="comment">// Play 0.75 seconds from now (to play immediately pass in 0)</span>
    oneShotSound.start(context.currentTime + 0.75);
}
</code></pre>
</div>
</div>
</div>
</div>



<div id="UpMix-section" class="section">
<h2 id="UpMix">9. チャンネルのアップミックスとダウンミックス <button onclick="OpenOriginal('UpMix')">原文</button></h2>

<p class="norm">このセクションは基準情報です。</p>

<img src="images/unity-gain-summing-junction.png" alt="Graph: mixing sound from two sources into the AudioDestination" />

<p>
<a href="#MixerGainStructure-section">ミキサーゲイン構成</a>では、AudioNodeの<dfn>入力</dfn>が1つ以上のAudioNodeの<dfn>出力</dfn>からどのように
接続されるかを記述しています。
これらの出力からの接続のそれぞれは、0以外の特定のチャンネル数を持っています。
入力はそれら全ての接続のチャンネルを組み合わせるための<em>ミキシング規則</em>を持っています。
単純な例としては、もし入力がモノラル出力とステレオ出力から接続されている場合、そのモノラル接続は通常、ステレオにアップミックスされ、ステレオ接続と加算されます。
しかしもちろん、全てのAudioNodeの全ての入力について、その正確な<em>ミキシング規則</em>を定義する事が重要です。
全ての入力に対するデフォルトのミキシング規則は、特に非常に良く使われるモノラルとステレオのストリームに対しては、あまり詳細について煩わされる事なく
"ちゃんと動作する"ように選ばれます。
しかし、高度な使用例、特にマルチチャンネルの場合にはその規則は変更する事が可能です。
</p>

<p>幾つかの用語の定義として、<em>アップミックス</em>は、小さなチャンネル数のストリームを受け取り、大きなチャンネル数のストリームに変換する処理を指します。
  <em>ダウンミックス</em>は、大きなチャンネル数のストリームを受け取り、小さなチャンネル数のストリームに変換する処理を指します。
</p>

<p>
  AudioNodeの入力は全ての出力からの接続をどのようにミックスするかを決定するために、3つの基本的な情報を使用します。
  この処理の一部として、任意の時刻における、入力の実際のチャンネル数を表す内部的な値、<dfn>computedNumberOfChannels</dfn>を計算します。
</p>

<p>
  チャンネルのアップミックス、ダウンミックス規則に関わるAudioNodeの属性は<a href="#attributes-AudioNode-section">上</a>に定義されています。
  以下はそのそれぞれが何を意味しているかの、より正確な仕様です。
</p>

<ul>
<li><dfn>channelCount</dfn>は<dfn>computedNumberOfChannels</dfn>を計算する補助として使用されます。</li>

<li><dfn>channelCountMode</dfn>は<dfn>computedNumberOfChannels</dfn>がどのように計算されるかを決定します。
  この数が一度計算されると、その数のチャンネル数に全ての接続はアップまたはダウンミックスされます。
  ほとんどのノードはデフォルトの値として"max"を持ちます。

<ul>
<li>“max”: <dfn>computedNumberOfChannels</dfn>は全ての接続のチャンネル数の最大値として計算されます。
このモードでは<dfn>channelCount</dfn>は無視されます。</li>
<li>“clamped-max”: "max"と同様ですが、<dfn>channelCount</dfn>を上限とします。</li>
<li>“explicit”: <dfn>computedNumberOfChannels</dfn>の正確な値が<dfn>channelCount</dfn>で指定されます。</li>
</ul>

</li>

<li><dfn>channelInterpretation</dfn>はそれぞれのチャンネルがどのように取り扱われるかを指定します。
  例えば、特定の配置をされたスピーカーや、単なる別々のチャンネルがあります。
  この値はまさにアップ、ダウンミックスがどのように行われるかに影響します。
  デフォルトの値は"speakers"です。

<ul>
<li>“speakers”: <a href="#ChannelLayouts">モノラル/ステレオ/クワッド/5.1のためのアップ・ダウンミックス式</a>を使用します。
  チャンネル数がこれらのスピーカー基本レイアウトに一致しない場合、"discrete"に戻します。
</li>
<li>“discrete”: アップミックスの場合、チャンネルを使い切るまで順に埋めて行き、余っているチャンネルには0を出力します。
  ダウンミックスでは、可能な限りチャンネルを順に埋め、余ったチャンネルは捨てられます。</li>
</ul>

</li>

</ul>

<p>
  AudioNodeのそれぞれの入力の実装は次のようにしなくてはなりません:
</p>

<ol>
<li><dfn>computedNumberOfChannels</dfn>を計算します。</li>
<li>入力への接続のそれぞれについて:
<ul>
<li> 接続を<dfn>channelInterpretation</dfn>に従って<dfn>computedNumberOfChannels</dfn>にアップミックスまたはダウンミックスします。</li>
<li> (他の接続からの)ミックスされたストリームとミックスします。これは各接続の対応するそれぞれのチャンネルを素直にミックスします。</li>
</ul>
</li>
</ol>




<div id="ChannelLayouts-section" class="section">
<h3 id="ChannelLayouts">9.1. スピーカーチャンネル配置 <button onclick="OpenOriginal('ChannelLayouts')">原文</button></h3>

<p class="norm">このセクションは基準情報です。</p>

<p><dfn>channelInterpretation</dfn>が"speakers"の場合、特定のチャンネル配置に対してのアップミックスおよびダウンミックスが定義されます。
</p>

<p>これらのスピーカーの配置に対してチャンネルの順序(そして幾つかの省略)が重要になります。
</p>

<p>
  ではまず、モノラル、ステレオ、クワッド、5.1について考えてみます。
  その他のチャンネル配置は後で定義します。
</p>

<h4 id ="ChannelOrdering">9.1.1. チャンネルの順序</h4>

<pre>  Mono
    0: M: mono

  Stereo
    0: L: left
    1: R: right
  </pre>

<pre>  Quad
    0: L:  left
    1: R:  right
    2: SL: surround left
    3: SR: surround right

  5.1
    0: L:   left
    1: R:   right
    2: C:   center
    3: LFE: subwoofer
    4: SL:  surround left
    5: SR:  surround right
  </pre>

<h4 id="UpMix-sub">9.1.2. アップミックスのスピーカー配置</h4>

<pre>Mono up-mix:

    1 -&gt; 2 : up-mix from mono to stereo
        output.L = input;
        output.R = input;

    1 -&gt; 4 : up-mix from mono to quad
        output.L = input;
        output.R = input;
        output.SL = 0;
        output.SR = 0;

    1 -&gt; 5.1 : up-mix from mono to 5.1
        output.L = 0;
        output.R = 0;
        output.C = input; // put in center channel
        output.LFE = 0;
        output.SL = 0;
        output.SR = 0;

Stereo up-mix:

    2 -&gt; 4 : up-mix from stereo to quad
        output.L = input.L;
        output.R = input.R;
        output.SL = 0;
        output.SR = 0;

    2 -&gt; 5.1 : up-mix from stereo to 5.1
        output.L = input.L;
        output.R = input.R;
        output.C = 0;
        output.LFE = 0;
        output.SL = 0;
        output.SR = 0;

Quad up-mix:

    4 -&gt; 5.1 : up-mix from quad to 5.1
        output.L = input.L;
        output.R = input.R;
        output.C = 0;
        output.LFE = 0;
        output.SL = input.SL;
        output.SR = input.SR;</pre>

<h4 id="down-mix">9.1.3. ダウンミックスのスピーカー配置</h4>

<p>ダウンミックスは例えば5.1チャンネルソースの素材を処理する場合に必要になりますが、再生はステレオになります。</p>
<pre>
Mono down-mix:

    2 -&gt; 1 : stereo to mono
        output = 0.5 * (input.L + input.R);

    4 -&gt; 1 : quad to mono
        output = 0.25 * (input.L + input.R + input.SL + input.SR);

    5.1 -&gt; 1 : 5.1 to mono
        output = 0.7071 * (input.L + input.R) + input.C + 0.5 * (input.SL + input.SR)


Stereo down-mix:

    4 -&gt; 2 : quad to stereo
        output.L = 0.5 * (input.L + input.SL);
        output.R = 0.5 * (input.R + input.SR);

    5.1 -&gt; 2 : 5.1 to stereo
        output.L = L + 0.7071 * (input.C + input.SL)
        output.R = R + 0.7071 * (input.C + input.SR)

Quad down-mix:

    5.1 -&gt; 4 : 5.1 to quad
        output.L = L + 0.7071 * input.C
        output.R = R + 0.7071 * input.C
        output.SL = input.SL
        output.SR = input.SR

</pre>
</div>

<h3 id="ChannelRules-section">9.2. チャンネル規則の例 <button onclick="OpenOriginal('ChannelRules-section')">原文</button></h3>

<p class="norm">このセクションは参考情報です。</p>

<div class="block">
<div class="blockTitleDiv">
<div class="blockContent">
<pre class="code"><code class="idl-code">
// Set gain node to explicit 2-channels (stereo).
gain.channelCount = 2;
gain.channelCountMode = "explicit";
gain.channelInterpretation = "speakers";

// Set "hardware output" to 4-channels for DJ-app with two stereo output busses.
context.destination.channelCount = 4;
context.destination.channelCountMode = "explicit";
context.destination.channelInterpretation = "discrete";

// Set "hardware output" to 8-channels for custom multi-channel speaker array
// with custom matrix mixing.
context.destination.channelCount = 8;
context.destination.channelCountMode = "explicit";
context.destination.channelInterpretation = "discrete";

// Set "hardware output" to 5.1 to play an HTMLAudioElement.
context.destination.channelCount = 6;
context.destination.channelCountMode = "explicit";
context.destination.channelInterpretation = "speakers";

// Explicitly down-mix to mono.
gain.channelCount = 1;
gain.channelCountMode = "explicit";
gain.channelInterpretation = "speakers";
</code></pre>
</div>
</div>
</div>
</div>


<div id="Spatialization-section" class="section">
<h2 id="Spatialization">11. 空間音響 / パンニング <button onclick="OpenOriginal('Spatialization')">原文</button></h2>

<h3 id="Spatialization-background">背景</h3>

<p>最近の3Dゲームで良く要求される機能として、動的な空間音響と複数の音源の3D空間での移動があります。
  OpenAL、FMOD、クリエイティブ社のEAX、マイクロソフト社のXACT Audioなどのようなゲームオーディオエンジンはこの機能を持っています。
</p>

<p><code>PannerNode</code>を使って、オーディオストリームを<code>AudioListener</code>に対する相対的な空間位置に配置し、定位させる事ができます。
  <a href="#AudioContext-section"><code>AudioContext</code></a>は単一の<code>AudioListener</code>を持っています。
  パンナーとリスナーはどちらも右手系デカルト座標の3D空間内の位置を持っています。
  エフェクトの計算で使われる座標系は、メートルやフィートのような特別な単位とは独立した不変の座標系になっているため、座標空間で使用される単位は定められておらず、その必要もありません。
  (ソースストリームの)<code>PannerNode</code>オブジェクトは音が放出される方向を示す<code>orientation</code>ベクトルを持っています。
  加えてそれらは音の指向性の強さを示す<code>サウンドコーン</code>を持っています。
  例えば、音が無指向性であれば、方向には関係なくどこからでも聴こえますが、指向性が強い場合、それがリスナーの方向を向いている場合にだけ聴こえます。
  (人間の耳を表す)<code>AudioListener</code>オブジェクトは人間が向いている方向を表すために<code>orientation</code>と<code>up</code>のベクトルを持っています。
  音源ストリームとリスナーの両方が移動できるため、それらはどちらも移動の速度と方向を表す<code>velocity</code>ベクトルを持っています。
  これらを基にした2つの速度から、ピッチが変動するドップラー効果を作り出す事ができます。</p>

  <p>レンダリングの間、<code>PannerNode</code>は<em>azimuth</em>と<em>elevation</em>を計算します。
  これらの値は空間配置をレンダリングするために実装によって内部的に使用されます。
  これらの値がどのように使われるかの詳細については、<a href="#Spatialization-panning-algorithm">パンニングアルゴリズム</a>を参照してください。
</p>

<p><em>azimuth</em>と<em>elevation</em>を計算するために、次のアルゴリズムが使用されなくてはなりません:
</p>

<div class="block">
<div class="blockTitleDiv">
<div class="blockContent">
<pre class="code"><code class="es-code">
// Calculate the source-listener vector.
vec3 sourceListener = source.position - listener.position;

if (sourceListener.isZero()) {
    // Handle degenerate case if source and listener are at the same point.
    azimuth = 0;
    elevation = 0;
    return;
}

sourceListener.normalize();

// Align axes.
vec3 listenerFront = listener.orientation;
vec3 listenerUp = listener.up;
vec3 listenerRight = listenerFront.cross(listenerUp);
listenerRight.normalize();

vec3 listenerFrontNorm = listenerFront;
listenerFrontNorm.normalize();

vec3 up = listenerRight.cross(listenerFrontNorm);

float upProjection = sourceListener.dot(up);

vec3 projectedSource = sourceListener - upProjection * up;
projectedSource.normalize();

azimuth = 180 * acos(projectedSource.dot(listenerRight)) / PI;

// Source in front or behind the listener.
double frontBack = projectedSource.dot(listenerFrontNorm);
if (frontBack &lt; 0)
    azimuth = 360 - azimuth;

// Make azimuth relative to "front" and not "right" listener vector.
if ((azimuth >= 0) &amp;&amp; (azimuth &lt;= 270))
    azimuth = 90 - azimuth;
else
    azimuth = 450 - azimuth;

elevation = 90 - 180 * acos(sourceListener.dot(up)) / PI;

if (elevation > 90)
    elevation = 180 - elevation;
else if (elevation &lt; -90)
    elevation = -180 - elevation;
</code></pre>
</div>
</div>
</div>

<h3 id="Spatialization-panning-algorithm">パンニングアルゴリズム <button onclick="OpenOriginal('Spatialization-panning-algorithm')">原文</button></h3>

<p>
<em>モノラル -> ステレオ</em>と<em>ステレオ -> ステレオ</em>のパンニングがサポートされなくてはなりません。
<em>モノラル -> ステレオ</em>処理は入力への全ての接続がモノラルの場合に使用されます。
そうでない場合、<em>ステレオ -> ステレオ</em>処理が使用されます。</p>

<p>次のアルゴリズムが実装されなくてはなりません: </p>
<ul>
  <li>等価パワー(ベクトルベース)パンニング
    <p>これは基本的な、簡単で比較的コストの低いアルゴリズムですが、妥当な結果が得られます。
      これは音楽ソースに対するパンニングで良く用いられます。
    </p>
    このパンニングアルゴリズムでは<em>elevation</em>値は無視されます。

    <p>
      処理のため、次のステップが使われます:
    </p>

    <ol>

    <li>
    <p>
    <em>azimuth</em>値はまず次の手順で、 -90 &lt;= <em>azimuth</em> &lt;= +90 の範囲内にされます:
    </p>
    <pre>
    // Clamp azimuth to allowed range of -180 -> +180.
    azimuth = max(-180, azimuth);
    azimuth = min(180, azimuth);

    // Now wrap to range -90 -> +90.
    if (azimuth &lt; -90)
        azimuth = -180 - azimuth;
    else if (azimuth > 90)
        azimuth = 180 - azimuth;
    </pre>
    </li>

    <li>
    <p>
      <em>モノラル -> ステレオ</em>の場合、<em>azimuth</em>から 0 -> 1に正規化された値<em>x</em>が計算されます:
    </p>
    <pre>
    x = (azimuth + 90) / 180
    </pre>

    <p>
    または<em>ステレオ -> ステレオ</em>の場合:
    </p>
    <pre>
    if (azimuth &lt;= 0) { // from -90 -> 0
        // inputL -> outputL and "equal-power pan" inputR as in mono case
        // by transforming the "azimuth" value from -90 -> 0 degrees into the range -90 -> +90.
        x = (azimuth + 90) / 90;
    } else { // from 0 -> +90
        // inputR -> outputR and "equal-power pan" inputL as in mono case
        // by transforming the "azimuth" value from 0 -> +90 degrees into the range -90 -> +90.
        x = azimuth / 90;
    }
    </pre>
    </li>

    <li>
    <p>
      左右のチャンネルのゲインが計算されます:
    </p>
    <pre>
    gainL = cos(0.5 * PI * x);
    gainR = sin(0.5 * PI * x);
    </pre>
    </li>

    <li>
    <p>
      <em>モノラル -> ステレオ</em>の場合、出力の計算は次のようになります:</p>
    <pre>
    outputL = input * gainL
    outputR = input * gainR
    </pre>
    <p>
      あるいは<em>ステレオ -> ステレオ</em>の場合、出力の計算は次のようになります:</p>
    <pre>
    if (azimuth &lt;= 0) { // from -90 -> 0
        outputL = inputL + inputR * gainL;
        outputR = inputR * gainR;
    } else { // from 0 -> +90
        outputL = inputL * gainL;
        outputR = inputR + inputL * gainR;
    }
    </pre>
    </li>

    </ol>



  </li>
  <li><a href="http://en.wikipedia.org/wiki/Head-related_transfer_function">HRTF</a>パンニング(ステレオの場合のみ)
    <p>この処理には様々なazimuthとelevationで記録されたHRTFインパルスレスポンスのセットが必要です。
      オープンでフリーなインパルスレスポンスが幾つか存在します。
      実装には高度に最適化されたコンボリューション機能が必要になります。
      これは"equal-power"よりも幾らかコストが必要ですが、より空間的な音を得る事ができます。</p>
    <img alt="HRTF panner" src="images/HRTF_panner.png" /></li>
</ul>

<h3 id="Spatialization-distance-effects">距離効果 <button onclick="OpenOriginal('Spatialization-distance-effects')">原文</button></h3>
<p>
  近くの音は大きく、遠くの音は小さくなります。
  リスナーからの距離に対して、正確に<em>どれだけ</em>音量が変化するのかは、<em>distanceModel</em>属性に依存します。
</p>


<p>オーディオレンダリングの間、<em>distance</em>値がパンナーとリスナーの位置を基に次のように計算されます:
</p>
<pre>
v = panner.position - listener.position
</pre>
<pre>
distance = sqrt(dot(v, v))
</pre>

<p>
  そして、<em>distance</em>を使って<em>distanceModel</em>属性に依存する<em>distanceGain</em>が計算されます。
  それぞれの距離モデルについて、これがどのように計算されるかの詳細は<a href="#dfn-distanceModel">distanceModel</a>セクションを参照してください。
</p>
<p>
  この処理の一部分として、<code>PannerNode</code>は入力されるオーディオ信号を<em>distanceGain</em>でスケーリング/増幅し、遠くの音は小さく近ければ大きくします。
</p>




<h3 id="Spatialization-sound-cones">サウンドコーン <button onclick="OpenOriginal('Spatialization-sound-cones')">原文</button></h3>

<p>リスナーとそれぞれの音源はそれがどの方向を向いているかを表す方向ベクトルを持っています。
  それぞれの音源の音の放射特性は、音源の方向ベクトルに対してソース/リスナー間の角度の関数で音の大きさを表した内部および外部の"コーン"で表現されます。
  つまり、直接リスナーの方を向いた音源は、違う方向を向いた音源よりも大きく聴こえます。
  音源はまた、無指向性に設定する事も可能です。</p>

<p>
  あるソース(<code>PannerNode</code>)とリスナーに対して、コーン効果によるゲインへの影響を計算するためには、次のアルゴリズムを使用しなくてはなりません。
</p>

<div class="block">
<div class="blockTitleDiv">
<div class="blockContent">
<pre class="code"><code class="idl-code">
if (source.orientation.isZero() || ((source.coneInnerAngle == 360) &amp;&amp; (source.coneOuterAngle == 360)))
    return 1; // no cone specified - unity gain

// Normalized source-listener vector
vec3 sourceToListener = listener.position - source.position;
sourceToListener.normalize();

vec3 normalizedSourceOrientation = source.orientation;
normalizedSourceOrientation.normalize();

// Angle between the source orientation vector and the source-listener vector
double dotProduct = sourceToListener.dot(normalizedSourceOrientation);
double angle = 180 * acos(dotProduct) / PI;
double absAngle = fabs(angle);

// Divide by 2 here since API is entire angle (not half-angle)
double absInnerAngle = fabs(source.coneInnerAngle) / 2;
double absOuterAngle = fabs(source.coneOuterAngle) / 2;
double gain = 1;

if (absAngle &lt;= absInnerAngle)
    // No attenuation
    gain = 1;
else if (absAngle &gt;= absOuterAngle)
    // Max attenuation
    gain = source.coneOuterGain;
else {
    // Between inner and outer cones
    // inner -> outer, x goes from 0 -> 1
    double x = (absAngle - absInnerAngle) / (absOuterAngle - absInnerAngle);
    gain = (1 - x) + source.coneOuterGain * x;
}

return gain;
</code></pre>
</div>
</div>
</div>

<h3 id="Spatialization-doppler-shift">ドップラー効果 <button onclick="OpenOriginal('Spatialization-doppler-shift')">原文</button></h3>
<ul>
  <li>移動する音源を現実的にシミュレートしたピッチ変化をもたらします。</li>
  <li>音源とリスナーの移動ベクトル、音速、ドップラー係数に依存します。</li>
</ul>

<p>
  AudioPannerNodeに直接または間接的に接続されている全てのAudioBufferSourceNodeに対する付加的な再生速度係数として使用される、ドップラー効果の変化量を計算するため、次のアルゴリズムが使用されなくてはなりません。
</p>

<div class="block">
<div class="blockTitleDiv">
<div class="blockContent">
<pre class="code"><code class="idl-code">
double dopplerShift = 1; // Initialize to default value
double dopplerFactor = listener.dopplerFactor;

if (dopplerFactor > 0) {
    double speedOfSound = listener.speedOfSound;

    // Don't bother if both source and listener have no velocity.
    if (!source.velocity.isZero() || !listener.velocity.isZero()) {
        // Calculate the source to listener vector.
        vec3 sourceToListener = source.position - listener.position;

        double sourceListenerMagnitude = sourceToListener.length();

        double listenerProjection = sourceToListener.dot(listener.velocity) / sourceListenerMagnitude;
        double sourceProjection = sourceToListener.dot(source.velocity) / sourceListenerMagnitude;

        listenerProjection = -listenerProjection;
        sourceProjection = -sourceProjection;

        double scaledSpeedOfSound = speedOfSound / dopplerFactor;
        listenerProjection = min(listenerProjection, scaledSpeedOfSound);
        sourceProjection = min(sourceProjection, scaledSpeedOfSound);

        dopplerShift = ((speedOfSound - dopplerFactor * listenerProjection) / (speedOfSound - dopplerFactor * sourceProjection));
        fixNANs(dopplerShift); // Avoid illegal values

        // Limit the pitch shifting to 4 octaves up and 3 octaves down.
        dopplerShift = min(dopplerShift, 16);
        dopplerShift = max(dopplerShift, 0.125);
    }
}
</code></pre>
</div>
</div>
</div>




</div>

<div id="Convolution-section" class="section">
<h2 id="Convolution">12. コンボリューションによる線形エフェクト <button onclick="OpenOriginal('Convolution')">原文</button></h2>

<h3 id="Convolution-background">背景</h3>

<p><a href="http://en.wikipedia.org/wiki/Convolution">コンボリューション</a>はオーディオ信号に、興味深い多くの高品質な線形エフェクトを施すための数学的な処理です。
  このエフェクトは、例えばコンサートホール、教会、屋外劇場のような音響空間をシミュレートするために、非常に頻繁に使われます。
  それはまた例えば、クローゼットの中から聴こえるくぐもった音や、水中の音、電話を通した音、古いスピーカーキャビネットで演奏した音などのような複雑なフィルタエフェクトにも使用されます。
  この技術は、主要な動画や音楽の制作で非常に一般的に使われ、極めて応用範囲が広く高品質なものです。
</p>

<p>個別のエフェクトは<code>インパルスレスポンス</code>によって定義されます。インパルスレスポンスはオーディオファイルの形になっていて、
  例えば洞窟などのような、実際の音響空間で<a href="#recording-impulse-responses">記録する事ができ</a>、あるいは様々な技術によって合成する事もできます。
</p>

<h3 id="Convolution-motivation">標準として使用する動機</h3>

<p>多くのゲームオーディオエンジン(OpenAL、FMOD、クリエイティブ社のEAX、マイクロソフト社のXACTなど)の特徴的な機能として、音響空間での残響エフェクトのシミュレーションがあります。
  しかしエフェクトを発生するためのコードは一般的に特殊で、アルゴリズム的です
  (一般的には手作業での微調整が行われた、相互にフィードバックを掛けたディレイとオールパスフィルタの組み合わせです)。
  ほとんどの場合、そのコードは特殊な実装であるだけでなく、プロプライエタリであり、ソースは公開されておらず、各社が唯一の品質を達成するための独自の"魔術"を加えています。
  それぞれの実装は、異なるパラメータセットを持った特殊なものになり、欲しい効果を統一的に得る事が不可能になっています。
  そしてそのコードはプロプライエタリであるため、どの実装も標準として採用する事が不可能です。
  更に、アルゴリズム的な残響エフェクトは、パラメータにどのように手を加えても効果のバリエーションは比較的狭い範囲に限られます。
</p>

<p>コンボリューションエフェクトはこれらの問題を、非常に正確に定義された数学的アルゴリズムを基本処理として使う事で解決します。
  インパルスレスポンスはオーディオストリームに適用する音響効果を、URLで示す事ができるオーディオファイルによって正確に表現します。
  可能なエフェクトの範囲は膨大です。
</p>

<h3 id="Convolution-implementation-guide">実装ガイド <button onclick="OpenOriginal('Convolution-implementation-guide')">原文</button></h3>
<p>
  線形コンボリューションは効率的に実装する事が可能です。
実際にどう実装するかを記述した幾つかの<a href="convolution.html">メモが</a>ここにあります。
</p>

<h3 id="Convolution-reverb-effect">リバーブ・エフェクト (マトリックス付き)</h3>

<p class="norm">このセクションは基準情報です。</p>

<p>一般的な状況として、ソースがN個の入力チャンネル、インパルスレスポンスがK個のチャンネル、再生システムがM個の出力チャンネルを持っています。
  そのため、最終的な出力を得るためにこれらのチャンネルをどうマトリックスするかが問題になります。
</p>

<p>N、M、Kの集合に対して以下の実装がされなくてはなりません(最初の図は単に一般的なケースを図示したもので基準ではなく、それ以降の図は基準になります)。
より複雑で自由なマトリックスを望む開発者は、一般性を失う事なく複数の<code>ConvolverNode</code>オブジェクトを<code>ChannelMergerNode</code>と組み合わせて使う事ができます。
</p>

<p>モノラルのオーディオ入力に対し、モノラルのインパルスレスポンスを使った1チャンネルのコンボリューション演算を行う事で、モノラルの出力を得る事ができます。
  しかし、より広がりのある音を得るためには、2チャンネルのオーディオ入力と1、2、または4チャンネルのインパルスレスポンスの使用考えなくてはなりません。
  次の図は、NとMが1、2、4の場合のステレオ再生の一般的なケースです。
</p>
<img alt="reverb matrixing" src="images/reverb-matrixing.png" />

<h3 id="recording-impulse-responses">インパルスレスポンスの記録 <button onclick="OpenOriginal('recording-impulse-responses')">原文</button></h3>

<p class="norm">このセクションは参考情報です。</p>
<img alt="impulse response" src="images/impulse-response.png" /> <br />
<br />


<p>実際の音響空間でインパルスレスポンスを記録する<a href="http://pcfarina.eng.unipr.it/Public/Papers/226-AES122.pdf">最新</a>で精密な手段は、
  長い指数的サインスイープを用いる方法です。
  テスト用のトーンは20から30秒あるいはもっと長くする事もできます。
  <br />
  スピーカーを通したテストトーンの再生は、部屋の様々な位置と方向にセットされたマイクで幾つか録音されます。
  それぞれの録音について、スピーカーの配置と方向、マイクの種類、それらの設定、配置、方向を記録する事が重要です。
</p>

<p>これらの録音に対し、テストトーンとの逆コンボリューション演算を行う事で、そのマイクの配置に対応した、部屋のインパルスレスポンスを作り出すという後処理が必要です。
  これらのインパルスレスポンスをコンボリューションリバーブ・エンジンにロードすると、その部屋にいる時の音を再現する事ができます。</p>

<h3 id="tools">ツール</h3>

<p>2つのコマンドラインツールが書かれました:<br />

  <code>generate_testtones</code>は指数的サインスイープのテストトーンおよびその反転を生成します。
  もう一つのツール<code>convolve</code>は後処理用です。
  これらのツールによって、録音機材があれば誰でも独自のインパルスレスポンスを記録する事が可能です。
  ツールを実際にテストするために、幾つかの録音は面白い音の響きがある倉庫の空間で行われました。
  これらは後で、コマンドラインツールによって後処理が行われました。</p>

<pre>% generate_testtones -h
Usage: generate_testtone
	[-o /Path/To/File/To/Create] Two files will be created: .tone and .inverse
	[-rate &lt;sample rate&gt;] sample rate of the generated test tones
	[-duration &lt;duration&gt;] The duration, in seconds, of the generated files
	[-min_freq &lt;min_freq&gt;] The minimum frequency, in hertz, for the sine sweep

% convolve -h
Usage: convolve input_file impulse_response_file output_file</pre>
<br />


<h3 id="recording-setup">レコーディング・セットアップ <button onclick="OpenOriginal('recording-setup')">原文</button></h3>
<img alt="recording setup" src="images/recording-setup.png" /> <br />
<br />
オーディオインターフェース: Metric Halo Mobile I/O 2882 <br />
<br />
<br />
<br />
<img alt="microphones speaker" src="images/microphones-speaker.png" /> <br />
<br />
<img alt="microphone" src="images/microphone.png" /> <img alt="speaker"
src="images/speaker.png" /> <br />
<br />
マイクロフォン: AKG 414s, スピーカー: Mackie HR824 <br />
<br />
<br />


<h3 id="warehouse">倉庫空間</h3>
<img alt="warehouse" src="images/warehouse.png" /> <br />
<br />
</div>

<div id="JavaScriptProcessing-section" class="section">
<h2 id="JavaScriptProcessing">13. JavaScript による合成と処理 <button onclick="OpenOriginal('JavaScriptProcessing')">原文</button></h2>

<p class="norm">このセクションは参考情報です。</p>

<p>Mozillaプロジェクトは、JavaScriptによる直接的なオーディオの合成と処理を<a href="https://wiki.mozilla.org/Audio_Data_API">実験</a>的に実施してきました。
このアプローチは、ある程度のクラスのオーディオ処理に関して興味深いもので、彼らは多くの印象的なデモを産み出しました。
この仕様は、JavaScriptによる直接的な合成と処理を行う手段を、<a href="#ScriptProcessorNode-section"><code>ScriptProcessorNode</code></a>と呼ばれる<a href="#AudioNode-section"><code>AudioNode</code></a>の特別なサブタイプを使う事で包含しています。
</p>

<p>ここにJavaScriptの直接処理が有用であるというる幾つかの興味深い例があります:</p>

<h3 id="custom-DSP-effects">カスタムDSPエフェクト</h3>

<p>普通ではない面白いカスタムオーディオ処理をJSで直接行う事ができます。
  それはまた、新しいアルゴリズムのプロトタイプの良いテスト環境になります。
  これは極めて贅沢な事です。
</p>

<h3 id="educational-applications">教育アプリケーション</h3>

<p>JSによる処理は、例えば矩形波の倍音への分解やFMシンセシス技術など、コンピュータでの音楽合成と処理の概念を図示するのに理想的です。
</p>

<h3 id="javaScript-performance">JavaScript パフォーマンス</h3>

<p>JavaScriptは様々な<a href="#JavaScriptPerformance-section">パフォーマンスの問題</a>を持っているため、全てのタイプのオーディオ処理に適しているわけではありません。
この文書で提案されているアプローチは、例えばマルチソースの3D空間音響やC++コードで最適化されたコンボリューションなど、オーディオ処理のために計算上の負荷がかかる(JavaScriptによってリアルタイムで実行するにはコストがかかりすぎる)側面のある機能を含んでいます。
<a href="#ModularRouting-section">モジュラー方式</a>のAPIは、JavaScriptによる直接処理とC++で最適化されたコードの両方を組み合わせる事を可能にします。
</p>
</div>

<div id="Performance-section" class="section">
<h2 id="Performance">15. パフォーマンスについての考察 <button onclick="OpenOriginal('Performance')">原文</button></h2>

<div id="Latency-section" class="section">
<h3 id="Latency">15.1. レイテンシー: それは何で何故重要なのか</h3>
</div>
<img alt="latency" src="images/latency.png" />

<p>Webアプリケーションでは、マウスとキーボードのイベント(keydown、mousedown等)と聴こえる音の間のディレイタイムは重要です。
</p>

<p>この時間の遅れはレイテンシーと呼ばれ、幾つかの要因(入力デバイスのレイテンシー、内部バッファのレイテンシー、DSP処理のレイテンシー、出力デバイスのレイテンシー、スピーカーとユーザーの耳の距離、など)によって引き起こされ、累積されてゆきます。
レイテンシーが大きいとユーザー体験の満足度は下がります。
極端な場合、それは音楽制作やゲームプレイを不可能にする事もあります。
ある程度のレベルになるとそれはタイミングに影響し、音が遅れている、あるいはゲームが反応しないなどの印象を与えます。
音楽アプリケーションではタイミングの問題はリズムに影響します。
ゲームではタイミングの問題はゲームプレイの精度に影響します。
インタラクティブなアプリケーションでは、それは一般的にアニメーションのフレームレートがとても低いのと同じようにユーザー体験を非常に安っぽくします。
満足できるレイテンシーはアプリケーションによって異なり、3～6ミリ秒から25～50ミリ秒程度です。
</p>

<div id="Glitching-section" class="section">
<h3 id="audio-glitching">15.2. オーディオグリッジ <button onclick="OpenOriginal('audio-glitching')">原文</button></h3>
</div>

<p>オーディオグリッジは正常な連続したオーディオストリームが途切れる事で発生し、大きなクリックノイズやポップノイズを引き起こします。
それはマルチメディアシステムでは最悪の失敗と考えられ、絶対に避けなければなりません。
それは適切な優先度を持っていなかったり時間的制約から起こるスケジューリングの遅延のような事が原因で、オーディオストリームをハードウェアに供給するスレッドの反応速度の問題によって引き起こされる事があります。
また、それはオーディオDSPが与えられたCPUの速度ではリアルタイムで処理できないような多量の仕事をしようとする事で起こる場合もあります。
</p>

<h3 id="hardware-scalability">15.3. ハードウェアのスケーラビリティ <button onclick="OpenOriginal('hardware-scalability')">原文</button></h3>

<p>システムはオーディオフレームの欠落を起こさないように、リソースに制約のある状況下では潔くオーディオ処理の品質を下げるべきです。</p>

<p>まず最初に、プラットフォームによらず明確であるのは、オーディオ処理の負荷によってマシンを完全にロックアップさせてはならないという事です。
次に、オーディオのレンダリングは、耳に聴こえるような<a href="#Glitching-section">グリッジ</a>のないクリーンで途切れないオーディオストリームを作り出す必要があると言う事です。</p>

<p>システムはモバイルフォンやタブレットデバイスからラップトップ、デスクトップコンピュータまで広い範囲のハードウェアで動作しなくてはなりません。
しかし、よりコンピューティングリソースに制限のある電話デバイスでは、規模を縮小してオーディオレンダリングの複雑さを減少させる技術について考慮する必要があります。
例えば、ある時間に同時に発音するノートの合計数を減らすため、ボイスドロッピング・アルゴリズムを実装する事もできます。</p>

<p>ここにCPU消費を抑えるための幾つかの技術をあげます:
</p>

<h4 id="CPU-monitoring">15.3.1. CPU監視</h4>

<p>オーディオが途切れる事を避けるため、CPU消費は100%以下に留まらなければなりません。</p>

<p>それぞれのAudioNode(そして繋がれたノードのチェーン)に関する相対的なCPU消費はレンダリングの時間単位に対するパーセンテージとして動的に計測可能です。
  シングルスレッドの実装では、全体のCPU消費が100%以下でなくてはなりません。
  計測された消費量は、レンダリングに対する動的な調整のために実装が内部的に使用しても良いでしょう。
  またそれは、JavaScriptからの使用のため<code>AudioNode</code>の<code>cpuUsage</code>属性を通して見えるようにする事もできます。
</p>

<p>計測されたCPU消費が100%に近く(あるいは高すぎると考えられるどんなスレッショルドでも)になった時、レンダリンググラフに更に<code>AudioNode</code>を追加しようとするとボイスドロッピングをトリガーする事もできます。</p>

<h4 id="Voice-dropping">15.3.2. ボイスドロッピング</h4>

<p>ボイスドロッピングはCPU消費を適度な範囲に維持するため、同時に発音するボイス(ノート)の数を制限する技術です。
  許されるボイスの合計数に上限のスレッショルドを設定するか、または、動的にCPU消費を監視しCPU消費がスレッショルドを超えたらボイスをドロップする事もできます。
  あるいは、それら2つの技術の組み合わせにする事も可能です。
  各ボイスについてのCPU消費を監視する場合、それはソースノードから経由するエフェクトでの、そのボイスに対する処理の全体について計測する事も可能です。
</p>

<p>ボイスが"ドロップ"される時、それはレンダリングするオーディオストリームに耳に聴こえるクリックノイズやポップノイズを引き起こさないようにする事が必要です。
  このための方法の1つは、そのボイスが完全にレンダリンググラフから除去される前にそのオーディオレンダリングを素早くフェードアウトする事です。</p>

<p>1つ以上のボイスをドロップする事が必要になった時、現在再生中のトータルのアンサンブルの中でどのボイスをドロップするかについて、様々な戦略があります。
  この決定の助けになる、組み合わせで使っても良い幾つかの要素として次のものがあります:</p>
<ul>
  <li>新しいボイスではなく、もっとも長く再生している古いボイス</li>
  <li>音の大きいボイスではなく、全体のミックスへの寄与が少ない音量の小さいボイス</li>
  <li>CPU消費の低いボイスではなく、相対的によりCPUリソースを消費するボイス</li>
  <li>AudioNodeに<code>priority</code>属性を持たせて相対的なボイスの重要性を決定に使用するのも良いでしょう</li>
</ul>

<h4 id="Simplification-of-Effects-Processing">15.3.3. エフェクトの単純化</h4>

<p>この文書で記述されているエフェクトのほとんどは比較的コストが低く、低速のモバイル機器でも動作させる事ができそうなものです。
  しかしながら、<a href="#ConvolverNode-section">コンボリューションエフェクト</a>では様々なインパルスレスポンスが設定でき、その幾つかはモバイル機器では重すぎる事もあります。
  一般的に言えば、CPU消費はインパルスレスポンスの長さとチャンネル数で変わります。
  そのため、インパルスレスポンスがある程度の長さを超える場合には、実行させないという考え方も合理的です。
  正確なリミットは機器の速度から決定する事もできます。
  これらの長いレスポンスのコンボリューションを完全に排除する代わりにインパルスレスポンスの長さを許される最大値まで短縮する、あるいはインパルスレスポンスのチャンネル数を削減するのも面白いかも知れません。
</p>

<p>コンボリューションエフェクトに加え、<a href="#PannerNode-section"><code>PannerNode</code></a>もまた、HRTFパンニングモデルを使う場合には、コストが高くなるかも知れません。
  より低速な機器では、コンピューティングリソースを節約するためにEQUALPOWERのようなコストの低いアルゴリズムを使う事もできます。</p>

<h4 id="Sample-rate">15.3.4. サンプルレート</h4>

<p>非常に低速なデバイスでは、通常よりも低いサンプルレートでレンダリングする事を考慮する価値があるかも知れません。
  例えば、サンプルレートを44.1kHzから22.05kHzに下げる事も可能です。
  サンプルレートをオンザフライで変更する事は実装が困難で、変更の際に耳に聴こえるグリッジを引き起こすため、この判断は<code>AudioContext</code>が作成される時になされる必要があります。</p>

<h4 id="pre-flighting">15.3.5. プリフライト</h4>

<p>マシンパワーを大まかに決定するため、(JavaScriptを通して)ある種の"プリフライト"コードを呼び出す事が可能です。
  そしてJavaScriptコードはこの情報を、それ以上処理が集中しないよう、パワフルなマシンでは通常動作する処理を縮小するために使用できます。
  また、下層にある実装がボイスドロッピング・アルゴリズムの中でこの情報を要素として使用しても良いでしょう。</p>

<p><span class="ednote">TODO: add specification and more detail here </span></p>

<h4 id="Authoring-for-different-user-agents">15.3.6. 異なるユーザーエージェントのためのオーサリング</h4>
JavaScriptコードはユーザーエージェント情報を、処理が集中しないよう、パワフルなマシンでは通常動作する処理を縮小するために使用する事もできます。

<h4 id="Scalability-of-Direct-JavaScript-Synthesis">15.3.7. JavaScriptによる直接合成/処理のスケーラビリティ</h4>

<p>JavaScriptで直接行われるどんなオーディオDSP処理もスケーラビリティと関わってきます。
  可能な範囲でJavaScriptコード自身がCPU消費を監視し、非力なデバイスで動作する時は大掛かりな処理を縮小します。
  もしそれが"オール・オア・ナッシング"型の処理の場合は、オーディオストリームの破綻を避けるために、ユーザーエージェントチェックかプリフライトが必要でしょう。</p>

<div id="JavaScriptPerformance-section" class="section">
<h3 id="JavaScriptPerformance">15.4. リアルタイム処理と合成に関するJavaScriptの課題: <button onclick="OpenOriginal('JavaScriptPerformance')">原文</button></h3>
</div>
  JavsScriptでオーディオを処理する際に、妥当な低レイテンシーでオーディオにグリッジが無く、信頼性を確保するのは、特にCPU負荷が高い状況においては極めてチャレンジングな事です。
<ul>
  <li>JavaScriptは良く最適化されたC++コードに比べると非常に低速であり、近年のプロセッサで高いパフォーマンスを得るために必須のSSEとマルチスレッディングを利用する事ができません。
    最適化されたネイティブコードはJavaScriptに比べ、FFTを20倍程度高速に実行できます。
    それはコンボリューションや3D空間音響や大量のオーディオソースのようなオーディオのヘビーデューティーな処理を行えるほど効率的ではありません。</li>
  <li>setInterval()とXHRハンドリングはオーディオ処理から時間を奪います。
    ある程度複雑なゲームでは、ゲームの物理モデルとグラフィックスのために幾つかのJavaScriptのリソースが必要になります。
    オーディオレングリングは、(グリッジを避けつつ充分にレイテンシーを低くするため)デッドライン・ドリブンであるため、これが課題になります。</li>
  <li>JavaScriptはリアルタイム処理スレッドでは動作せず、そのためシステム中で走っている他の多くのスレッドによってプリエンプトされる事があります。</li>
  <li>ガベージコレクション(そしてMac OS Xのオートリリースプールも)は、JavaScriptスレッドに予測不能な遅延を引き起こす事があります。</li>
  <li>複数のJavaScriptコンテキストがメインスレッド上で走る事が可能で、処理を行っているコンテキストから時間を奪います。</li>
  <li>ページレンダリングのような(JavaScript以外の)他のコードがメインスレッド上で動作します。</li>
  <li>JavaScriptスレッド上で、ロックを取り、メモリをアロケーションする事ができます。これは余計なスレッドのプリエンプションを引き起こす事があります。</li>
</ul>
比較的非力なパフォーマンスと消費電力/バッテリー寿命問題を抱える今日のモバイルデバイスでは、問題はさらに難しくなります。
</div>
<br />
<br />


<div id="ExampleApplications-section" class="section">
<h2 id="ExampleApplications">16. アプリケーション例 <button onclick="OpenOriginal('ExampleApplications')">原文</button></h2>

<p class="norm">このセクションは参考情報です。</p>

<p>動作の例は<a href="http://chromium.googlecode.com/svn/trunk/samples/audio/index.html">デモ</a>ページを参照してください。</p>

<p>ここにWebオーディオシステムでサポート可能にするべき幾つかのタイプのアプリケーションがあります:</p>

<h3 id="basic-sound-playback">基本的な音の再生</h3>

<p>マウスクリックやロールオーバー、キープレスのようなユーザーアクションに反応して、シンプルで<a href="#Latency-section"><strong>低レイテンシー</strong></a>な効果音が再生されます。
</p>
<br />


<h3 id="threeD-environmentse-and-games">3D環境とゲーム</h3>
<img alt="quake" src="images/redteam_680.jpg" />
<br />
<br />


<p>エレクトロニックアーツは3D空間音響とコンボリューションによるルームシミュレーションを活かした<a href="http://sophie-lu.com/Strike-Fortress-EA">Strike Fortress</a>という、印象的な没入型のゲームを開発しました。
</p>

<img alt="beach demo" src="images/beach-demo.png" />

<p>音を伴った3D環境はデスクトップアプリケーションとゲームコンソールのゲームでは一般的です。
  空間音響を持った3Dの島の環境で、カモメが頭上を飛び、波が海岸に打ち付け、火が爆ぜ、橋が軋み、木々が風にざわつくのを想像してみてください。
  人がそのシーン中を移動すると音も自然な位置に移動します。
  更に水中に潜ると、ローパスフィルタによって手が加えられ、まさに水中にいるような音になります。
</p>
<br />
<br />
<img alt="box2d" src="images/box2d.png" /> <img alt="8-ball"
src="images/8-ball.png" /> <br />
<br />


<p><a href="http://box2d.org/">Box2D</a>は興味深いオープンソースの2Dゲーム用物理エンジンです。
これは様々な実装を持っていますが、その1つにCanvas 2Dを基にしたものがあります。
速度ベクトルを考慮した音の空間への配置、フィルタカットオフなどのオーディオエフェクトのバラメータのモジュレーションを使い、オブジェクトの衝突に対する動的な効果音を持ったデモが作られました。
</p>

<p>マルチサンプルの効果音を使ったバーチャル・プールゲームも作られています。
</p>
<br />


<h3 id="musical-applications">音楽アプリケーション</h3>
<img alt="garageband" src="images/garage-band.png" /> <img
alt="shiny drum machine" src="images/shiny-drum-machine.png" /> <img
alt="tonecraft" src="images/tonecraft.png" /> <br />
<br />
多くの作曲および音楽制作アプリケーションが作れるようになります。
オーディオイベントのタイトなスケジューリングが必要なアプリケーションも実装でき、教育的なものとエンターテインメント的なものの両方が可能です。
ドラムマシン、デジタルDJアプリケーション、<a href="http://en.wikipedia.org/wiki/GarageBand">GarageBand</a>にある機能の幾つかを持ったタイムラインベースのデジタルミュージック・プロダクションソフトウェアさえ書くことができます。<br />
<br />


<h3 id="music-visualizers">ミュージックビジュアライザー</h3>
<img alt="music visualizer" src="images/music-visualizer.png" /> <br />
<br />
WebGLのGLSLシェーダーと組み合わせる事で、リアルタイム解析データをエンターテインメント的に表示する事ができます。
これらはiTunesに見られるような先進的なものにできます。<br />
<br />


<h3 id="educational-applications_2">教育アプリケーション</h3>
<img alt="javascript processing" src="images/javascript-processing.png" />

<p>様々な教育アプリケーションを書く事ができます。
  図示しているのは音楽理論とコンピュータミュージック合成と処理分野でのコンセプトです。
</p>
<br />


<h3 id="artistic-audio-exploration">芸術的な音の探究</h3>

<p>インスタレーションのための芸術的な音環境には多くの創造的な可能性があります。
</p>
<br />
</div>

<div id="SecurityConsiderations-section" class="section">
<h2 id="SecurityConsiderations">17. セキュリティに関する考察 <button onclick="OpenOriginal('SecurityConsiderations')">原文</button></h2>

<p>このセクションは<em>参考情報</em>です。 </p>
</div>

<div id="PrivacyConsiderations-section" class="section">
<h2 id="PrivacyConsiderations">18. プライバシーに関する考察 <button onclick="OpenOriginal('PrivacyConsiderations')">原文</button></h2>

<p>このセクションは<em>参考情報</em>です。
AudioNodeが持っている様々な情報から、Web Audio APIは潜在的に(オーディオハードウェアのサンプルレートのような)クライアントの特徴的な機能の情報をAudioNodeインータフェースを使うどんなページにでもさらけ出します。
更に、RealtimeAnalyzerNodeやScriptProcessorNodeのインターフェースからタイミングに関する情報も得られます。
情報はその後クライアントのフィンガープリントを作成するために使われるかも知れません。
</p>

<p>現在の所オーディオ入力についてはこの文書では規定していませんが、それはクライアント機器のオーディオ入力やマイクへのアクセスを得る事ができます。
  これにはおそらく<a href="http://developers.whatwg.org/">getUserMedia()API</a>が使われますが、ユーザーからの許可を適切に得る事が必要になります。
</p>
</div>

<div id="requirements-section" class="section">
<h2 id="requirements">19. 要求と使用例 <button onclick="OpenOriginal('requirements')">原文</button></h2>

<p><a href="#ExampleApplications-section">アプリケーション例</a>を参照してください。
</p>
</div>


</div>

<div class="appendix section" id="references">
<h2 id="L17310">A.リファレンス <button onclick="OpenOriginal('L17310')">原文</button></h2>

<div class="section" id="normative-references">
<h3 id="Normative-references">A.1 基準リファレンス</h3>
<dl>
  <dt id="DOM">[DOM] </dt>
  <dd><a href="http://dom.spec.whatwg.org/">DOM</a>,
      A. van Kesteren, A. Gregor, Ms2ger. WHATWG.</dd>
  <dt id="HTML">[HTML] </dt>
  <dd><a href="http://www.whatwg.org/specs/web-apps/current-work/multipage/">HTML</a>,
      I. Hickson. WHATWG.</dd>
  <dt id="RFC2119">[RFC2119] </dt>
    <dd>S. Bradner. <a
      href="http://www.ietf.org/rfc/rfc2119.txt"><cite><span>Key words for use
      in RFCs to Indicate Requirement Levels.</span></cite></a> Internet RFC
      2119. URL: <a
      href="http://www.ietf.org/rfc/rfc2119.txt">http://www.ietf.org/rfc/rfc2119.txt</a>
    </dd>
</dl>
</div>

<div class="section" id="informative-references">
<h3 id="Informative-references">A.2 参考リファレンス</h3>

<p>参考リファレンスはありません。</p>
</div>
</div>

<div class="section" id="acknowledgements">
<h2 id="L17335">B.謝辞</h2>

<p>この仕様は<a href="http://www.w3.org/2011/audio/">Audioワーキンググループ</a>が共同で作成したものです。</p>

<p>ワーキンググループのメンバーは以下の通りです(執筆時点、アルファベット順):<br />
Adenot, Paul (Mozilla Foundation);
Akhgari, Ehsan (Mozilla Foundation);
Berkovitz, Joe (Invited Expert);
Bossart, Pierre (Intel Corporation);
Carlson, Eric (Apple, Inc.);
Geelnard, Marcus (Opera Software);
Goode, Adam (Google, Inc.);
Gregan, Matthew (Mozilla Foundation);
Jägenstedt, Philip (Opera Software);
Kalliokoski, Jussi (Invited Expert);
Lilley, Chris (W3C Staff);
Lowis, Chris (Invited Expert. WG co-chair from December 2012 to September 2013, affiliated with British Broadcasting Corporation);
Mandyam, Giridhar (Qualcomm Innovation Center, Inc);
Noble, Jer (Apple, Inc.);
O'Callahan, Robert(Mozilla Foundation);
Onumonu, Anthony (British Broadcasting Corporation);
Paradis, Matthew (British Broadcasting Corporation);
Raman, T.V. (Google, Inc.);
Schepers, Doug (W3C/MIT);
Shires, Glen (Google, Inc.);
Smith, Michael (W3C/Keio);
Thereaux, Olivier (British Broadcasting Corporation) – WG Chair;
Verdie, Jean-Charles (MStar Semiconductor, Inc.);
Wilson, Chris (Google,Inc.);
ZERGAOUI, Mohamed (INNOVIMAX)
</p>

<p>仕様作成に貢献したワーキンググループの元メンバーは以下の通りです:<br />
Caceres, Marcos (Invited Expert);
Cardoso, Gabriel (INRIA);
Chen, Bin (Baidu, Inc.);
MacDonald, Alistair (W3C Invited Experts) — WG co-chair from March 2011 to July 2012;
Michel, Thierry (W3C/ERCIM);
Rogers, Chris (Google, Inc.) – Specification Editor until August 2013;
Wei, James (Intel Corporation);
</p>
</div>

<div class="section" id="ChangeLog-section">
<h2 id="ChangeLog">C. Web Audio API 更新履歴</h2>
<p>See <a href="changelog.html">changelog.html</a>.</p>
</div>
</body>
</html>
