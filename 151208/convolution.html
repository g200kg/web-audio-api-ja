<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
      "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<!--  <title>Web Audio API - Convolution Architecture</title>-->
    <title>Web Audio API - コンボリューション・アーキテクチャ</title>
  <link rel="stylesheet" href="http://www.w3.org/StyleSheets/TR/W3C-WD" type="text/css" />
    <style type="text/css">
    body {
      line-height:125%;
    }
    .annotate {
      color:red;
    }
    em {
      padding:4px;
    }
    dfn {
      padding:4px;
    }
    </style>
</head>

  <body>

<!--   <h3>Convolution Reverb</h3>-->
   <h3>コンボリューション・リバーブ</h3>
<p>
<!--    <em>This section is informative and may be helpful to implementors</em>-->
    <em>このセクションは、実装する際に役立つかも知れない参考情報です。</em>
</p>

<p>
<!--    A <a href="http://en.wikipedia.org/wiki/Convolution_reverb">convolution reverb</a> can be used to simulate an acoustic space with very high quality.
      It can also be used as the basis for creating a vast number of unique and interesting special effects.  This technique is widely used
      in modern professional audio and motion picture production, and is an excellent choice to create room effects in a game engine.-->
      <a href="http://en.wikipedia.org/wiki/Convolution_reverb">コンボリューション・リバーブ</a>は、高品質な音響空間のシミュレーションに使用する事ができます。
      また非常に広範囲な特殊効果の作成に使用する事も可能です。この技術は、近年のプロフェッショナルなオーディオ/動画制作でも使われており、また、ゲーム・エンジンのルーム・エフェクトを作り出す素晴らしい選択でもあります。
</p>
    <p>

    <!--Creating a well-optimized real-time convolution engine is one of the more challenging parts of the Web Audio API implementation.
  When convolving an input audio stream of unknown (or theoretically infinite) length, the <a href="http://en.wikipedia.org/wiki/Overlap-add_method">overlap-add</a> approach is used, chopping the
  input stream into pieces of length L, performing the convolution on each piece, then re-constructing the output signal by delaying each result and summing.-->
  良く最適化されたコンボリューション・エンジンを作る事はWeb Audio APIの実装の中でも非常に難易度の高い部分です。
  入力される不定長の(あるいは理論上無限の長さの)オーディオストリームに対してコンボリューション演算を行う際には、ストリームを長さLの断片に分割し、それぞれの断片にコンボリューション演算を行い、その結果を遅延させながら足し合わせて出力信号を再構築する、「<a href="http://en.wikipedia.org/wiki/Overlap-add_method">overlap-add</a>」手法が使われます。
</p>

  <!--<h4>Overlap-Add Convolution</h4>-->
  <h4>Overlap-Add コンボリューション</h4>
  <img src="http://upload.wikimedia.org/wikipedia/commons/7/77/Depiction_of_overlap-add_algorithm.png" alt="Depiction of overlap-add algorithm" />



  <p>

    <!--Direct convolution is far too computationally expensive due to the extremely long impulse responses typically used.  Therefore an approach using
    <a href="http://en.wikipedia.org/wiki/FFT">FFTs</a> must be used.  But naively doing a standard overlap-add FFT convolution using an FFT of size N with L=N/2, where N is chosen to be at least twice the length of the convolution kernel (zero-padding the kernel) to perform each convolution operation in the diagram above would incur a substantial input to output pipeline latency on the order of L samples.  Because of the enormous audible delay, this simple method cannot be used.  Aside from the enormous delay, the size N of the FFT could be extremely large.  For example, with an impulse response of 10 seconds at 44.1Khz, N would equal 1048576 (2^20).
      This would take a very long time to evaluate.  Furthermore, such large FFTs are not practical due to substantial phase errors.-->
    コンボリューション演算を直接的に行うと、よくある長いインパルスレスポンスを使用する場合に極めて高い計算能力が必要になるため、、<a href="http://en.wikipedia.org/wiki/FFT">FFT</a>演算を用いる手法を使う必要があります。しかし、単純にOverlap-Addを使用して、FFTのサイズNをL=N/2とし、(0パディングした)コンボリューション・カーネルのサイズの少なくとも2倍に取り、上の図のような構成にすると、Lサンプル分の大きな入出力間の経路のレイテンシーが発生します。聴感上のディレイが大きくなるため、この単純な手法は使えません。 また大きなディレイは別にしても、FFTのサイズ N は極めて大きくなります。例えば44.1kHzで10秒間のインパルスレスポンスの場合Nは1048576(2^20)です。 このため実行に非常に長い時間が必要になり、更に、そのような長いFFTは位相エラーが大きくなるため実際的ではありません。
    </p>


    <!--<h4>Optimizations and Tricks</h4>-->
    <h4>最適化とトリック</h4>


    <p>
<!--    There exist several clever tricks which break the impulse response into smaller pieces, performing separate convolutions, then
    combining the results (exploiting the property of linearity).  The best ones use a divide and conquer approach using different size FFTs and a direct
    convolution for the initial (leading) portion of the impulse response to achieve a zero-latency output.    There are additional optimizations which can be done exploiting the
    fact that the tail of the reverb typically contains very little or no high-frequency energy.  For this part, the convolution may be done at a lower sample-rate...-->
    幾つかのトリックとして、インパルスレスポンスを(直線性の特性を利用して)小さなサイズに分割し、個別にコンボリューションを実行して結果を再合成する方法があります。最も良い方法の１つは異なるサイズのFFTとインパルスレスポンスの初期(先頭)部分に対する直接コンボリューション演算によってゼロ・レイテンシー出力を得る分割攻略手法です。
    更に加えてリバーブが典型的には末尾部分に高周波成分のエネルギーをほとんど、あるいは全く含まない事を利用した最適化を行う事も可能です。この部分に関してはコンボリューションは非常に低いサンプルレートで演算する事も可能になります。
    </p>

    <p>
    <!--Performance can be quite good, easily
    done in real-time without creating undo stress on modern mid-range CPUs.  A multi-threaded implementation is really required if low (or zero) latency is required because of the way the buffering / processing chunking works.  Achieving good performance requires a highly optimized FFT algorithm.-->

    パフォーマンスは、最近のミッドレンジCPUで不安定になるようなストレスを与える事なくリアルタイム処理が容易にできるくらいに良くする事ができます。低(あるいはゼロ)レイテンシー処理を行うには、バッファリング/プロセシングをブロック毎に実行するため、マルチスレッド実装が必要になります。また高いパフォーマンスを得るには高度に最適化されたFFTアルゴリズムが必要になります。
    </p>

    <!--<h4>Multi-channel convolution</h4>-->
    <h4>マルチチャンネル・コンボリューション</h4>
    <p>
      <!--It should be noted that a convolution reverb typically involves two convolution operations, with separate impulse responses for the left and right channels in
      the stereo case.  For 5.1 surround, at least five separate convolution operations are necessary to generate output for each of the five channels.-->

      コンボリューション・リバーブは、ステレオ信号の場合、通常左および右チャンネルの独立したインパルスレスポンスによる、2つのコンボリューション演算を行います。 5.1チャンネルサラウンドの場合ならば5チャンネルのそれぞれの出力を得るために少なくとも5つの独立したコンボリューション演算が必要になります。
    </p>

    <!--<h4>Impulse Responses</h4>-->
    <h4>インパルスレスポンス</h4>
    <p>
      <!--Similar to other assets such as JPEG images, WAV sound files, MP4 videos, shaders, and geometry, impulse responses can be considered as multi-media assets.  As with these other
      assets, they require work to produce, and the high-quality ones are considered valuable.  For example, a company called Audio Ease makes a fairly expensive ($500 - $1000)
      product called <a href="http://www.audioease.com/Pages/Altiverb/AltiverbMain.html">Altiverb</a>
      containing several nicely recorded impulse responses along with a convolution reverb engine.-->

      例えばJPEGイメージ、WAV音声ファイル、MP4ビデオ、シェーダー、ジオメトリなどの他のリソースと同様に、インパルスレスポンスはマルチメディア・リソースの１つと考えられます。他のリソースと同じく、作成のためには作業が必要であり、高品質なものには価値があります。例えば、Audio Ease社ではかなり高価格($500～$1000)な<a href="http://www.audioease.com/Pages/Altiverb/AltiverbMain.html">Altiverb</a>と呼ばれる製品を開発しており、コンボリューション・エンジンと共に幾つかの良く録音されたインパルスレスポンスを含んでいます。
      </p>


    <!--<h3>Convolution Engine Implementation</h3>-->
    <h3>コンボリューション・エンジンの実装</h3>


    <!--<h4>FFTConvolver (short convolutions)</h4>-->
    <h4>FFTコンボルバー (ショート・コンボリューション)</h4>

    <p>
    <!--The <code>FFTConvolver</code> is able to do short convolutions with the FFT size N being at least twice as large as the
    length of the short impulse response.  It incurs a latency of N/2 sample-frames.  Because of this latency and performance considerations,
    it is not suitable for long convolutions.  Multiple instances of this building block can be used to perform extremely long convolutions.-->

    <code>FFTコンボルバー</code>は、短時間のインパルスレスポンスに対して、少なくとも2倍の長さNのサイズのFFTを利用して短時間のコンボリューションを行う事ができます。この方法はN/2サンプルフレームのレイテンシーを招きます。このレイテンシーとパフォーマンス面を考慮すると長いコンボリューションには適しません。この構成のブロックのインスタンスを複数使用する事で極めて長いコンボリューションを実行する事が可能になります。
    </p>

    <img src="images/fft-convolver.png" alt="description of FFT convolver" />


    <!--<h4>ReverbConvolver (long convolutions)</h4> -->
    <h4>リバーブ・コンボルバー (ロング・コンボリューション)</h4>
    <!--The <code>ReverbConvolver</code> is able to perform extremely long real-time convolutions on a single audio channel.
    It uses multiple <code>FFTConvolver</code> objects as well as an input buffer and an accumulation buffer.  Note that it's
    possible to get a multi-threaded implementation by exploiting the parallelism.  Also note that the leading sections of the long
    impulse response are processed in the real-time thread for minimum latency.  In theory it's possible to get zero latency if the
    very first FFTConvolver is replaced with a DirectConvolver (not using a FFT).-->

    <code>リバーブ・コンボルバー</code>は単一のオーディオ・チャンネルに対して非常に長時間のリアルタイム・コンボリューション処理を行う事ができます。
    これは、入力バッファと積算バッファに加えて複数の<code>FFTコンボルバー</code>オブジェクトを使用します。
    特筆すべき点としては並列処理を活用するためマルチ・スレッド実装をする事も可能です。
    また、レイテンシーを最小化するために、長いインパルスレスポンスの先頭部分をリアルタイム・スレッドで処理する事も可能です。理論上は先頭のFFTコンボルバーを(FFTを使わない)直接コンボリューション演算に置き換える事によりゼロ・レイテンシーの処理も可能です。



    <img src="images/reverb-convolver.png" alt="description of reverb convolver" />


    <!--<h4>Reverb Effect (with matrixing)</h4>-->
    <h4>リバーブ・エフェクト (マトリックス使用)</h4>
    <img src="images/reverb-matrixing.png" alt="description of reverb matrixing" />





    <!--<h3>Recording Impulse Responses</h3>-->
    <h3>インパルスレスポンスの録音</h3>


    <img src="images/impulse-response.png" alt="impulse-response waveforms" />


    <p>
      <!--The most <a href="http://pcfarina.eng.unipr.it/Public/Papers/226-AES122.pdf">modern</a>
     and accurate way to record the impulse response of a real acoustic space is to use
    a long exponential sine sweep. The test-tone can be as long as 20 or 30 seconds, or longer.-->
    実際の音響空間でインパルスレスポンスを記録する<a href="http://pcfarina.eng.unipr.it/Public/Papers/226-AES122.pdf">最新</a>で精密な手段は、長い指数的サインスイープを用いる方法です。テスト用のトーンは20～30秒あるいはもっと長くする事もできます。
    </p>




    <p>
    <!--  Several recordings of the
    test tone played through a speaker can be made with microphones placed and oriented at various positions in the room.  It's important
    to document speaker placement/orientation, the types of microphones, their settings, placement, and orientations for each recording taken.-->

    スピーカーを通したテストトーンの再生は、部屋の様々な位置と方向にセットされたマイクで幾つか録音されます。 それぞれの録音について、スピーカーの配置と方向、マイクの種類、それらの設定、配置、方向を記録する事が重要です。

    </p>

    <p>
    <!--Post-processing is required for each of these recordings by performing an inverse-convolution with the test tone,
    yielding the impulse response of the room with the corresponding microphone placement.  These impulse responses are then
    ready to be loaded into the convolution reverb engine to re-create the sound of being in the room.-->

    これらの録音に対し、テストトーンとの逆コンボリューション演算を行う事で、そのマイクの配置に対応した、部屋のインパルスレスポンスを作り出すという後処理が必要です。 これらのインパルスレスポンスをコンボリューションリバーブ・エンジンにロードすると、その部屋にいる時の音を再現する事ができます。

    </p>

    <!--<h3>Tools</h3>-->
    <h3>ツール</h3>
    <p>
    <!--Two command-line tools have been written:-->
    2つのコマンドラインツールが書かれました:
    </p>
    <p>
     <!--<code>generate_testtones</code> generates an exponential sine-sweep test-tone and its inverse.  Another
    tool <code>convolve</code> was written for post-processing.  With these tools, anybody with recording equipment can record their own impulse responses.
      To test the tools in practice, several recordings were made in a warehouse space with interesting
    acoustics.  These were later post-processed with the command-line tools.-->
    <code>generate_testtones</code>は指数的サインスイープのテストトーンおよびその反転を生成します。 もう一つのツール<code>convolve</code>は後処理用です。 これらのツールによって、録音機材があれば誰でも独自のインパルスレスポンスを記録する事が可能です。 ツールを実際にテストするために、幾つかの録音は面白い音の響きがある倉庫の空間で行われました。 これらは後で、コマンドラインツールによって後処理が行われました。
    </p>

    <pre>

    % generate_testtones -h
    Usage: generate_testtone
    	[-o /Path/To/File/To/Create] Two files will be created: .tone and .inverse
    	[-rate &lt;sample rate&gt;] sample rate of the generated test tones
    	[-duration &lt;duration&gt;] The duration, in seconds, of the generated files
    	[-min_freq &lt;min_freq&gt;] The minimum frequency, in hertz, for the sine sweep

    % convolve -h
    Usage: convolve input_file impulse_response_file output_file
        </pre>
  </body>
  </html>
